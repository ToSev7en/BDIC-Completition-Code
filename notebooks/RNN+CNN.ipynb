{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "from keras.models import load_model\n",
    "\n",
    "from keras.layers import Activation\n",
    "from keras.layers import Input, Dense, Embedding, Conv1D, Conv2D, MaxPooling1D, MaxPool2D\n",
    "from keras.layers import Reshape, Flatten, Dropout, Concatenate\n",
    "from keras.layers import SpatialDropout1D, concatenate\n",
    "from keras.layers import GRU, Bidirectional, GlobalAveragePooling1D, GlobalMaxPooling1D\n",
    "from keras.layers.recurrent import LSTM,SimpleRNN\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from keras.callbacks import Callback, ModelCheckpoint, EarlyStopping\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "import jieba\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入自定义库\n",
    "from utils.data_utils import clean_str\n",
    "from utils.data_utils import build_vocab\n",
    "from utils.data_utils import get_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(102024, 2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 加载训练集合\n",
    "df_train_dataset = pd.read_csv('./data/training-inspur.csv', encoding='utf-8')\n",
    "# 加载测试集\n",
    "df_test_dataset = pd.read_csv('./data/Preliminary-texting-1.csv', encoding='utf-8')\n",
    "# 查看数据集合 shape\n",
    "df_test_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 提取数据集中所需的字段\n",
    "df_train_dataset = df_train_dataset[['COMMCONTENT', 'COMMLEVEL']]\n",
    "df_test_dataset = df_test_dataset[['COMMCONTENT']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(122024, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 合并数据集用于构建词汇表\n",
    "df_all_dataset = pd.concat([df_train_dataset, df_test_dataset], ignore_index=True)\n",
    "df_all_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = '/Users/tsw/ScenicSpotReviews'\n",
    "\n",
    "W2V_DIR = BASE_DIR + '/embeddings/'\n",
    "\n",
    "TEXT_DATA_DIR = BASE_DIR + '/data/'\n",
    "\n",
    "MAX_NUM_WORDS = 33950\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 150 # 每篇文章选取150个词\n",
    "\n",
    "MAX_NB_WORDS = 80000 # 将字典设置为含有1万个词84480\n",
    "\n",
    "EMBEDDING_DIM = 300 # 词向量维度，300维\n",
    "\n",
    "VALIDATION_SPLIT = 0.1 # 测试集大小，全部数据的10%\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "NUM_LABELS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seg_corpus(corpus):\n",
    "    seg_corpus = []\n",
    "    for line in corpus:\n",
    "        line = str(line).strip()\n",
    "        seg_list = jieba.cut(line, cut_all=False)\n",
    "        # 过滤空字符\n",
    "        seg_list = [w for w in seg_list if w != ' ']\n",
    "        seg_corpus.append(\" \".join(seg_list))\n",
    "    return seg_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/5n/2_by50851fxc4d_snc1d9wf80000gn/T/jieba.cache\n",
      "Loading model cost 3.185 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "# 对所有文本分词\n",
    "seged_text = seg_corpus(df_all_dataset['COMMCONTENT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>COMMCONTENT</th>\n",
       "      <th>COMMLEVEL</th>\n",
       "      <th>COMMCONTENT_SEG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>普通公园一个只是多了几个泉而已，人不多，适合老人孩子闲逛，买票的话还是贵了，人家说6.30之...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>普通 公园 一个 只是 多 了 几个 泉 而已 ， 人不多 ， 适合 老人 孩子 闲逛 ， ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>跟儿子在里面玩了一天，非常好！跟儿子在里面玩了一天，非常好！真的很不错哦，有空还要去</td>\n",
       "      <td>1.0</td>\n",
       "      <td>跟 儿子 在 里面 玩 了 一天 ， 非常 好 ！ 跟 儿子 在 里面 玩 了 一天 ， 非...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>这已经是第五次来这里玩了。每次孩子都很喜欢，不愿意从水里出来。有机会还会再来。还有比我更忠诚...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>这 已经 是 第五次 来 这里 玩 了 。 每次 孩子 都 很 喜欢 ， 不 愿意 从水里 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>当天在携程上定的票，打温泉度假村咨询电话和携程客服都说次日生效，但到酒店后，票能用。请客服人...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>当天 在 携程 上定 的 票 ， 打 温泉 度假村 咨询电话 和 携程 客服 都 说 次日 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>烟台历史的一部分，非常值得推荐去看看！海边景色也很漂亮！</td>\n",
       "      <td>1.0</td>\n",
       "      <td>烟台 历史 的 一部分 ， 非常 值得 推荐 去 看看 ！ 海边 景色 也 很漂亮 ！</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         COMMCONTENT  COMMLEVEL  \\\n",
       "0  普通公园一个只是多了几个泉而已，人不多，适合老人孩子闲逛，买票的话还是贵了，人家说6.30之...        1.0   \n",
       "1         跟儿子在里面玩了一天，非常好！跟儿子在里面玩了一天，非常好！真的很不错哦，有空还要去        1.0   \n",
       "2  这已经是第五次来这里玩了。每次孩子都很喜欢，不愿意从水里出来。有机会还会再来。还有比我更忠诚...        1.0   \n",
       "3  当天在携程上定的票，打温泉度假村咨询电话和携程客服都说次日生效，但到酒店后，票能用。请客服人...        1.0   \n",
       "4                       烟台历史的一部分，非常值得推荐去看看！海边景色也很漂亮！        1.0   \n",
       "\n",
       "                                     COMMCONTENT_SEG  \n",
       "0  普通 公园 一个 只是 多 了 几个 泉 而已 ， 人不多 ， 适合 老人 孩子 闲逛 ， ...  \n",
       "1  跟 儿子 在 里面 玩 了 一天 ， 非常 好 ！ 跟 儿子 在 里面 玩 了 一天 ， 非...  \n",
       "2  这 已经 是 第五次 来 这里 玩 了 。 每次 孩子 都 很 喜欢 ， 不 愿意 从水里 ...  \n",
       "3  当天 在 携程 上定 的 票 ， 打 温泉 度假村 咨询电话 和 携程 客服 都 说 次日 ...  \n",
       "4        烟台 历史 的 一部分 ， 非常 值得 推荐 去 看看 ！ 海边 景色 也 很漂亮 ！  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 将分词后的数据并入 df_all_dataset\n",
    "df_all_dataset['COMMCONTENT_SEG'] = pd.DataFrame(seged_text,columns=['COMMCONTENT_SEG'])\n",
    "df_all_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_corpus\n",
    "text_corpus = df_all_dataset['COMMCONTENT_SEG']\n",
    "# 传入我们词向量的字典\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS) \n",
    "# 传入我们的训练数据，得到训练数据中出现的词的字典\n",
    "tokenizer.fit_on_texts(text_corpus) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 根据训练数据中出现的词的字典，将训练数据转换为sequences\n",
    "dataset_sequences = tokenizer.texts_to_sequences(text_corpus) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 100134 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(122024, 150)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_dataset_sequences = pad_sequences(dataset_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "padded_dataset_sequences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_dataset_seg = df_all_dataset['COMMCONTENT_SEG'][20000:]\n",
    "test_dataset_sequences = tokenizer.texts_to_sequences(df_test_dataset_seg)\n",
    "padded_test_dataset_sequences = pad_sequences(test_dataset_sequences, maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 划分训练集和测试集\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_X,valid_X,train_y,valid_y =train_test_split(padded_dataset_sequences[:df_train_dataset.shape[0]], \n",
    "                                                  df_all_dataset['COMMLEVEL'][:df_train_dataset.shape[0]], \n",
    "                                                  test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label one-hot 表示\n",
    "labels = df_all_dataset['COMMLEVEL'].dropna().map(int)#.values.tolist()\n",
    "labels = to_categorical(labels-1) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80002"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab,vocab_freqs = build_vocab(df_all_dataset['COMMCONTENT_SEG'])\n",
    "vocab_size = min(MAX_NB_WORDS, len(vocab_freqs)) + 2\n",
    "word2index = {x[0]: i+2 for i, x in enumerate(vocab_freqs.most_common(MAX_NB_WORDS))}\n",
    "word2index[\"PAD\"] = 0\n",
    "word2index[\"UNK\"] = 1\n",
    "index2word = {v:k for k, v in word2index.items()}\n",
    "len(word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing word embeddings.\n",
      "word embedding 195201\n"
     ]
    }
   ],
   "source": [
    "print('Indexing word embeddings.')  \n",
    "embeddings_index = {}\n",
    "with open('./embeddings/sgns.weibo.word','r') as f:\n",
    "    f = f.readlines()\n",
    "    for i in f[1:]:\n",
    "        values = i.strip().split(' ')\n",
    "        word = str(values[0])\n",
    "        embedding = np.asarray(values[1:],dtype='float')\n",
    "        embeddings_index[word] = embedding\n",
    "print('word embedding',len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80000"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_words = min(MAX_NB_WORDS,len(word2index))\n",
    "nb_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embedding_matrix = np.zeros((nb_words + 1, EMBEDDING_DIM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word, i in word2index.items():\n",
    "    if i > MAX_NB_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(str(word).upper())\n",
    "    if embedding_vector is not None:\n",
    "        word_embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_embedding_layer =  Embedding(input_dim = nb_words+1, \n",
    "                             output_dim = EMBEDDING_DIM, \n",
    "                            weights=[word_embedding_matrix], \n",
    "                             input_length=MAX_SEQUENCE_LENGTH, \n",
    "                             mask_zero=True,\n",
    "                             trainable=False\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Lambda\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rnn_cnn_model():\n",
    "    embedding_dim = 300\n",
    "    inp = Input(shape=(MAX_SEQUENCE_LENGTH,))\n",
    "    x =  Embedding(input_dim = nb_words+1, \n",
    "                             output_dim = EMBEDDING_DIM, \n",
    "                             weights=[word_embedding_matrix], \n",
    "                             input_length=MAX_SEQUENCE_LENGTH, \n",
    "                             mask_zero=False,\n",
    "                             trainable=True\n",
    "                            )(inp)\n",
    "    x = SpatialDropout1D(0.3)(x)\n",
    "    x = Bidirectional(GRU(100, return_sequences=True))(x)\n",
    "    x = Conv1D(64, kernel_size=3, padding=\"valid\", kernel_initializer=\"he_uniform\")(x)\n",
    "    avg_pool = GlobalAveragePooling1D()(x)\n",
    "    max_pool = GlobalMaxPooling1D()(x)\n",
    "    kmax_pool = Lambda(lambda x: K.max(x, axis=1), output_shape=(64,))(x)\n",
    "    conc = concatenate([avg_pool, max_pool, kmax_pool])\n",
    "    outp = Dense(3, activation=\"softmax\")(conc)\n",
    "\n",
    "    model = Model(inputs=inp, outputs=outp)\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_cnn_model = get_rnn_cnn_model()\n",
    "\n",
    "plot_model(rnn_cnn_model, to_file='./rnn_cnn_model.png', \n",
    "show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!['./rnn_cnn_model.png'](./rnn_cnn_model.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 18000 samples, validate on 2000 samples\n",
      "Epoch 1/4\n",
      "18000/18000 [==============================] - 111s 6ms/step - loss: 0.6449 - acc: 0.7257 - val_loss: 0.7952 - val_acc: 0.6540\n",
      "Epoch 2/4\n",
      "18000/18000 [==============================] - 167s 9ms/step - loss: 0.6247 - acc: 0.7363 - val_loss: 0.7994 - val_acc: 0.6590\n",
      "Epoch 3/4\n",
      "18000/18000 [==============================] - 162s 9ms/step - loss: 0.6102 - acc: 0.7383 - val_loss: 0.7966 - val_acc: 0.6550\n",
      "Epoch 4/4\n",
      "18000/18000 [==============================] - 136s 8ms/step - loss: 0.5824 - acc: 0.7567 - val_loss: 0.8132 - val_acc: 0.6460\n"
     ]
    }
   ],
   "source": [
    "batch_size = 256\n",
    "epochs = 4\n",
    "\n",
    "# model.fit(x=train_X, y=to_categorical(train_y-1, num_classes=3), \n",
    "#                     validation_data=(valid_X, to_categorical(valid_y-1, num_classes=3)[:]), \n",
    "#                     batch_size=batch_size, \n",
    "#                     epochs=epochs,\n",
    "#           shuffle=True,\n",
    "#                     verbose=1\n",
    "#          )\n",
    "history = rnn_cnn_model.fit(x=train_X, y=to_categorical(train_y-1, num_classes=3), \n",
    "                    validation_data=(valid_X, to_categorical(valid_y-1, num_classes=3)[:]), \n",
    "                    batch_size=batch_size, \n",
    "                    epochs=epochs,\n",
    "                    shuffle=True,\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    0.386429\n",
       "1    0.352074\n",
       "3    0.261497\n",
       "dtype: float64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_test_preds = rnn_cnn_model.predict(padded_test_dataset_sequences, batch_size=1024)\n",
    "w2v7 = np.argmax(all_test_preds,axis=1)[:]+1\n",
    "pd.Series(w2v7).value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 18000 samples, validate on 2000 samples\n",
      "Epoch 1/4\n",
      "18000/18000 [==============================] - 161s 9ms/step - loss: 0.9939 - acc: 0.4934 - val_loss: 0.8558 - val_acc: 0.6075\n",
      "Epoch 2/4\n",
      "18000/18000 [==============================] - 151s 8ms/step - loss: 0.7919 - acc: 0.6446 - val_loss: 0.7624 - val_acc: 0.6525\n",
      "Epoch 3/4\n",
      "18000/18000 [==============================] - 151s 8ms/step - loss: 0.6897 - acc: 0.6982 - val_loss: 0.7643 - val_acc: 0.6560\n",
      "Epoch 4/4\n",
      "18000/18000 [==============================] - 167s 9ms/step - loss: 0.6004 - acc: 0.7483 - val_loss: 0.7472 - val_acc: 0.6675\n"
     ]
    }
   ],
   "source": [
    "# tainable = True\n",
    "batch_size = 256\n",
    "epochs = 4\n",
    "\n",
    "# model.fit(x=train_X, y=to_categorical(train_y-1, num_classes=3), \n",
    "#                     validation_data=(valid_X, to_categorical(valid_y-1, num_classes=3)[:]), \n",
    "#                     batch_size=batch_size, \n",
    "#                     epochs=epochs,\n",
    "#           shuffle=True,\n",
    "#                     verbose=1\n",
    "#          )\n",
    "history = rnn_cnn_model.fit(x=train_X, y=to_categorical(train_y-1, num_classes=3), \n",
    "                    validation_data=(valid_X, to_categorical(valid_y-1, num_classes=3)[:]), \n",
    "                    batch_size=batch_size, \n",
    "                    epochs=epochs,\n",
    "                    shuffle=True,\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_test_preds = rnn_cnn_model.predict(padded_test_dataset_sequences, batch_size=1024)\n",
    "w2v8 = np.argmax(all_test_preds,axis=1)[:]+1\n",
    "pd.Series(w2v8).value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add kmax pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 18000 samples, validate on 2000 samples\n",
      "Epoch 1/4\n",
      "18000/18000 [==============================] - 165s 9ms/step - loss: 0.9959 - acc: 0.4907 - val_loss: 0.8695 - val_acc: 0.5880\n",
      "Epoch 2/4\n",
      "18000/18000 [==============================] - 159s 9ms/step - loss: 0.7919 - acc: 0.6396 - val_loss: 0.7781 - val_acc: 0.6475\n",
      "Epoch 3/4\n",
      "18000/18000 [==============================] - 162s 9ms/step - loss: 0.6880 - acc: 0.7008 - val_loss: 0.7498 - val_acc: 0.6695\n",
      "Epoch 4/4\n",
      "18000/18000 [==============================] - 156s 9ms/step - loss: 0.5987 - acc: 0.7523 - val_loss: 0.7466 - val_acc: 0.6755\n"
     ]
    }
   ],
   "source": [
    "batch_size = 256\n",
    "epochs = 4\n",
    "\n",
    "# model.fit(x=train_X, y=to_categorical(train_y-1, num_classes=3), \n",
    "#                     validation_data=(valid_X, to_categorical(valid_y-1, num_classes=3)[:]), \n",
    "#                     batch_size=batch_size, \n",
    "#                     epochs=epochs,\n",
    "#           shuffle=True,\n",
    "#                     verbose=1\n",
    "#          )\n",
    "history = rnn_cnn_model.fit(x=train_X, y=to_categorical(train_y-1, num_classes=3), \n",
    "                    validation_data=(valid_X, to_categorical(valid_y-1, num_classes=3)[:]), \n",
    "                    batch_size=batch_size, \n",
    "                    epochs=epochs,\n",
    "                    shuffle=True,\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 18000 samples, validate on 2000 samples\n",
      "Epoch 1/1\n",
      "18000/18000 [==============================] - 158s 9ms/step - loss: 0.4216 - acc: 0.8386 - val_loss: 0.8321 - val_acc: 0.6685\n"
     ]
    }
   ],
   "source": [
    "batch_size = 256\n",
    "epochs = 1\n",
    "\n",
    "# model.fit(x=train_X, y=to_categorical(train_y-1, num_classes=3), \n",
    "#                     validation_data=(valid_X, to_categorical(valid_y-1, num_classes=3)[:]), \n",
    "#                     batch_size=batch_size, \n",
    "#                     epochs=epochs,\n",
    "#           shuffle=True,\n",
    "#                     verbose=1\n",
    "#          )\n",
    "history = rnn_cnn_model.fit(x=train_X, y=to_categorical(train_y-1, num_classes=3), \n",
    "                    validation_data=(valid_X, to_categorical(valid_y-1, num_classes=3)[:]), \n",
    "                    batch_size=batch_size, \n",
    "                    epochs=epochs,\n",
    "                    shuffle=True,\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_test_preds = rnn_cnn_model.predict(padded_test_dataset_sequences, batch_size=256)\n",
    "rnn_cnn_k_max = np.argmax(all_test_preds,axis=1)[:]+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.397642\n",
       "3    0.361670\n",
       "2    0.240688\n",
       "dtype: float64"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(rnn_cnn_k_max).value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"rnn-cnn-0.6685-result.txt\", rnn_cnn_k_max,fmt=\"%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 18000 samples, validate on 2000 samples\n",
      "Epoch 1/5\n",
      "18000/18000 [==============================] - 160s 9ms/step - loss: 1.0040 - acc: 0.4832 - val_loss: 0.8355 - val_acc: 0.6260\n",
      "Epoch 2/5\n",
      "18000/18000 [==============================] - 156s 9ms/step - loss: 0.7918 - acc: 0.6458 - val_loss: 0.7754 - val_acc: 0.6505\n",
      "Epoch 3/5\n",
      "18000/18000 [==============================] - 156s 9ms/step - loss: 0.6891 - acc: 0.7009 - val_loss: 0.7527 - val_acc: 0.6635\n",
      "Epoch 4/5\n",
      "18000/18000 [==============================] - 152s 8ms/step - loss: 0.5947 - acc: 0.7536 - val_loss: 0.7569 - val_acc: 0.6740\n",
      "Epoch 5/5\n",
      "18000/18000 [==============================] - 154s 9ms/step - loss: 0.5116 - acc: 0.7956 - val_loss: 0.7846 - val_acc: 0.6770\n"
     ]
    }
   ],
   "source": [
    "batch_size = 256\n",
    "epochs = 5\n",
    "\n",
    "# model.fit(x=train_X, y=to_categorical(train_y-1, num_classes=3), \n",
    "#                     validation_data=(valid_X, to_categorical(valid_y-1, num_classes=3)[:]), \n",
    "#                     batch_size=batch_size, \n",
    "#                     epochs=epochs,\n",
    "#           shuffle=True,\n",
    "#                     verbose=1\n",
    "#          )\n",
    "history = rnn_cnn_model.fit(x=train_X, y=to_categorical(train_y-1, num_classes=3), \n",
    "                    validation_data=(valid_X, to_categorical(valid_y-1, num_classes=3)[:]), \n",
    "                    batch_size=batch_size, \n",
    "                    epochs=epochs,\n",
    "                    shuffle=True,\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.377529\n",
       "3    0.369825\n",
       "2    0.252646\n",
       "dtype: float64"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_test_preds = rnn_cnn_model.predict(padded_test_dataset_sequences, batch_size=256)\n",
    "rnn_cnn_k_max2 = np.argmax(all_test_preds,axis=1)[:]+1\n",
    "pd.Series(rnn_cnn_k_max2).value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"rnn-cnn-0.6770-result.txt\", rnn_cnn_k_max2,fmt=\"%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_test_preds = rnn_cnn_model.predict(padded_test_dataset_sequences, batch_size=256)\n",
    "w2v6 = np.argmax(all_test_preds,axis=1)[:]+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.423273\n",
       "3    0.330932\n",
       "2    0.245795\n",
       "dtype: float64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(w2v6).value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"rnn-cnn-0.6320-result.txt\", w2v6,fmt=\"%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(pre_embedding_layer)\n",
    "model.add(Bidirectional(GRU(256, dropout=0.2, recurrent_dropout=0.1, return_sequences=True)))\n",
    "model.add(Bidirectional(GRU(256, dropout=0.2, recurrent_dropout=0.1)))\n",
    "model.add(Dense(NUM_LABELS, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rnn_cnn_model():\n",
    "    embedding_dim = 300\n",
    "    inp = Input(shape=(MAX_SEQUENCE_LENGTH,))\n",
    "    x =  Embedding(input_dim = nb_words+1, \n",
    "                             output_dim = EMBEDDING_DIM, \n",
    "                             weights=[word_embedding_matrix], \n",
    "                             input_length=MAX_SEQUENCE_LENGTH, \n",
    "                             mask_zero=False,\n",
    "                             trainable=False\n",
    "                            )(inp)\n",
    "    x = SpatialDropout1D(0.3)(x)\n",
    "    x = Bidirectional(GRU(128, return_sequences=True,dropout=0.1,recurrent_dropout=0.1))(x)\n",
    "    x = Conv1D(64, kernel_size=3, padding=\"valid\", kernel_initializer=\"glorot_uniform\")(x)\n",
    "    avg_pool = GlobalAveragePooling1D()(x)\n",
    "#     max_pool = GlobalMaxPooling1D()(x)\n",
    "    kmax_pool = Lambda(lambda x: K.max(x, axis=1), output_shape=(64,))(x)\n",
    "    conc = concatenate([avg_pool, kmax_pool])\n",
    "    outp = Dense(3, activation=\"softmax\")(conc)\n",
    "\n",
    "    model = Model(inputs=inp, outputs=outp)\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "              #optimizer='adam',\n",
    "                  optimizer=Adam(lr=1e-3),\n",
    "              metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_cnn_model = get_rnn_cnn_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 18000 samples, validate on 2000 samples\n",
      "Epoch 1/5\n",
      "18000/18000 [==============================] - 170s 9ms/step - loss: 1.0061 - acc: 0.4828 - val_loss: 0.8898 - val_acc: 0.5995\n",
      "Epoch 2/5\n",
      "18000/18000 [==============================] - 154s 9ms/step - loss: 0.8893 - acc: 0.5828 - val_loss: 0.8448 - val_acc: 0.6200\n",
      "Epoch 3/5\n",
      "18000/18000 [==============================] - 159s 9ms/step - loss: 0.8430 - acc: 0.6107 - val_loss: 0.8212 - val_acc: 0.6430\n",
      "Epoch 4/5\n",
      "18000/18000 [==============================] - 174s 10ms/step - loss: 0.8132 - acc: 0.6263 - val_loss: 0.8134 - val_acc: 0.6410\n",
      "Epoch 5/5\n",
      "18000/18000 [==============================] - 153s 9ms/step - loss: 0.7819 - acc: 0.6469 - val_loss: 0.8271 - val_acc: 0.6380\n"
     ]
    }
   ],
   "source": [
    "batch_size = 256\n",
    "epochs = 5\n",
    "\n",
    "# model.fit(x=train_X, y=to_categorical(train_y-1, num_classes=3), \n",
    "#                     validation_data=(valid_X, to_categorical(valid_y-1, num_classes=3)[:]), \n",
    "#                     batch_size=batch_size, \n",
    "#                     epochs=epochs,\n",
    "#           shuffle=True,\n",
    "#                     verbose=1\n",
    "#          )\n",
    "history = rnn_cnn_model.fit(x=train_X, y=to_categorical(train_y-1, num_classes=3), \n",
    "                    validation_data=(valid_X, to_categorical(valid_y-1, num_classes=3)[:]), \n",
    "                    batch_size=batch_size, \n",
    "                    epochs=epochs,\n",
    "                    shuffle=True,\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 18000 samples, validate on 2000 samples\n",
      "Epoch 1/5\n",
      "18000/18000 [==============================] - 155s 9ms/step - loss: 1.0383 - acc: 0.4613 - val_loss: 0.9236 - val_acc: 0.5565\n",
      "Epoch 2/5\n",
      "18000/18000 [==============================] - 160s 9ms/step - loss: 0.9044 - acc: 0.5735 - val_loss: 0.8621 - val_acc: 0.6180\n",
      "Epoch 3/5\n",
      "18000/18000 [==============================] - 154s 9ms/step - loss: 0.8567 - acc: 0.5992 - val_loss: 0.8289 - val_acc: 0.6245\n",
      "Epoch 4/5\n",
      "  768/18000 [>.............................] - ETA: 2:27 - loss: 0.7897 - acc: 0.6510"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-74-9fe0de815133>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m                     \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m                     verbose=1)\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/TensorFlowKerasEnv/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1040\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1042\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1043\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1044\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/envs/TensorFlowKerasEnv/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/TensorFlowKerasEnv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2659\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2660\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2661\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2662\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2663\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/TensorFlowKerasEnv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2629\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2630\u001b[0m                                 session)\n\u001b[0;32m-> 2631\u001b[0;31m         \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2632\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/TensorFlowKerasEnv/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1449\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_with_new_api\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1450\u001b[0m           return tf_session.TF_SessionRunCallable(\n\u001b[0;32m-> 1451\u001b[0;31m               self._session._session, self._handle, args, status, None)\n\u001b[0m\u001b[1;32m   1452\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1453\u001b[0m           return tf_session.TF_DeprecatedSessionRunCallable(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 256\n",
    "epochs = 5\n",
    "\n",
    "# model.fit(x=train_X, y=to_categorical(train_y-1, num_classes=3), \n",
    "#                     validation_data=(valid_X, to_categorical(valid_y-1, num_classes=3)[:]), \n",
    "#                     batch_size=batch_size, \n",
    "#                     epochs=epochs,\n",
    "#           shuffle=True,\n",
    "#                     verbose=1\n",
    "#          )\n",
    "history = rnn_cnn_model.fit(x=train_X, y=to_categorical(train_y-1, num_classes=3), \n",
    "                    validation_data=(valid_X, to_categorical(valid_y-1, num_classes=3)[:]), \n",
    "                    batch_size=batch_size, \n",
    "                    epochs=epochs,\n",
    "                    shuffle=True,\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
