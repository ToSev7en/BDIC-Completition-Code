{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "from keras.models import load_model\n",
    "\n",
    "from keras.layers import Activation\n",
    "from keras.layers import Input, Dense, Embedding, Conv1D, Conv2D, MaxPooling1D, MaxPool2D\n",
    "from keras.layers import Reshape, Flatten, Dropout, Concatenate\n",
    "from keras.layers import SpatialDropout1D, concatenate\n",
    "from keras.layers import GRU, Bidirectional, GlobalAveragePooling1D, GlobalMaxPooling1D\n",
    "from keras.layers.recurrent import LSTM,SimpleRNN\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from keras.callbacks import Callback, ModelCheckpoint, EarlyStopping\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "from keras.models import load_model\n",
    "\n",
    "import jieba\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入自定义库\n",
    "from utils.data_utils import clean_str\n",
    "from utils.data_utils import build_vocab\n",
    "from utils.data_utils import get_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(102024, 2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 加载训练集合\n",
    "df_train_dataset = pd.read_csv('./data/training-inspur.csv', encoding='utf-8')\n",
    "# 加载测试集\n",
    "df_test_dataset = pd.read_csv('./data/Preliminary-texting-1.csv', encoding='utf-8')\n",
    "# 查看数据集合 shape\n",
    "df_test_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 提取数据集中所需的字段\n",
    "df_train_dataset = df_train_dataset[['COMMCONTENT', 'COMMLEVEL']]\n",
    "df_test_dataset = df_test_dataset[['COMMCONTENT']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(122024, 2)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 合并数据集用于构建词汇表\n",
    "df_all_dataset = pd.concat([df_train_dataset, df_test_dataset], ignore_index=True)\n",
    "df_all_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = '/Users/tsw/ScenicSpotReviews'\n",
    "\n",
    "W2V_DIR = BASE_DIR + '/embeddings/'\n",
    "\n",
    "TEXT_DATA_DIR = BASE_DIR + '/data/'\n",
    "\n",
    "MAX_NUM_WORDS = 33950\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 150 # 每篇文章选取150个词\n",
    "\n",
    "MAX_NB_WORDS = 80000 # 将字典设置为含有1万个词84480\n",
    "\n",
    "EMBEDDING_DIM = 300 # 词向量维度，300维\n",
    "\n",
    "VALIDATION_SPLIT = 0.1 # 测试集大小，全部数据的10%\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "NUM_LABELS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seg_corpus(corpus):\n",
    "    seg_corpus = []\n",
    "    for line in corpus:\n",
    "        line = str(line).strip()\n",
    "        seg_list = jieba.cut(line, cut_all=False)\n",
    "        # 过滤空字符\n",
    "        seg_list = [w for w in seg_list if w != ' ']\n",
    "        seg_corpus.append(\" \".join(seg_list))\n",
    "    return seg_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/5n/2_by50851fxc4d_snc1d9wf80000gn/T/jieba.cache\n",
      "Loading model cost 0.859 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "# 对所有文本分词\n",
    "seged_text = seg_corpus(df_all_dataset['COMMCONTENT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>COMMCONTENT</th>\n",
       "      <th>COMMLEVEL</th>\n",
       "      <th>COMMCONTENT_SEG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>普通公园一个只是多了几个泉而已，人不多，适合老人孩子闲逛，买票的话还是贵了，人家说6.30之...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>普通 公园 一个 只是 多 了 几个 泉 而已 ， 人不多 ， 适合 老人 孩子 闲逛 ， ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>跟儿子在里面玩了一天，非常好！跟儿子在里面玩了一天，非常好！真的很不错哦，有空还要去</td>\n",
       "      <td>1.0</td>\n",
       "      <td>跟 儿子 在 里面 玩 了 一天 ， 非常 好 ！ 跟 儿子 在 里面 玩 了 一天 ， 非...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>这已经是第五次来这里玩了。每次孩子都很喜欢，不愿意从水里出来。有机会还会再来。还有比我更忠诚...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>这 已经 是 第五次 来 这里 玩 了 。 每次 孩子 都 很 喜欢 ， 不 愿意 从水里 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>当天在携程上定的票，打温泉度假村咨询电话和携程客服都说次日生效，但到酒店后，票能用。请客服人...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>当天 在 携程 上定 的 票 ， 打 温泉 度假村 咨询电话 和 携程 客服 都 说 次日 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>烟台历史的一部分，非常值得推荐去看看！海边景色也很漂亮！</td>\n",
       "      <td>1.0</td>\n",
       "      <td>烟台 历史 的 一部分 ， 非常 值得 推荐 去 看看 ！ 海边 景色 也 很漂亮 ！</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         COMMCONTENT  COMMLEVEL  \\\n",
       "0  普通公园一个只是多了几个泉而已，人不多，适合老人孩子闲逛，买票的话还是贵了，人家说6.30之...        1.0   \n",
       "1         跟儿子在里面玩了一天，非常好！跟儿子在里面玩了一天，非常好！真的很不错哦，有空还要去        1.0   \n",
       "2  这已经是第五次来这里玩了。每次孩子都很喜欢，不愿意从水里出来。有机会还会再来。还有比我更忠诚...        1.0   \n",
       "3  当天在携程上定的票，打温泉度假村咨询电话和携程客服都说次日生效，但到酒店后，票能用。请客服人...        1.0   \n",
       "4                       烟台历史的一部分，非常值得推荐去看看！海边景色也很漂亮！        1.0   \n",
       "\n",
       "                                     COMMCONTENT_SEG  \n",
       "0  普通 公园 一个 只是 多 了 几个 泉 而已 ， 人不多 ， 适合 老人 孩子 闲逛 ， ...  \n",
       "1  跟 儿子 在 里面 玩 了 一天 ， 非常 好 ！ 跟 儿子 在 里面 玩 了 一天 ， 非...  \n",
       "2  这 已经 是 第五次 来 这里 玩 了 。 每次 孩子 都 很 喜欢 ， 不 愿意 从水里 ...  \n",
       "3  当天 在 携程 上定 的 票 ， 打 温泉 度假村 咨询电话 和 携程 客服 都 说 次日 ...  \n",
       "4        烟台 历史 的 一部分 ， 非常 值得 推荐 去 看看 ！ 海边 景色 也 很漂亮 ！  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 将分词后的数据并入 df_all_dataset\n",
    "df_all_dataset['COMMCONTENT_SEG'] = pd.DataFrame(seged_text,columns=['COMMCONTENT_SEG'])\n",
    "df_all_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_corpus\n",
    "text_corpus = df_all_dataset['COMMCONTENT_SEG']\n",
    "# 传入我们词向量的字典\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS) \n",
    "# 传入我们的训练数据，得到训练数据中出现的词的字典\n",
    "tokenizer.fit_on_texts(text_corpus) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 根据训练数据中出现的词的字典，将训练数据转换为sequences\n",
    "dataset_sequences = tokenizer.texts_to_sequences(text_corpus) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 100134 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(122024, 150)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_dataset_sequences = pad_sequences(dataset_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "padded_dataset_sequences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_dataset_seg = df_all_dataset['COMMCONTENT_SEG'][20000:]\n",
    "test_dataset_sequences = tokenizer.texts_to_sequences(df_test_dataset_seg)\n",
    "padded_test_dataset_sequences = pad_sequences(test_dataset_sequences, maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 划分训练集和测试集\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_X,valid_X,train_y,valid_y =train_test_split(padded_dataset_sequences[:df_train_dataset.shape[0]], \n",
    "                                                  df_all_dataset['COMMLEVEL'][:df_train_dataset.shape[0]], \n",
    "                                                  test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label one-hot 表示\n",
    "labels = df_all_dataset['COMMLEVEL'].dropna().map(int)#.values.tolist()\n",
    "labels = to_categorical(labels-1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80002"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab,vocab_freqs = build_vocab(df_all_dataset['COMMCONTENT_SEG'])\n",
    "vocab_size = min(MAX_NB_WORDS, len(vocab_freqs)) + 2\n",
    "word2index = {x[0]: i+2 for i, x in enumerate(vocab_freqs.most_common(MAX_NB_WORDS))}\n",
    "word2index[\"PAD\"] = 0\n",
    "word2index[\"UNK\"] = 1\n",
    "index2word = {v:k for k, v in word2index.items()}\n",
    "len(word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing word embeddings.\n",
      "word embedding 195201\n"
     ]
    }
   ],
   "source": [
    "import codecs\n",
    "print('Indexing word embeddings.')  \n",
    "embeddings_index = {}\n",
    "with codecs.open('./embeddings/sgns.weibo.word','r','utf-8') as f:\n",
    "    f = f.readlines()\n",
    "    for i in f[1:]:\n",
    "        values = i.strip().split(' ')\n",
    "        word = str(values[0])\n",
    "        embedding = np.asarray(values[1:],dtype='float')\n",
    "        embeddings_index[word] = embedding\n",
    "print('word embedding',len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.186542,  0.153161, -0.092138, -0.409595, -0.277637,  0.32679 ,\n",
       "        0.460779, -0.290725, -0.11773 , -0.026282,  0.306992, -0.241884,\n",
       "       -0.131621, -0.072939, -0.353897,  0.325635, -0.245221,  0.192655,\n",
       "        0.491776, -0.038478,  0.172667, -0.099799, -0.022893,  0.421129,\n",
       "       -0.021248,  0.113363, -0.240293, -0.269463,  0.262599,  0.059695,\n",
       "       -0.068543, -0.164919, -0.236679, -0.12863 ,  0.009809,  0.025645,\n",
       "       -0.272379, -0.154907, -0.161305, -0.176863,  0.377503,  0.223636,\n",
       "       -0.387001, -0.244671,  0.41847 , -0.04869 ,  0.067996,  0.012222,\n",
       "       -0.035722, -0.052362, -0.650677,  0.100913, -0.202876, -0.612033,\n",
       "        0.438661,  0.193497, -0.267914, -0.278571, -0.292877, -0.049786,\n",
       "        0.236615,  0.059674,  0.245647, -0.156111,  0.307591, -0.11428 ,\n",
       "       -0.322858, -0.481675, -0.14655 ,  0.519312, -0.155763,  0.156163,\n",
       "        0.01132 , -0.157673, -0.117068, -0.556283,  0.487568, -0.175978,\n",
       "        0.105386, -0.092337, -0.262746,  0.513361,  0.221847,  0.207966,\n",
       "       -0.101157, -0.658571, -0.055578, -0.305677, -0.123846, -0.027605,\n",
       "       -0.22905 ,  0.304842,  0.276181, -0.098963, -0.338204,  0.118403,\n",
       "        0.039357, -0.215989,  0.218499, -0.29857 , -0.15877 ,  0.313112,\n",
       "       -0.178787,  0.043225, -0.056945, -0.220683, -0.038965, -0.382084,\n",
       "        0.043113, -0.17092 ,  0.28547 ,  0.28078 ,  0.131956, -0.318758,\n",
       "       -0.18066 , -0.172934,  0.554534,  0.171541, -0.336901, -0.218235,\n",
       "       -0.099939, -0.085401, -0.084318,  0.239298, -0.056081, -0.35254 ,\n",
       "        0.272402, -0.624044, -0.072284,  0.123875,  0.091384, -0.075948,\n",
       "        0.316214,  0.435909, -0.405805, -0.078294,  0.249417, -0.177704,\n",
       "        0.311815,  0.547678, -0.038453,  0.113031, -0.016061, -0.502175,\n",
       "       -0.200813,  0.287193,  0.387152,  0.220599,  0.141931, -0.430183,\n",
       "        0.135963,  0.375786, -0.02326 ,  0.202768,  0.17746 , -0.013933,\n",
       "       -0.00458 , -0.085091, -0.232387, -0.216109, -0.119622,  0.498833,\n",
       "        0.259414,  0.424168, -0.410326,  0.110454,  0.09454 ,  0.482673,\n",
       "       -0.386016, -0.093817,  0.209817,  0.183858,  0.127696,  0.081777,\n",
       "        0.089483, -0.258859,  0.099303,  0.00612 ,  0.136222,  0.325763,\n",
       "        0.193566,  0.105741, -0.429904,  0.214485,  0.214182, -0.262945,\n",
       "        0.033062,  0.029283, -0.205776,  0.054114, -0.110412,  0.253166,\n",
       "        0.357556,  0.258397, -0.198831, -0.263028,  0.349537,  0.116031,\n",
       "        0.127722, -0.014469, -0.112248, -0.31539 , -0.582305, -0.112283,\n",
       "        0.05113 ,  0.083036,  0.198855,  0.331975, -0.132747, -0.270296,\n",
       "       -0.190807,  0.133632,  0.416707, -0.321506, -0.216177,  0.224395,\n",
       "       -0.048179, -0.340167, -0.722469,  0.193065, -0.007463,  0.295169,\n",
       "       -0.548366, -0.103934,  0.290824,  0.365923, -0.401075, -0.43174 ,\n",
       "       -0.045268, -0.086799,  0.070361,  0.369335, -0.466064, -0.128541,\n",
       "        0.094096,  0.17215 ,  0.201017,  0.215586,  0.09929 , -0.339092,\n",
       "        0.484344, -0.507536, -0.482544, -0.477994,  0.087186, -0.244745,\n",
       "       -0.174335, -0.158854,  0.194269, -0.477995, -0.238662,  0.30574 ,\n",
       "        0.168578, -0.276158,  0.199264,  0.302587,  0.153977,  0.09388 ,\n",
       "        0.233261, -0.741296, -0.17742 ,  0.007087,  0.476249,  0.254905,\n",
       "       -0.049342, -0.010848,  0.600524, -0.03712 ,  0.396826,  0.228229,\n",
       "       -0.175323, -0.093165,  0.016571, -0.224082, -0.092199, -0.47859 ,\n",
       "       -0.193525,  0.214239, -0.131165, -0.19732 , -0.298881,  0.358156,\n",
       "       -0.207846, -0.328151,  0.386076,  0.239587, -0.002729, -0.033819,\n",
       "        0.030327,  0.078348,  0.168712, -0.204893, -0.419589,  0.057267,\n",
       "       -0.181735, -0.207478, -0.133524, -0.010901, -0.344588,  0.269266])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_index['中国']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80000"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_words = min(MAX_NB_WORDS,len(word2index))\n",
    "nb_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embedding_matrix = np.zeros((nb_words + 1, EMBEDDING_DIM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word, i in word2index.items():\n",
    "    if i > MAX_NB_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(str(word).upper())\n",
    "    if embedding_vector is not None:\n",
    "        word_embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre_embedding_layer =  Embedding(input_dim = nb_words+1, \n",
    "#                              output_dim = EMBEDDING_DIM, \n",
    "#                             weights=[word_embedding_matrix], \n",
    "#                              input_length=MAX_SEQUENCE_LENGTH, \n",
    "#                              mask_zero=True,\n",
    "#                              trainable=False\n",
    "#                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Lambda,BatchNormalization\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rnn_cnn_model():\n",
    "    embedding_dim = 300\n",
    "    inp = Input(shape=(MAX_SEQUENCE_LENGTH,))\n",
    "    x =  Embedding(input_dim = nb_words+1, \n",
    "                             output_dim = EMBEDDING_DIM, \n",
    "                             weights=[word_embedding_matrix], \n",
    "                             input_length=MAX_SEQUENCE_LENGTH, \n",
    "                             mask_zero=False,\n",
    "                             trainable=True\n",
    "                            )(inp)\n",
    "    x = SpatialDropout1D(0.3)(x) # 0.3 - 0.6970\n",
    "    x = Bidirectional(GRU(80, return_sequences=True))(x) # 80-0.6970\n",
    "    \n",
    "    x1 = Conv1D(32, kernel_size=2, padding=\"valid\", kernel_initializer=\"he_uniform\")(x)\n",
    "    avg_pool = GlobalAveragePooling1D()(x1)\n",
    "    max_pool = GlobalMaxPooling1D()(x1)\n",
    "    kmax_pool = Lambda(lambda x: K.max(x, axis=1), output_shape=(32,))(x1)\n",
    "    conc1 = concatenate([avg_pool, kmax_pool])\n",
    "    \n",
    "    x2 = Conv1D(32, kernel_size=3, padding=\"valid\", kernel_initializer=\"he_uniform\")(x)\n",
    "    avg_pool2 = GlobalAveragePooling1D()(x2)\n",
    "    max_pool2 = GlobalMaxPooling1D()(x2)\n",
    "    kmax_pool2 = Lambda(lambda x: K.max(x, axis=1), output_shape=(32,))(x2)\n",
    "    conc2 = concatenate([avg_pool2, kmax_pool2])\n",
    "    \n",
    "    #x3 = Conv1D(64, kernel_size=4, padding=\"valid\", kernel_initializer=\"he_uniform\")(x)\n",
    "    #avg_pool3 = GlobalAveragePooling1D()(x3)\n",
    "    #max_pool3 = GlobalMaxPooling1D()(x3)\n",
    "    #kmax_pool3 = Lambda(lambda x: K.max(x, axis=1), output_shape=(64,))(x3)\n",
    "    #conc3 = concatenate([avg_pool3, max_pool3, kmax_pool3])\n",
    "    \n",
    "    merge = concatenate([conc1, conc2])\n",
    "    \n",
    "    drop_merge = Dropout(0.25)(merge)\n",
    "    \n",
    "    #drop_merge = BatchNormalization()(drop_merge)\n",
    "    \n",
    "    #drop_merge = Dense(300, activation=\"relu\")(drop_merge)\n",
    "    \n",
    "    \n",
    "    #drop_merge = Dropout(0.2)(drop_merge)\n",
    "    #drop_merge = BatchNormalization()(drop_merge)\n",
    "    outp = Dense(3, activation=\"softmax\")(drop_merge)\n",
    "\n",
    "    model = Model(inputs=inp, outputs=outp)\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_cnn_model = get_rnn_cnn_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(rnn_cnn_model, to_file='./best_model_layer_name.png', \n",
    "show_shapes=False, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!['./best_model.png'](./rnn_cnn_model.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"./checkpoints/rnn-cnn-fasttext/weights-improvement-{epoch:02d}-{val_acc:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True,mode='max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 18000 samples, validate on 2000 samples\n",
      "Epoch 1/15\n",
      "18000/18000 [==============================] - 176s 10ms/step - loss: 1.0272 - acc: 0.4658 - val_loss: 0.8537 - val_acc: 0.6155\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.61550, saving model to ./checkpoints/rnn-cnn-fasttext/weights-improvement-01-0.6155.hdf5\n",
      "Epoch 2/15\n",
      "18000/18000 [==============================] - 179s 10ms/step - loss: 0.8357 - acc: 0.6154 - val_loss: 0.7642 - val_acc: 0.6745\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.61550 to 0.67450, saving model to ./checkpoints/rnn-cnn-fasttext/weights-improvement-02-0.6745.hdf5\n",
      "Epoch 3/15\n",
      "18000/18000 [==============================] - 177s 10ms/step - loss: 0.7383 - acc: 0.6732 - val_loss: 0.7267 - val_acc: 0.6875\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.67450 to 0.68750, saving model to ./checkpoints/rnn-cnn-fasttext/weights-improvement-03-0.6875.hdf5\n",
      "Epoch 4/15\n",
      "18000/18000 [==============================] - 171s 10ms/step - loss: 0.6545 - acc: 0.7179 - val_loss: 0.7310 - val_acc: 0.6950\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.68750 to 0.69500, saving model to ./checkpoints/rnn-cnn-fasttext/weights-improvement-04-0.6950.hdf5\n",
      "Epoch 5/15\n",
      "18000/18000 [==============================] - 171s 9ms/step - loss: 0.5732 - acc: 0.7637 - val_loss: 0.7414 - val_acc: 0.6855\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.69500\n",
      "Epoch 6/15\n",
      "18000/18000 [==============================] - 172s 10ms/step - loss: 0.4896 - acc: 0.8053 - val_loss: 0.7862 - val_acc: 0.6705\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.69500\n",
      "Epoch 7/15\n",
      "18000/18000 [==============================] - 172s 10ms/step - loss: 0.4136 - acc: 0.8423 - val_loss: 0.8680 - val_acc: 0.6470\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.69500\n"
     ]
    }
   ],
   "source": [
    "batch_size = 256\n",
    "epochs = 15\n",
    "\n",
    "history = rnn_cnn_model.fit(x=train_X, y=to_categorical(train_y-1, num_classes=3), \n",
    "                    validation_data=(valid_X, to_categorical(valid_y-1, num_classes=3)[:]), \n",
    "                    batch_size=batch_size, \n",
    "                    epochs=epochs,\n",
    "                    shuffle=True,\n",
    "                    callbacks = [EarlyStopping(monitor='val_acc', patience=3, mode='auto'),checkpoint],\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.452207\n",
       "3    0.287256\n",
       "2    0.260537\n",
       "dtype: float64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_rnn_cnn_model = load_model('./checkpoints/rnn-cnn-fasttext/weights-improvement-04-0.6950.hdf5')\n",
    "\n",
    "y_pred_rnn_cnn = best_rnn_cnn_model.predict(padded_test_dataset_sequences, batch_size=1024)\n",
    "\n",
    "pooled_gru_conv_model_preds = np.argmax(y_pred_rnn_cnn,axis=1)[:]+1\n",
    "\n",
    "pd.Series(pooled_gru_conv_model_preds).value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"weights-improvement-04-0.6950.txt\", pooled_gru_conv_model_preds,fmt=\"%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
