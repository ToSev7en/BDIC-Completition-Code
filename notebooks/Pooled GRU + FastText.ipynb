{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "from keras.models import load_model\n",
    "\n",
    "from keras.layers import Activation\n",
    "from keras.layers import Input, Dense, Embedding, Conv1D, Conv2D, MaxPooling1D, MaxPool2D\n",
    "from keras.layers import Reshape, Flatten, Dropout, Concatenate\n",
    "from keras.layers import SpatialDropout1D, concatenate\n",
    "from keras.layers import GRU, Bidirectional, GlobalAveragePooling1D, GlobalMaxPooling1D\n",
    "from keras.layers.recurrent import LSTM,SimpleRNN\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from keras.callbacks import Callback, ModelCheckpoint, EarlyStopping\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "import jieba\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入自定义库\n",
    "from utils.data_utils import clean_str\n",
    "from utils.data_utils import build_vocab\n",
    "from utils.data_utils import get_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(102024, 2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 加载训练集合\n",
    "df_train_dataset = pd.read_csv('./data/training-inspur.csv', encoding='utf-8')\n",
    "# 加载测试集\n",
    "df_test_dataset = pd.read_csv('./data/Preliminary-texting-1.csv', encoding='utf-8')\n",
    "# 查看数据集合 shape\n",
    "df_test_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 提取数据集中所需的字段\n",
    "df_train_dataset = df_train_dataset[['COMMCONTENT', 'COMMLEVEL']]\n",
    "df_test_dataset = df_test_dataset[['COMMCONTENT']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(122024, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 合并数据集用于构建词汇表\n",
    "df_all_dataset = pd.concat([df_train_dataset, df_test_dataset], ignore_index=True)\n",
    "df_all_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = '/Users/tsw/ScenicSpotReviews'\n",
    "\n",
    "W2V_DIR = BASE_DIR + '/embeddings/'\n",
    "\n",
    "TEXT_DATA_DIR = BASE_DIR + '/data/'\n",
    "\n",
    "MAX_NUM_WORDS = 33950\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 150 # 每篇文章选取150个词\n",
    "\n",
    "MAX_NB_WORDS = 80000 # 将字典设置为含有1万个词84480\n",
    "\n",
    "EMBEDDING_DIM = 300 # 词向量维度，300维\n",
    "\n",
    "VALIDATION_SPLIT = 0.1 # 测试集大小，全部数据的10%\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "NUM_LABELS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seg_corpus(corpus):\n",
    "    seg_corpus = []\n",
    "    for line in corpus:\n",
    "        line = str(line).strip()\n",
    "        seg_list = jieba.cut(line, cut_all=False)\n",
    "        # 过滤空字符\n",
    "        seg_list = [w for w in seg_list if w != ' ']\n",
    "        seg_corpus.append(\" \".join(seg_list))\n",
    "    return seg_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/5n/2_by50851fxc4d_snc1d9wf80000gn/T/jieba.cache\n",
      "Loading model cost 0.868 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "# 对所有文本分词\n",
    "seged_text = seg_corpus(df_all_dataset['COMMCONTENT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>COMMCONTENT</th>\n",
       "      <th>COMMLEVEL</th>\n",
       "      <th>COMMCONTENT_SEG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>普通公园一个只是多了几个泉而已，人不多，适合老人孩子闲逛，买票的话还是贵了，人家说6.30之...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>普通 公园 一个 只是 多 了 几个 泉 而已 ， 人不多 ， 适合 老人 孩子 闲逛 ， ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>跟儿子在里面玩了一天，非常好！跟儿子在里面玩了一天，非常好！真的很不错哦，有空还要去</td>\n",
       "      <td>1.0</td>\n",
       "      <td>跟 儿子 在 里面 玩 了 一天 ， 非常 好 ！ 跟 儿子 在 里面 玩 了 一天 ， 非...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>这已经是第五次来这里玩了。每次孩子都很喜欢，不愿意从水里出来。有机会还会再来。还有比我更忠诚...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>这 已经 是 第五次 来 这里 玩 了 。 每次 孩子 都 很 喜欢 ， 不 愿意 从水里 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>当天在携程上定的票，打温泉度假村咨询电话和携程客服都说次日生效，但到酒店后，票能用。请客服人...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>当天 在 携程 上定 的 票 ， 打 温泉 度假村 咨询电话 和 携程 客服 都 说 次日 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>烟台历史的一部分，非常值得推荐去看看！海边景色也很漂亮！</td>\n",
       "      <td>1.0</td>\n",
       "      <td>烟台 历史 的 一部分 ， 非常 值得 推荐 去 看看 ！ 海边 景色 也 很漂亮 ！</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         COMMCONTENT  COMMLEVEL  \\\n",
       "0  普通公园一个只是多了几个泉而已，人不多，适合老人孩子闲逛，买票的话还是贵了，人家说6.30之...        1.0   \n",
       "1         跟儿子在里面玩了一天，非常好！跟儿子在里面玩了一天，非常好！真的很不错哦，有空还要去        1.0   \n",
       "2  这已经是第五次来这里玩了。每次孩子都很喜欢，不愿意从水里出来。有机会还会再来。还有比我更忠诚...        1.0   \n",
       "3  当天在携程上定的票，打温泉度假村咨询电话和携程客服都说次日生效，但到酒店后，票能用。请客服人...        1.0   \n",
       "4                       烟台历史的一部分，非常值得推荐去看看！海边景色也很漂亮！        1.0   \n",
       "\n",
       "                                     COMMCONTENT_SEG  \n",
       "0  普通 公园 一个 只是 多 了 几个 泉 而已 ， 人不多 ， 适合 老人 孩子 闲逛 ， ...  \n",
       "1  跟 儿子 在 里面 玩 了 一天 ， 非常 好 ！ 跟 儿子 在 里面 玩 了 一天 ， 非...  \n",
       "2  这 已经 是 第五次 来 这里 玩 了 。 每次 孩子 都 很 喜欢 ， 不 愿意 从水里 ...  \n",
       "3  当天 在 携程 上定 的 票 ， 打 温泉 度假村 咨询电话 和 携程 客服 都 说 次日 ...  \n",
       "4        烟台 历史 的 一部分 ， 非常 值得 推荐 去 看看 ！ 海边 景色 也 很漂亮 ！  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 将分词后的数据并入 df_all_dataset\n",
    "df_all_dataset['COMMCONTENT_SEG'] = pd.DataFrame(seged_text,columns=['COMMCONTENT_SEG'])\n",
    "df_all_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_corpus\n",
    "text_corpus = df_all_dataset['COMMCONTENT_SEG']\n",
    "# 传入我们词向量的字典\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS) \n",
    "# 传入我们的训练数据，得到训练数据中出现的词的字典\n",
    "tokenizer.fit_on_texts(text_corpus) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 根据训练数据中出现的词的字典，将训练数据转换为sequences\n",
    "dataset_sequences = tokenizer.texts_to_sequences(text_corpus) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 100134 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(122024, 150)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_dataset_sequences = pad_sequences(dataset_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "padded_dataset_sequences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_dataset_seg = df_all_dataset['COMMCONTENT_SEG'][20000:]\n",
    "test_dataset_sequences = tokenizer.texts_to_sequences(df_test_dataset_seg)\n",
    "padded_test_dataset_sequences = pad_sequences(test_dataset_sequences, maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 划分训练集和测试集\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_X,valid_X,train_y,valid_y =train_test_split(padded_dataset_sequences[:df_train_dataset.shape[0]], \n",
    "                                                  df_all_dataset['COMMLEVEL'][:df_train_dataset.shape[0]], \n",
    "                                                  test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label one-hot 表示\n",
    "labels = df_all_dataset['COMMLEVEL'].dropna().map(int)#.values.tolist()\n",
    "labels = to_categorical(labels-1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80002"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab,vocab_freqs = build_vocab(df_all_dataset['COMMCONTENT_SEG'])\n",
    "vocab_size = min(MAX_NB_WORDS, len(vocab_freqs)) + 2\n",
    "word2index = {x[0]: i+2 for i, x in enumerate(vocab_freqs.most_common(MAX_NB_WORDS))}\n",
    "word2index[\"PAD\"] = 0\n",
    "word2index[\"UNK\"] = 1\n",
    "index2word = {v:k for k, v in word2index.items()}\n",
    "len(word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing word embeddings.\n",
      "word embedding 195201\n"
     ]
    }
   ],
   "source": [
    "print('Indexing word embeddings.')  \n",
    "embeddings_index = {}\n",
    "with open('./embeddings/sgns.weibo.word','r') as f:\n",
    "    f = f.readlines()\n",
    "    for i in f[1:]:\n",
    "        values = i.strip().split(' ')\n",
    "        word = str(values[0])\n",
    "        embedding = np.asarray(values[1:],dtype='float')\n",
    "        embeddings_index[word] = embedding\n",
    "print('word embedding',len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80000"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_words = min(MAX_NB_WORDS,len(word2index))\n",
    "nb_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embedding_matrix = np.zeros((nb_words + 1, EMBEDDING_DIM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word, i in word2index.items():\n",
    "    if i > MAX_NB_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(str(word).upper())\n",
    "    if embedding_vector is not None:\n",
    "        word_embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_embedding_layer =  Embedding(input_dim = nb_words+1, \n",
    "                             output_dim = EMBEDDING_DIM, \n",
    "                            weights=[word_embedding_matrix], \n",
    "                             input_length=MAX_SEQUENCE_LENGTH, \n",
    "                             mask_zero=True,\n",
    "                             trainable=False\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Lambda\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rnn_cnn_model():\n",
    "    embedding_dim = 300\n",
    "    inp = Input(shape=(MAX_SEQUENCE_LENGTH,))\n",
    "    x =  Embedding(input_dim = nb_words+1, \n",
    "                             output_dim = EMBEDDING_DIM, \n",
    "                             weights=[word_embedding_matrix], \n",
    "                             input_length=MAX_SEQUENCE_LENGTH, \n",
    "                             mask_zero=False,\n",
    "                             trainable=True\n",
    "                            )(inp)\n",
    "    x = SpatialDropout1D(0.3)(x)\n",
    "    x = Bidirectional(GRU(80, return_sequences=True))(x)\n",
    "    \n",
    "    x1 = Conv1D(64, kernel_size=2, padding=\"valid\", kernel_initializer=\"he_uniform\")(x)\n",
    "    avg_pool = GlobalAveragePooling1D()(x1)\n",
    "    max_pool = GlobalMaxPooling1D()(x1)\n",
    "    kmax_pool = Lambda(lambda x: K.max(x, axis=1), output_shape=(64,))(x1)\n",
    "    conc1 = concatenate([avg_pool, max_pool, kmax_pool])\n",
    "    \n",
    "    x2 = Conv1D(64, kernel_size=3, padding=\"valid\", kernel_initializer=\"he_uniform\")(x)\n",
    "    avg_pool2 = GlobalAveragePooling1D()(x2)\n",
    "    max_pool2 = GlobalMaxPooling1D()(x2)\n",
    "    kmax_pool2 = Lambda(lambda x: K.max(x, axis=1), output_shape=(64,))(x2)\n",
    "    conc2 = concatenate([avg_pool2, max_pool2, kmax_pool2])\n",
    "    \n",
    "    merge = concatenate([conc1, conc2])\n",
    "    \n",
    "    outp = Dense(3, activation=\"softmax\")(merge)\n",
    "\n",
    "    model = Model(inputs=inp, outputs=outp)\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_cnn_model = get_rnn_cnn_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(rnn_cnn_model, to_file='./rnn_cnn_model.png', \n",
    "show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!['./rnn_cnn_model.png'](./rnn_cnn_model.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 18000 samples, validate on 2000 samples\n",
      "Epoch 1/6\n",
      "18000/18000 [==============================] - 165s 9ms/step - loss: 0.9768 - acc: 0.5082 - val_loss: 0.8396 - val_acc: 0.6140\n",
      "Epoch 2/6\n",
      "18000/18000 [==============================] - 152s 8ms/step - loss: 0.7800 - acc: 0.6513 - val_loss: 0.7917 - val_acc: 0.6465\n",
      "Epoch 3/6\n",
      "18000/18000 [==============================] - 154s 9ms/step - loss: 0.6843 - acc: 0.7046 - val_loss: 0.7446 - val_acc: 0.6855\n",
      "Epoch 4/6\n",
      "18000/18000 [==============================] - 159s 9ms/step - loss: 0.5971 - acc: 0.7503 - val_loss: 0.7294 - val_acc: 0.6850\n"
     ]
    }
   ],
   "source": [
    "batch_size = 256\n",
    "epochs = 6\n",
    "\n",
    "# model.fit(x=train_X, y=to_categorical(train_y-1, num_classes=3), \n",
    "#                     validation_data=(valid_X, to_categorical(valid_y-1, num_classes=3)[:]), \n",
    "#                     batch_size=batch_size, \n",
    "#                     epochs=epochs,\n",
    "#           shuffle=True,\n",
    "#                     verbose=1\n",
    "#          )\n",
    "history = rnn_cnn_model.fit(x=train_X, y=to_categorical(train_y-1, num_classes=3), \n",
    "                    validation_data=(valid_X, to_categorical(valid_y-1, num_classes=3)[:]), \n",
    "                    batch_size=batch_size, \n",
    "                    epochs=epochs,\n",
    "                    shuffle=True,\n",
    "                    callbacks = [EarlyStopping(monitor='val_acc', patience=0, mode='auto')],\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.382332\n",
       "2    0.327394\n",
       "3    0.290275\n",
       "dtype: float64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_test_preds = rnn_cnn_model.predict(padded_test_dataset_sequences, batch_size=1024)\n",
    "pooled_gru_conv_model_preds = np.argmax(all_test_preds,axis=1)[:]+1\n",
    "pd.Series(pooled_gru_conv_model_preds).value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"pooled_gru_2filter_conv-val_acc_0.6850-0.38-0.32-0.29-result.txt\", pooled_gru_conv_model_preds,fmt=\"%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "epochs = 4\n",
    "\n",
    "# model.fit(x=train_X, y=to_categorical(train_y-1, num_classes=3), \n",
    "#                     validation_data=(valid_X, to_categorical(valid_y-1, num_classes=3)[:]), \n",
    "#                     batch_size=batch_size, \n",
    "#                     epochs=epochs,\n",
    "#           shuffle=True,\n",
    "#                     verbose=1\n",
    "#          )\n",
    "history = rnn_cnn_model.fit(x=train_X, y=to_categorical(train_y-1, num_classes=3), \n",
    "                    validation_data=(valid_X, to_categorical(valid_y-1, num_classes=3)[:]), \n",
    "                    batch_size=batch_size, \n",
    "                    epochs=epochs,\n",
    "                    shuffle=True,\n",
    "                    callbacks = [EarlyStopping(monitor='val_acc', patience=0, mode='auto')],\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_test_preds = rnn_cnn_model.predict(padded_test_dataset_sequences, batch_size=1024)\n",
    "w2v7 = np.argmax(all_test_preds,axis=1)[:]+1\n",
    "pd.Series(w2v7).value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "epochs = 6\n",
    "\n",
    "# model.fit(x=train_X, y=to_categorical(train_y-1, num_classes=3), \n",
    "#                     validation_data=(valid_X, to_categorical(valid_y-1, num_classes=3)[:]), \n",
    "#                     batch_size=batch_size, \n",
    "#                     epochs=epochs,\n",
    "#           shuffle=True,\n",
    "#                     verbose=1\n",
    "#          )\n",
    "history = rnn_cnn_model.fit(x=train_X, y=to_categorical(train_y-1, num_classes=3), \n",
    "                    validation_data=(valid_X, to_categorical(valid_y-1, num_classes=3)[:]), \n",
    "                    batch_size=batch_size, \n",
    "                    epochs=epochs,\n",
    "                    shuffle=True,\n",
    "                    callbacks = [EarlyStopping(monitor='val_acc', patience=0, mode='auto')],\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_test_preds = rnn_cnn_model.predict(padded_test_dataset_sequences, batch_size=1024)\n",
    "w2v7 = np.argmax(all_test_preds,axis=1)[:]+1\n",
    "pd.Series(w2v7).value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tainable = True\n",
    "batch_size = 256\n",
    "epochs = 4\n",
    "\n",
    "# model.fit(x=train_X, y=to_categorical(train_y-1, num_classes=3), \n",
    "#                     validation_data=(valid_X, to_categorical(valid_y-1, num_classes=3)[:]), \n",
    "#                     batch_size=batch_size, \n",
    "#                     epochs=epochs,\n",
    "#           shuffle=True,\n",
    "#                     verbose=1\n",
    "#          )\n",
    "history = rnn_cnn_model.fit(x=train_X, y=to_categorical(train_y-1, num_classes=3), \n",
    "                    validation_data=(valid_X, to_categorical(valid_y-1, num_classes=3)[:]), \n",
    "                    batch_size=batch_size, \n",
    "                    epochs=epochs,\n",
    "                    shuffle=True,\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_test_preds = rnn_cnn_model.predict(padded_test_dataset_sequences, batch_size=1024)\n",
    "w2v8 = np.argmax(all_test_preds,axis=1)[:]+1\n",
    "pd.Series(w2v8).value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add kmax pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "epochs = 4\n",
    "\n",
    "# model.fit(x=train_X, y=to_categorical(train_y-1, num_classes=3), \n",
    "#                     validation_data=(valid_X, to_categorical(valid_y-1, num_classes=3)[:]), \n",
    "#                     batch_size=batch_size, \n",
    "#                     epochs=epochs,\n",
    "#           shuffle=True,\n",
    "#                     verbose=1\n",
    "#          )\n",
    "history = rnn_cnn_model.fit(x=train_X, y=to_categorical(train_y-1, num_classes=3), \n",
    "                    validation_data=(valid_X, to_categorical(valid_y-1, num_classes=3)[:]), \n",
    "                    batch_size=batch_size, \n",
    "                    epochs=epochs,\n",
    "                    shuffle=True,\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "epochs = 1\n",
    "\n",
    "# model.fit(x=train_X, y=to_categorical(train_y-1, num_classes=3), \n",
    "#                     validation_data=(valid_X, to_categorical(valid_y-1, num_classes=3)[:]), \n",
    "#                     batch_size=batch_size, \n",
    "#                     epochs=epochs,\n",
    "#           shuffle=True,\n",
    "#                     verbose=1\n",
    "#          )\n",
    "history = rnn_cnn_model.fit(x=train_X, y=to_categorical(train_y-1, num_classes=3), \n",
    "                    validation_data=(valid_X, to_categorical(valid_y-1, num_classes=3)[:]), \n",
    "                    batch_size=batch_size, \n",
    "                    epochs=epochs,\n",
    "                    shuffle=True,\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_test_preds = rnn_cnn_model.predict(padded_test_dataset_sequences, batch_size=256)\n",
    "rnn_cnn_k_max = np.argmax(all_test_preds,axis=1)[:]+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(rnn_cnn_k_max).value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"rnn-cnn-0.6685-result.txt\", rnn_cnn_k_max,fmt=\"%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "epochs = 5\n",
    "\n",
    "# model.fit(x=train_X, y=to_categorical(train_y-1, num_classes=3), \n",
    "#                     validation_data=(valid_X, to_categorical(valid_y-1, num_classes=3)[:]), \n",
    "#                     batch_size=batch_size, \n",
    "#                     epochs=epochs,\n",
    "#           shuffle=True,\n",
    "#                     verbose=1\n",
    "#          )\n",
    "history = rnn_cnn_model.fit(x=train_X, y=to_categorical(train_y-1, num_classes=3), \n",
    "                    validation_data=(valid_X, to_categorical(valid_y-1, num_classes=3)[:]), \n",
    "                    batch_size=batch_size, \n",
    "                    epochs=epochs,\n",
    "                    shuffle=True,\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_test_preds = rnn_cnn_model.predict(padded_test_dataset_sequences, batch_size=256)\n",
    "rnn_cnn_k_max2 = np.argmax(all_test_preds,axis=1)[:]+1\n",
    "pd.Series(rnn_cnn_k_max2).value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"rnn-cnn-0.6770-result.txt\", rnn_cnn_k_max2,fmt=\"%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_test_preds = rnn_cnn_model.predict(padded_test_dataset_sequences, batch_size=256)\n",
    "w2v6 = np.argmax(all_test_preds,axis=1)[:]+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(w2v6).value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"rnn-cnn-0.6320-result.txt\", w2v6,fmt=\"%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(pre_embedding_layer)\n",
    "model.add(Bidirectional(GRU(256, dropout=0.2, recurrent_dropout=0.1, return_sequences=True)))\n",
    "model.add(Bidirectional(GRU(256, dropout=0.2, recurrent_dropout=0.1)))\n",
    "model.add(Dense(NUM_LABELS, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rnn_cnn_model():\n",
    "    embedding_dim = 300\n",
    "    inp = Input(shape=(MAX_SEQUENCE_LENGTH,))\n",
    "    x =  Embedding(input_dim = nb_words+1, \n",
    "                             output_dim = EMBEDDING_DIM, \n",
    "                             weights=[word_embedding_matrix], \n",
    "                             input_length=MAX_SEQUENCE_LENGTH, \n",
    "                             mask_zero=False,\n",
    "                             trainable=False\n",
    "                            )(inp)\n",
    "    x = SpatialDropout1D(0.3)(x)\n",
    "    x = Bidirectional(GRU(128, return_sequences=True,dropout=0.1,recurrent_dropout=0.1))(x)\n",
    "    x = Conv1D(64, kernel_size=3, padding=\"valid\", kernel_initializer=\"glorot_uniform\")(x)\n",
    "    avg_pool = GlobalAveragePooling1D()(x)\n",
    "#     max_pool = GlobalMaxPooling1D()(x)\n",
    "    kmax_pool = Lambda(lambda x: K.max(x, axis=1), output_shape=(64,))(x)\n",
    "    conc = concatenate([avg_pool, kmax_pool])\n",
    "    outp = Dense(3, activation=\"softmax\")(conc)\n",
    "\n",
    "    model = Model(inputs=inp, outputs=outp)\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "              #optimizer='adam',\n",
    "                  optimizer=Adam(lr=1e-3),\n",
    "              metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_cnn_model = get_rnn_cnn_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "epochs = 5\n",
    "\n",
    "# model.fit(x=train_X, y=to_categorical(train_y-1, num_classes=3), \n",
    "#                     validation_data=(valid_X, to_categorical(valid_y-1, num_classes=3)[:]), \n",
    "#                     batch_size=batch_size, \n",
    "#                     epochs=epochs,\n",
    "#           shuffle=True,\n",
    "#                     verbose=1\n",
    "#          )\n",
    "history = rnn_cnn_model.fit(x=train_X, y=to_categorical(train_y-1, num_classes=3), \n",
    "                    validation_data=(valid_X, to_categorical(valid_y-1, num_classes=3)[:]), \n",
    "                    batch_size=batch_size, \n",
    "                    epochs=epochs,\n",
    "                    shuffle=True,\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "epochs = 5\n",
    "\n",
    "# model.fit(x=train_X, y=to_categorical(train_y-1, num_classes=3), \n",
    "#                     validation_data=(valid_X, to_categorical(valid_y-1, num_classes=3)[:]), \n",
    "#                     batch_size=batch_size, \n",
    "#                     epochs=epochs,\n",
    "#           shuffle=True,\n",
    "#                     verbose=1\n",
    "#          )\n",
    "history = rnn_cnn_model.fit(x=train_X, y=to_categorical(train_y-1, num_classes=3), \n",
    "                    validation_data=(valid_X, to_categorical(valid_y-1, num_classes=3)[:]), \n",
    "                    batch_size=batch_size, \n",
    "                    epochs=epochs,\n",
    "                    shuffle=True,\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"dfdfs sfs il li li li li lililili\".count('li')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
