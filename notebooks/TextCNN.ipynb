{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#-*-coding:utf-8-*-\n",
    "import sys\n",
    "import pickle\n",
    "import traceback\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gensim import corpora,models\n",
    "\n",
    "from scipy.sparse import *\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers import Dense, Input, Flatten, Dropout\n",
    "from keras.layers import Convolution1D, MaxPooling1D, Embedding\n",
    "from keras.models import Model\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint, LearningRateScheduler\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.layers import Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import *\n",
    "import jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入自定义库\n",
    "from utils.data_utils import clean_str\n",
    "from utils.data_utils import build_vocab\n",
    "from utils.data_utils import get_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载训练集合\n",
    "df_train_dataset = pd.read_csv('./data/training-inspur.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载测试集\n",
    "df_test_dataset = pd.read_csv('./data/Preliminary-texting-1.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(102024, 2)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_dataset = df_train_dataset[['COMMCONTENT', 'COMMLEVEL']]\n",
    "df_test_dataset = df_test_dataset[['COMMCONTENT']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20000"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_dataset.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_dataset = pd.concat([df_train_dataset, df_test_dataset], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>COMMCONTENT</th>\n",
       "      <th>COMMLEVEL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>122019</th>\n",
       "      <td>主要是人太多了，太挤了</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122020</th>\n",
       "      <td>人少，温泉太旧，池子水咋的，有的没开</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122021</th>\n",
       "      <td>隐形收费，批东西贵得要死，坑人！</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122022</th>\n",
       "      <td>山不高，但是很累，3个小时爬上去，一个小时下山。感受了一下红色旅游景点。</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122023</th>\n",
       "      <td>占地不小，但没什么人气，从世园会结束后就没什么人去了</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 COMMCONTENT  COMMLEVEL\n",
       "122019                           主要是人太多了，太挤了        NaN\n",
       "122020                    人少，温泉太旧，池子水咋的，有的没开        NaN\n",
       "122021                      隐形收费，批东西贵得要死，坑人！        NaN\n",
       "122022  山不高，但是很累，3个小时爬上去，一个小时下山。感受了一下红色旅游景点。        NaN\n",
       "122023            占地不小，但没什么人气，从世园会结束后就没什么人去了        NaN"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all_dataset.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 150 # 每篇文章选取150个词\n",
    "\n",
    "MAX_NB_WORDS = 80000 # 将字典设置为含有1万个词84480\n",
    "\n",
    "EMBEDDING_DIM = 300 # 词向量维度，300维\n",
    "\n",
    "VALIDATION_SPLIT = 0.1 # 测试集大小，全部数据的20%\n",
    "\n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/5n/2_by50851fxc4d_snc1d9wf80000gn/T/jieba.cache\n",
      "Loading model cost 0.822 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "COMMCONTENT_SEG = []\n",
    "\n",
    "for sent in df_all_dataset['COMMCONTENT']:\n",
    "\n",
    "    # Extract Sentence\n",
    "    sent = str(sent).strip()\n",
    "\n",
    "    seg_list = jieba.cut(sent, cut_all=False)\n",
    "\n",
    "    seg_list = [i for i in seg_list if i != ' ']\n",
    "    \n",
    "    COMMCONTENT_SEG.append(\" \".join(seg_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_dataset['COMMCONTENT_SEG'] = pd.DataFrame(COMMCONTENT_SEG,columns=['COMMCONTENT_SEG'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "122019                                    主要 是 人太多 了 ， 太挤 了\n",
       "122020                       人少 ， 温泉 太旧 ， 池子 水 咋 的 ， 有 的 没开\n",
       "122021                            隐形 收费 ， 批 东西 贵得 要死 ， 坑人 ！\n",
       "122022    山不高 ， 但是 很累 ， 3 个 小时 爬上去 ， 一个 小时 下山 。 感受 了 一下 ...\n",
       "122023            占地 不小 ， 但 没什么 人气 ， 从 世园 会 结束 后 就 没什么 人去 了\n",
       "Name: COMMCONTENT_SEG, dtype: object"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all_dataset['COMMCONTENT_SEG'].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_corpus = df_all_dataset['COMMCONTENT_SEG']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS) # 传入我们词向量的字典\n",
    "\n",
    "tokenizer.fit_on_texts(text_corpus) # 传入我们的训练数据，得到训练数据中出现的词的字典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_sequences = tokenizer.texts_to_sequences(text_corpus) # 根据训练数据中出现的词的字典，将训练数据转换为sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 100134 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_dataset_sequences = pad_sequences(dataset_sequences, maxlen=MAX_SEQUENCE_LENGTH) # 限制每篇文章的长度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(122024, 150)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_dataset_sequences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_dataset_seg = df_all_dataset['COMMCONTENT_SEG'][20000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset_sequences = tokenizer.texts_to_sequences(df_test_dataset_seg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_test_dataset_sequences = pad_sequences(test_dataset_sequences, maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(102024, 150)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_test_dataset_sequences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label one hot表示\n",
    "labels = df_all_dataset['COMMLEVEL'].dropna().map(int)#.values.tolist()\n",
    "labels = to_categorical(labels-1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (122024, 150)\n",
      "Shape of label tensor: 20000\n"
     ]
    }
   ],
   "source": [
    "print('Shape of data tensor:', padded_dataset_sequences.shape)\n",
    "print('Shape of label tensor:', len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20000"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(padded_dataset_sequences[:df_train_dataset.shape[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    0,     0,     0, ...,   586,   295,   714],\n",
       "       [    0,     0,     0, ...,  2558,   230,     7],\n",
       "       [    0,     0,     0, ...,   332,    59,   742],\n",
       "       ...,\n",
       "       [    0,     0,     0, ...,     6, 61456,     3],\n",
       "       [    0,     0,     0, ..., 43363,     6,     6],\n",
       "       [    0,     0,     0, ...,   129,    65,  4692]], dtype=int32)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_dataset_sequences[:df_train_dataset.shape[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20000"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_all_dataset['COMMLEVEL'][:df_train_dataset.shape[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        1.0\n",
       "1        1.0\n",
       "2        1.0\n",
       "3        1.0\n",
       "4        1.0\n",
       "5        1.0\n",
       "6        1.0\n",
       "7        1.0\n",
       "8        1.0\n",
       "9        1.0\n",
       "10       1.0\n",
       "11       1.0\n",
       "12       1.0\n",
       "13       1.0\n",
       "14       1.0\n",
       "15       1.0\n",
       "16       1.0\n",
       "17       1.0\n",
       "18       1.0\n",
       "19       1.0\n",
       "20       1.0\n",
       "21       1.0\n",
       "22       1.0\n",
       "23       1.0\n",
       "24       1.0\n",
       "25       1.0\n",
       "26       1.0\n",
       "27       1.0\n",
       "28       1.0\n",
       "29       1.0\n",
       "        ... \n",
       "19970    3.0\n",
       "19971    3.0\n",
       "19972    3.0\n",
       "19973    3.0\n",
       "19974    3.0\n",
       "19975    3.0\n",
       "19976    3.0\n",
       "19977    3.0\n",
       "19978    3.0\n",
       "19979    3.0\n",
       "19980    3.0\n",
       "19981    3.0\n",
       "19982    3.0\n",
       "19983    3.0\n",
       "19984    3.0\n",
       "19985    3.0\n",
       "19986    3.0\n",
       "19987    3.0\n",
       "19988    3.0\n",
       "19989    3.0\n",
       "19990    3.0\n",
       "19991    3.0\n",
       "19992    3.0\n",
       "19993    3.0\n",
       "19994    3.0\n",
       "19995    3.0\n",
       "19996    3.0\n",
       "19997    3.0\n",
       "19998    3.0\n",
       "19999    3.0\n",
       "Name: COMMLEVEL, Length: 20000, dtype: float64"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all_dataset['COMMLEVEL'][:df_train_dataset.shape[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    0,     0,     0, ...,   586,   295,   714],\n",
       "       [    0,     0,     0, ...,  2558,   230,     7],\n",
       "       [    0,     0,     0, ...,   332,    59,   742],\n",
       "       ...,\n",
       "       [    0,     0,     0, ...,     6, 61456,     3],\n",
       "       [    0,     0,     0, ..., 43363,     6,     6],\n",
       "       [    0,     0,     0, ...,   129,    65,  4692]], dtype=int32)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_dataset_sequences[:df_train_dataset.shape[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X,valid_X,train_y,valid_y =train_test_split(padded_dataset_sequences[:df_train_dataset.shape[0]], \n",
    "                                                  df_all_dataset['COMMLEVEL'][:df_train_dataset.shape[0]], \n",
    "                                                  test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18000, 150)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(valid_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab,vocab_freqs = build_vocab(df_all_dataset['COMMCONTENT_SEG'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100793"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab_freqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = min(MAX_NB_WORDS, len(vocab_freqs)) + 2\n",
    "word2index = {x[0]: i+2 for i, x in enumerate(vocab_freqs.most_common(MAX_NB_WORDS))}\n",
    "word2index[\"PAD\"] = 0\n",
    "word2index[\"UNK\"] = 1\n",
    "index2word = {v:k for k, v in word2index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80002"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "\n",
    "tokenizer.fit_on_texts(df_dataset['COMMCONTENT_SEG'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_sequences = tokenizer.texts_to_sequences(df_dataset['COMMCONTENT_SEG'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_dataset_sequences = pad_sequences(dataset_sequences, maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_dataset_sequences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing word embeddings.\n",
      "word embedding 195202\n"
     ]
    }
   ],
   "source": [
    "print('Indexing word embeddings.')  \n",
    "embeddings_index = {}\n",
    "with open('./embeddings/sgns.weibo.word','r') as f:\n",
    "    f = f.readlines()\n",
    "    for i in f[:]:\n",
    "        values = i.strip().split(' ')\n",
    "#         print(values)\n",
    "        word = str(values[0])\n",
    "        embedding = np.asarray(values[1:],dtype='float')\n",
    "        embeddings_index[word] = embedding\n",
    "print('word embedding',len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80000"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_words = min(MAX_NB_WORDS,len(word2index))\n",
    "nb_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embedding_matrix = np.zeros((nb_words + 1, EMBEDDING_DIM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word, i in word2index.items():\n",
    "    if i > MAX_NB_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(str(word).upper())\n",
    "    if embedding_vector is not None:\n",
    "        word_embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.27259 ,  0.244615,  0.032857, ..., -0.199684, -0.084092,\n",
       "         0.060737],\n",
       "       [ 0.      ,  0.      ,  0.      , ...,  0.      ,  0.      ,\n",
       "         0.      ],\n",
       "       [ 0.094386, -0.200944, -0.030828, ...,  0.003085,  0.023796,\n",
       "        -0.201742],\n",
       "       ...,\n",
       "       [ 0.190794, -0.037967,  0.1013  , ..., -0.302136, -0.126407,\n",
       "        -0.178464],\n",
       "       [ 0.175443,  0.239842,  0.210521, ...,  0.071008,  0.177222,\n",
       "        -0.062866],\n",
       "       [-0.230501, -0.152982,  0.207998, ...,  0.007232, -0.494047,\n",
       "        -0.179105]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_embedding_matrix[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "\n",
    "\n",
    "from keras.layers import Input, Dense, Embedding, Conv1D, Conv2D, MaxPooling1D, MaxPool2D\n",
    "from keras.layers import Reshape, Flatten, Dropout, Concatenate\n",
    "from keras.layers import SpatialDropout1D, concatenate\n",
    "from keras.layers import GRU, Bidirectional, GlobalAveragePooling1D, GlobalMaxPooling1D\n",
    "from keras.layers.recurrent import LSTM,SimpleRNN\n",
    "from keras.layers import Activation\n",
    "\n",
    "from keras.callbacks import Callback\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "from keras.models import load_model\n",
    "\n",
    "from keras.utils.vis_utils import plot_model\n",
    "\n",
    "from keras.utils.np_utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_cnn(maxlen=MAX_SEQUENCE_LENGTH, max_features=2000, embed_size=32):\n",
    "    \n",
    "    # Inputs\n",
    "    sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "\n",
    "    # Embeddings layers\n",
    "    emb_comment =  Embedding(input_dim = MAX_NB_WORDS+1, \n",
    "                             output_dim = EMBEDDING_DIM, \n",
    "#                              weights=[word_embedding_matrix], \n",
    "                             input_length=MAX_SEQUENCE_LENGTH, \n",
    "                             mask_zero=False,\n",
    "                             trainable=True\n",
    "                            )(sequence_input)\n",
    "        # Embeddings layers\n",
    "    pre_emb_comment =  Embedding(input_dim = MAX_NB_WORDS+1, \n",
    "                             output_dim = EMBEDDING_DIM, \n",
    "                             weights=[word_embedding_matrix], \n",
    "                             input_length=MAX_SEQUENCE_LENGTH, \n",
    "                             mask_zero=False,\n",
    "                             trainable=True\n",
    "                            )(sequence_input)\n",
    "\n",
    "    # conv layers\n",
    "    convs = []\n",
    "    \n",
    "    filter_sizes = [2, 3, 4, 5]\n",
    "    \n",
    "    for fsz in filter_sizes:\n",
    "        l_conv = Conv1D(filters=100, kernel_size=fsz, activation='tanh')(pre_emb_comment)\n",
    "        \n",
    "        l_pool = MaxPooling1D(maxlen - fsz + 1)(l_conv)\n",
    "        \n",
    "        l_pool = Flatten()(l_pool)\n",
    "        \n",
    "        convs.append(l_pool)\n",
    "    \n",
    "    merge = concatenate(convs, axis=1)\n",
    "\n",
    "    out = Dropout(0.25)(merge)\n",
    "    \n",
    "    output = Dense(256, activation='tanh')(out)\n",
    "\n",
    "    output = Dense(len(np.unique(np.unique(df_all_dataset['COMMLEVEL'].dropna().map(int)))), activation='softmax')(output)\n",
    "\n",
    "    # model = Model([sequence_input], output)\n",
    "    # model = Model(inputs=sequence_input, output)\n",
    "    model = Model(sequence_input, output)\n",
    "    \n",
    "    #  adam = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "    #  model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=['accuracy'])\n",
    "    # 优化器我这里用了adadelta，也可以使用其他方法\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 150)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_4 (Embedding)         (None, 150, 300)     24000300    input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 149, 100)     60100       embedding_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 148, 100)     90100       embedding_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 147, 100)     120100      embedding_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 146, 100)     150100      embedding_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1D)  (None, 1, 100)       0           conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1D)  (None, 1, 100)       0           conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1D)  (None, 1, 100)       0           conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1D)  (None, 1, 100)       0           conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_5 (Flatten)             (None, 100)          0           max_pooling1d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_6 (Flatten)             (None, 100)          0           max_pooling1d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_7 (Flatten)             (None, 100)          0           max_pooling1d_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_8 (Flatten)             (None, 100)          0           max_pooling1d_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 400)          0           flatten_5[0][0]                  \n",
      "                                                                 flatten_6[0][0]                  \n",
      "                                                                 flatten_7[0][0]                  \n",
      "                                                                 flatten_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 400)          0           concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 256)          102656      dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 3)            771         dense_3[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 24,524,127\n",
      "Trainable params: 24,524,127\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = text_cnn()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_X,valid_X,train_y,valid_y =train_test_split(padded_dataset_sequences, df_dataset['COMMLEVEL'], test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((16000, 150), (4000, 150))"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X.shape,valid_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((16000,), (4000,))"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y.shape,valid_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0, ...,    1,    8, 1649],\n",
       "       [   0,    0,    0, ...,   10,   12,  138],\n",
       "       [   0,    0,    0, ...,   87,   53, 2362],\n",
       "       ...,\n",
       "       [   0,    0,    0, ...,   20,   64,   32],\n",
       "       [   0,    0,    0, ...,   67,  171,    2],\n",
       "       [   0,    0,    0, ..., 2046, 2046,    3]], dtype=int32)"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "#写一个LossHistory类，保存loss和acc\n",
    "class LossHistory(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = {'batch': [], 'epoch': []}\n",
    "        self.accuracy = {'batch': [], 'epoch': []}\n",
    "        self.val_loss = {'batch': [], 'epoch': []}\n",
    "        self.val_acc = {'batch': [], 'epoch': []}\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        self.losses['batch'].append(logs.get('loss'))\n",
    "        self.accuracy['batch'].append(logs.get('acc'))\n",
    "        self.val_loss['batch'].append(logs.get('val_loss'))\n",
    "        self.val_acc['batch'].append(logs.get('val_acc'))\n",
    "\n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        self.losses['epoch'].append(logs.get('loss'))\n",
    "        self.accuracy['epoch'].append(logs.get('acc'))\n",
    "        self.val_loss['epoch'].append(logs.get('val_loss'))\n",
    "        self.val_acc['epoch'].append(logs.get('val_acc'))\n",
    "\n",
    "    def loss_plot(self, loss_type):\n",
    "        iters = range(len(self.losses[loss_type]))\n",
    "        #创建一个图\n",
    "        plt.figure()\n",
    "        # acc\n",
    "        plt.plot(iters, self.accuracy[loss_type], 'r', label='train acc')#plt.plot(x,y)，这个将数据画成曲线\n",
    "        # loss\n",
    "        plt.plot(iters, self.losses[loss_type], 'g', label='train loss')\n",
    "        if loss_type == 'epoch':\n",
    "            # val_acc\n",
    "            plt.plot(iters, self.val_acc[loss_type], 'b', label='val acc')\n",
    "            # val_loss\n",
    "            plt.plot(iters, self.val_loss[loss_type], 'k', label='val loss')\n",
    "        plt.grid(True)#设置网格形式\n",
    "        plt.xlabel(loss_type)\n",
    "        plt.ylabel('acc-loss')#给x，y轴加注释\n",
    "        plt.legend(loc=\"upper right\")#设置图例显示位置\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       ...,\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_categorical(train_y.map(int)-1, num_classes=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 18000 samples, validate on 2000 samples\n",
      "Epoch 1/5\n",
      "18000/18000 [==============================] - 149s 8ms/step - loss: 1.0078 - acc: 0.5090 - val_loss: 0.8060 - val_acc: 0.6150\n",
      "Epoch 2/5\n",
      "18000/18000 [==============================] - 148s 8ms/step - loss: 0.6874 - acc: 0.6997 - val_loss: 0.7564 - val_acc: 0.6630\n",
      "Epoch 3/5\n",
      "18000/18000 [==============================] - 153s 8ms/step - loss: 0.4372 - acc: 0.8365 - val_loss: 0.8018 - val_acc: 0.6700\n",
      "Epoch 4/5\n",
      "18000/18000 [==============================] - 146s 8ms/step - loss: 0.2005 - acc: 0.9408 - val_loss: 1.0455 - val_acc: 0.6425\n",
      "Epoch 5/5\n",
      "18000/18000 [==============================] - 143s 8ms/step - loss: 0.0886 - acc: 0.9788 - val_loss: 1.0438 - val_acc: 0.6505\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1479e20f0>"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 0.1 valid data\n",
    "batch_size = 256\n",
    "epochs = 5\n",
    "\n",
    "# model.fit(x_train, y_train,\n",
    "#           validation_split=0.1,\n",
    "#           batch_size=batch_size,\n",
    "#           epochs=epochs,\n",
    "#           shuffle=True)\n",
    "#创建一个实例LossHistory\n",
    "history = LossHistory()\n",
    "\n",
    "model.fit(x=train_X, y=to_categorical(train_y.map(int)-1, num_classes=None), \n",
    "                    validation_data=(valid_X, to_categorical(valid_y.map(int)-1, num_classes=None)),\n",
    "                    batch_size=batch_size, \n",
    "                    #callbacks=[checkpoint],\n",
    "                    callbacks=[history],\n",
    "                    epochs=epochs,\n",
    "                    verbose=1\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "102024"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_test_preds2 = model.predict(padded_test_dataset_sequences, batch_size=256)\n",
    "w2v4 = np.argmax(all_test_preds2,axis=1)[:]+1\n",
    "len(w2v4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.385037\n",
       "3    0.315788\n",
       "2    0.299175\n",
       "dtype: float64"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(w2v4).value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.381322\n",
       "3    0.310496\n",
       "2    0.308182\n",
       "dtype: float64"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(np.argmax((all_test_preds2*0.7+all_test_preds*0.3),axis=1)+1).value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/5\n",
      "16000/16000 [==============================] - 131s 8ms/step - loss: 1.0434 - acc: 0.4870 - val_loss: 0.8374 - val_acc: 0.6225\n",
      "Epoch 2/5\n",
      "16000/16000 [==============================] - 129s 8ms/step - loss: 0.6981 - acc: 0.7086 - val_loss: 0.7812 - val_acc: 0.6415\n",
      "Epoch 3/5\n",
      "16000/16000 [==============================] - 130s 8ms/step - loss: 0.4480 - acc: 0.8384 - val_loss: 0.7894 - val_acc: 0.6570\n",
      "Epoch 4/5\n",
      "16000/16000 [==============================] - 125s 8ms/step - loss: 0.2111 - acc: 0.9405 - val_loss: 0.8914 - val_acc: 0.6515\n",
      "Epoch 5/5\n",
      "16000/16000 [==============================] - 123s 8ms/step - loss: 0.0777 - acc: 0.9849 - val_loss: 1.0152 - val_acc: 0.6575\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x147688048>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 256\n",
    "epochs = 5\n",
    "\n",
    "# model.fit(x_train, y_train,\n",
    "#           validation_split=0.1,\n",
    "#           batch_size=batch_size,\n",
    "#           epochs=epochs,\n",
    "#           shuffle=True)\n",
    "#创建一个实例LossHistory\n",
    "history = LossHistory()\n",
    "\n",
    "model.fit(x=train_X, y=to_categorical(train_y.map(int)-1, num_classes=None), \n",
    "                    validation_data=(valid_X, to_categorical(valid_y.map(int)-1, num_classes=None)),\n",
    "                    batch_size=batch_size, \n",
    "                    #callbacks=[checkpoint],\n",
    "                    callbacks=[history],\n",
    "                    epochs=epochs,\n",
    "                    verbose=1\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6.2206060e-01, 8.9762181e-02, 2.8817725e-01],\n",
       "       [2.5641244e-02, 4.8368481e-01, 4.9067396e-01],\n",
       "       [8.6228114e-01, 1.3433319e-02, 1.2428550e-01],\n",
       "       ...,\n",
       "       [1.8361310e-05, 9.3378540e-04, 9.9904782e-01],\n",
       "       [3.2784998e-01, 3.5401651e-01, 3.1813359e-01],\n",
       "       [3.0305763e-03, 7.9328582e-02, 9.1764081e-01]], dtype=float32)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_test_preds = model.predict(padded_test_dataset_sequences, batch_size=256)\n",
    "w2v3 = np.argmax(all_test_preds,axis=1)[:]+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "102024"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(w2v3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.364316\n",
       "2    0.339450\n",
       "3    0.296234\n",
       "dtype: float64"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(w2v3).value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"all_testset_preds.txt\", w2v3,fmt=\"%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102024\n"
     ]
    }
   ],
   "source": [
    "with open('./all_testset_preds.txt', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    print(len(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/10\n",
      "16000/16000 [==============================] - 123s 8ms/step - loss: 1.0423 - acc: 0.4513 - val_loss: 0.9727 - val_acc: 0.5148\n",
      "Epoch 2/10\n",
      "16000/16000 [==============================] - 145s 9ms/step - loss: 0.9111 - acc: 0.5616 - val_loss: 0.8645 - val_acc: 0.5818\n",
      "Epoch 3/10\n",
      "16000/16000 [==============================] - 130s 8ms/step - loss: 0.8268 - acc: 0.6211 - val_loss: 0.7930 - val_acc: 0.6372\n",
      "Epoch 4/10\n",
      "16000/16000 [==============================] - 143s 9ms/step - loss: 0.7703 - acc: 0.6594 - val_loss: 0.7774 - val_acc: 0.6490\n",
      "Epoch 5/10\n",
      "16000/16000 [==============================] - 129s 8ms/step - loss: 0.7011 - acc: 0.6919 - val_loss: 0.7569 - val_acc: 0.6583\n",
      "Epoch 6/10\n",
      "16000/16000 [==============================] - 123s 8ms/step - loss: 0.6648 - acc: 0.7146 - val_loss: 0.7680 - val_acc: 0.6508\n",
      "Epoch 7/10\n",
      "16000/16000 [==============================] - 123s 8ms/step - loss: 0.6155 - acc: 0.7398 - val_loss: 0.7902 - val_acc: 0.6468\n",
      "Epoch 8/10\n",
      "16000/16000 [==============================] - 127s 8ms/step - loss: 0.5802 - acc: 0.7621 - val_loss: 0.8337 - val_acc: 0.6420\n",
      "Epoch 9/10\n",
      "16000/16000 [==============================] - 164s 10ms/step - loss: 0.5248 - acc: 0.7888 - val_loss: 0.7817 - val_acc: 0.6555\n",
      "Epoch 10/10\n",
      "16000/16000 [==============================] - 135s 8ms/step - loss: 0.4687 - acc: 0.8154 - val_loss: 0.8010 - val_acc: 0.6470\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x13a2e0d30>"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 256\n",
    "epochs = 5\n",
    "\n",
    "# model.fit(x_train, y_train,\n",
    "#           validation_split=0.1,\n",
    "#           batch_size=batch_size,\n",
    "#           epochs=epochs,\n",
    "#           shuffle=True)\n",
    "#创建一个实例LossHistory\n",
    "history = LossHistory()\n",
    "\n",
    "model.fit(x=train_X, y=to_categorical(train_y.map(int)-1, num_classes=None), \n",
    "                    validation_data=(valid_X, to_categorical(valid_y.map(int)-1, num_classes=None)),\n",
    "                    batch_size=batch_size, \n",
    "                    #callbacks=[checkpoint],\n",
    "                    callbacks=[history],\n",
    "                    epochs=epochs,\n",
    "                    verbose=1\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/8\n",
      "16000/16000 [==============================] - 151s 9ms/step - loss: 1.0415 - acc: 0.4554 - val_loss: 0.9457 - val_acc: 0.5360\n",
      "Epoch 2/8\n",
      "16000/16000 [==============================] - 133s 8ms/step - loss: 0.9068 - acc: 0.5653 - val_loss: 0.8620 - val_acc: 0.5825\n",
      "Epoch 3/8\n",
      "16000/16000 [==============================] - 149s 9ms/step - loss: 0.8152 - acc: 0.6302 - val_loss: 0.8048 - val_acc: 0.6220\n",
      "Epoch 4/8\n",
      "16000/16000 [==============================] - 129s 8ms/step - loss: 0.7574 - acc: 0.6655 - val_loss: 0.7594 - val_acc: 0.6520\n",
      "Epoch 5/8\n",
      "16000/16000 [==============================] - 127s 8ms/step - loss: 0.7004 - acc: 0.6985 - val_loss: 0.8554 - val_acc: 0.6102\n",
      "Epoch 6/8\n",
      "16000/16000 [==============================] - 120s 8ms/step - loss: 0.6479 - acc: 0.7278 - val_loss: 0.7461 - val_acc: 0.6695\n",
      "Epoch 7/8\n",
      "16000/16000 [==============================] - 123s 8ms/step - loss: 0.6029 - acc: 0.7504 - val_loss: 0.7586 - val_acc: 0.6685\n",
      "Epoch 8/8\n",
      "16000/16000 [==============================] - 138s 9ms/step - loss: 0.5611 - acc: 0.7712 - val_loss: 0.8206 - val_acc: 0.6468\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x13c9231d0>"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 256\n",
    "epochs = 10\n",
    "\n",
    "# model.fit(x_train, y_train,\n",
    "#           validation_split=0.1,\n",
    "#           batch_size=batch_size,\n",
    "#           epochs=epochs,\n",
    "#           shuffle=True)\n",
    "#创建一个实例LossHistory\n",
    "history = LossHistory()\n",
    "\n",
    "model.fit(x=train_X, y=to_categorical(train_y.map(int)-1, num_classes=None), \n",
    "                    validation_data=(valid_X, to_categorical(valid_y.map(int)-1, num_classes=None)),\n",
    "                    batch_size=batch_size, \n",
    "                    #callbacks=[checkpoint],\n",
    "                    callbacks=[history],\n",
    "                    epochs=8,\n",
    "                    verbose=1\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/10\n",
      "16000/16000 [==============================] - 161s 10ms/step - loss: 0.9523 - acc: 0.5251 - val_loss: 1.3269 - val_acc: 0.2900\n",
      "Epoch 2/10\n",
      "16000/16000 [==============================] - 158s 10ms/step - loss: 0.8179 - acc: 0.6226 - val_loss: 1.1638 - val_acc: 0.4698\n",
      "Epoch 3/10\n",
      "16000/16000 [==============================] - 149s 9ms/step - loss: 0.7368 - acc: 0.6783 - val_loss: 1.0059 - val_acc: 0.5617\n",
      "Epoch 4/10\n",
      "16000/16000 [==============================] - 156s 10ms/step - loss: 0.6666 - acc: 0.7139 - val_loss: 1.3065 - val_acc: 0.3685\n",
      "Epoch 5/10\n",
      " 1152/16000 [=>............................] - ETA: 2:33 - loss: 0.5806 - acc: 0.7674"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-94-0a7261fe390f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m           \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m           \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m           \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m           )\n",
      "\u001b[0;32m~/anaconda3/envs/TensorFlowKerasEnv/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1040\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1042\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1043\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1044\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/envs/TensorFlowKerasEnv/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/TensorFlowKerasEnv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2659\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2660\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2661\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2662\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2663\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/TensorFlowKerasEnv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2629\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2630\u001b[0m                                 session)\n\u001b[0;32m-> 2631\u001b[0;31m         \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2632\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/TensorFlowKerasEnv/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1449\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_with_new_api\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1450\u001b[0m           return tf_session.TF_SessionRunCallable(\n\u001b[0;32m-> 1451\u001b[0;31m               self._session._session, self._handle, args, status, None)\n\u001b[0m\u001b[1;32m   1452\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1453\u001b[0m           return tf_session.TF_DeprecatedSessionRunCallable(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = LossHistory()\n",
    "model.fit(x=padded_dataset_sequences[:df_train_dataset.shape[0]],\n",
    "          y=labels, \n",
    "          batch_size=128, \n",
    "          epochs=10, \n",
    "          verbose=1, \n",
    "          callbacks=[history],\n",
    "          validation_split=0.2, \n",
    "          shuffle=True\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/7\n",
      "16000/16000 [==============================] - 131s 8ms/step - loss: 1.0181 - acc: 0.5037 - val_loss: 0.8370 - val_acc: 0.6130\n",
      "Epoch 2/7\n",
      "16000/16000 [==============================] - 118s 7ms/step - loss: 0.6899 - acc: 0.7055 - val_loss: 0.7778 - val_acc: 0.6450\n",
      "Epoch 3/7\n",
      "16000/16000 [==============================] - 118s 7ms/step - loss: 0.4381 - acc: 0.8407 - val_loss: 0.9323 - val_acc: 0.6040\n",
      "Epoch 4/7\n",
      "16000/16000 [==============================] - 117s 7ms/step - loss: 0.2107 - acc: 0.9356 - val_loss: 0.9179 - val_acc: 0.6543\n",
      "Epoch 5/7\n",
      "16000/16000 [==============================] - 118s 7ms/step - loss: 0.0860 - acc: 0.9813 - val_loss: 1.1007 - val_acc: 0.6460\n",
      "Epoch 6/7\n",
      "16000/16000 [==============================] - 118s 7ms/step - loss: 0.0434 - acc: 0.9913 - val_loss: 1.1391 - val_acc: 0.6493\n",
      "Epoch 7/7\n",
      "16000/16000 [==============================] - 119s 7ms/step - loss: 0.0239 - acc: 0.9966 - val_loss: 1.2496 - val_acc: 0.6497\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1ec68c6a0>"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 256\n",
    "epochs = 7\n",
    "\n",
    "# model.fit(x_train, y_train,\n",
    "#           validation_split=0.1,\n",
    "#           batch_size=batch_size,\n",
    "#           epochs=epochs,\n",
    "#           shuffle=True)\n",
    "#创建一个实例LossHistory\n",
    "history = LossHistory()\n",
    "\n",
    "model.fit(x=train_X, y=to_categorical(train_y.map(int)-1, num_classes=None), \n",
    "                    validation_data=(valid_X, to_categorical(valid_y.map(int)-1, num_classes=None)),\n",
    "                    batch_size=batch_size, \n",
    "                    #callbacks=[checkpoint],\n",
    "                    callbacks=[history],\n",
    "                    epochs=epochs,\n",
    "                    verbose=1\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000/4000 [==============================] - 9s 2ms/step\n",
      "test_loss: 1.249573, accuracy: 0.649750\n"
     ]
    }
   ],
   "source": [
    "score, acc = model.evaluate(valid_X[:],to_categorical(valid_y-1, num_classes=None)[:], batch_size=batch_size)\n",
    "#     print(score, acc)\n",
    "print('test_loss: %f, accuracy: %f' % (score, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "history.loss_plot('epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_dataset_sequences[df_train_dataset.shape[0]:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对图像进行分类\n",
    "preds = model.predict(padded_dataset_sequences[df_train_dataset.shape[0]:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: [[5.2982956e-01 2.2400606e-01 2.4616444e-01]\n",
      " [4.1301711e-03 2.1600150e-02 9.7426969e-01]\n",
      " [9.4749469e-01 4.7617290e-02 4.8880223e-03]\n",
      " ...\n",
      " [9.5940363e-01 3.4636229e-02 5.9601241e-03]\n",
      " [2.7177039e-01 5.9248245e-01 1.3574722e-01]\n",
      " [1.8757084e-04 5.3510573e-02 9.4630182e-01]]\n"
     ]
    }
   ],
   "source": [
    "# 输出预测概率\n",
    "print('Predicted:', preds.view())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65499"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.22400606\n"
     ]
    }
   ],
   "source": [
    "for i in preds:\n",
    "    print(out_classes(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def out_classes(preds):\n",
    "    if(preds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = np.argmax(preds,axis=1)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"result.txt\", result,fmt=\"%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 3, 1, 2, 1, 1, 2, 2, 3, 2, 1, 2, 2, 2, 2, 1, 3, 2, 1, 2])"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.52982956, 0.22400606, 0.24616444],\n",
       "       [0.00413017, 0.02160015, 0.9742697 ],\n",
       "       [0.9474947 , 0.04761729, 0.00488802],\n",
       "       [0.28808346, 0.6831014 , 0.02881507],\n",
       "       [0.4397474 , 0.3284209 , 0.23183167],\n",
       "       [0.88036036, 0.10871738, 0.01092222],\n",
       "       [0.00354895, 0.88335264, 0.11309841],\n",
       "       [0.3168921 , 0.4097239 , 0.273384  ],\n",
       "       [0.00416835, 0.16871381, 0.8271178 ],\n",
       "       [0.3726424 , 0.57038784, 0.0569697 ],\n",
       "       [0.72266513, 0.21734984, 0.05998506],\n",
       "       [0.22898637, 0.76364267, 0.00737093],\n",
       "       [0.22595192, 0.7005057 , 0.07354242],\n",
       "       [0.0170265 , 0.6762607 , 0.3067128 ],\n",
       "       [0.09329221, 0.74562746, 0.16108032],\n",
       "       [0.8487967 , 0.14513786, 0.00606542],\n",
       "       [0.3381511 , 0.31342646, 0.3484225 ],\n",
       "       [0.22215328, 0.58964825, 0.18819852]], dtype=float32)"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds[:18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对图像进行分类\n",
    "preds = model.predict(padded_dataset_sequences[df_train_dataset.shape[0]:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 3, 1, 2, 3, 1, 2, 2, 3, 1, 1, 2, 2, 2, 2, 1, 2, 2, 1, 2])"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(preds,axis=1)[:20]+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array([1, 3, 1, 2, 1, 1, 2, 2, 3, 2, 1, 2, 2, 2, 2, 1, 3, 2, 1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "r =np.argmax(preds,axis=1)[:]+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 3, 1, ..., 1, 2, 3])"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"result.txt\", r,fmt=\"%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preds = model.predict(train_X[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd =np.argmax(train_preds,axis=1)[:20]+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    0.45\n",
       "3    0.35\n",
       "1    0.20\n",
       "dtype: float64"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(dd).value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6992     1\n",
       "13919    3\n",
       "16497    3\n",
       "13380    3\n",
       "12713    2\n",
       "14253    3\n",
       "6115     1\n",
       "11491    2\n",
       "9424     2\n",
       "19664    3\n",
       "10442    2\n",
       "9779     2\n",
       "2847     1\n",
       "9388     2\n",
       "10746    2\n",
       "5206     1\n",
       "9625     2\n",
       "14833    3\n",
       "2829     1\n",
       "16004    3\n",
       "4770     1\n",
       "18400    3\n",
       "17905    3\n",
       "12405    2\n",
       "7289     2\n",
       "1062     1\n",
       "17155    3\n",
       "4574     1\n",
       "19107    3\n",
       "11444    2\n",
       "        ..\n",
       "10464    2\n",
       "17342    3\n",
       "12516    2\n",
       "5383     1\n",
       "6357     1\n",
       "10304    2\n",
       "16256    3\n",
       "2486     1\n",
       "4933     1\n",
       "11548    2\n",
       "14971    3\n",
       "11549    2\n",
       "15682    3\n",
       "12299    2\n",
       "10772    2\n",
       "241      1\n",
       "16998    3\n",
       "2557     1\n",
       "5151     1\n",
       "8596     2\n",
       "8813     2\n",
       "12807    2\n",
       "10826    2\n",
       "18594    3\n",
       "19590    3\n",
       "12876    2\n",
       "13991    3\n",
       "4811     1\n",
       "1626     1\n",
       "15442    3\n",
       "Name: COMMLEVEL, Length: 16000, dtype: int64"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y.map(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = model.predict(padded_test_dataset_sequences, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 3, 1, 2, 3, 1, 2, 2, 3, 1, 1, 2, 2, 2, 2, 1, 2, 2, 1, 2])"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(test_preds,axis=1)[:20]+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds_w2v = model.predict(padded_test_dataset_sequences, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = np.argmax(test_preds_w2v,axis=1)[:]+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65499"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"result-w2v-true.txt\", w2v,fmt=\"%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v2 = np.argmax(test_preds_w2v2,axis=1)[:]+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds_w2v2 = model.predict(padded_test_dataset_sequences, batch_size=256)\n",
    "w2v2 = np.argmax(test_preds_w2v2,axis=1)[:]+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"result-w2v2-true.txt\", w2v2,fmt=\"%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65499"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_preds_w2v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(65499, 3)"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_preds_w2v2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3,)"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rr= np.argmax(test_preds_w2v2,axis=0)+1\n",
    "rr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(65499,)"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rr= np.argmax(test_preds_w2v2,axis=1)+1\n",
    "rr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.1859379e-01, 2.0351185e-02, 4.6105501e-01],\n",
       "       [4.7960702e-01, 2.3151167e-02, 4.9724177e-01],\n",
       "       [9.8482740e-01, 5.6926869e-03, 9.4800284e-03],\n",
       "       ...,\n",
       "       [9.9976319e-01, 2.3535574e-04, 1.4494467e-06],\n",
       "       [7.4019539e-01, 2.5365427e-01, 6.1502913e-03],\n",
       "       [2.6803804e-07, 4.1422775e-04, 9.9958557e-01]], dtype=float32)"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_preds_w2v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred =pd.read_csv(\"result-w2v-true.txt\",'r', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    0.495916\n",
       "3    0.288783\n",
       "1    0.215301\n",
       "Name: 0, dtype: float64"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pred[0].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0    0.350125\n",
       "1.0    0.349750\n",
       "2.0    0.300125\n",
       "Name: COMMLEVEL, dtype: float64"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    0.495916\n",
       "3    0.288783\n",
       "1    0.215301\n",
       "Name: 0, dtype: float64"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pred =pd.read_csv(\"result-w2v-true.txt\",'r', header=None)\n",
    "df_pred[0].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.454343\n",
       "3    0.320035\n",
       "2    0.225622\n",
       "Name: 0, dtype: float64"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pred =pd.read_csv(\"result-w2v2-true.txt\",'r', header=None)\n",
    "df_pred[0].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    0.412006\n",
       "1    0.302188\n",
       "3    0.285806\n",
       "Name: 0, dtype: float64"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pred =pd.read_csv(\"./submissions/textcnn/dsjyycxds_preliminary.txt\",'r', header=None)\n",
    "df_pred[0].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 5, 5, 2],\n",
       "       [9, 6, 2, 8],\n",
       "       [3, 7, 9, 1]])"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.array([[1, 5, 5, 2],\n",
    "              [9, 6, 2, 8],\n",
    "              [3, 7, 9, 1]])\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0]\n"
     ]
    }
   ],
   "source": [
    "print(np.argmax(a[:2], axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mBDCI2017-MingLue-master.zip.download\u001b[m\u001b[m  lr_char_ngram.csv\r\n",
      "Emb+MLP.ipynb                         lr_char_ngram.pkl\r\n",
      "Embedding+MLP.ipynb                   lr_word_ngram.csv\r\n",
      "\u001b[34mHierarchical-Attention-Network-master\u001b[m\u001b[m lr_word_ngram.pkl\r\n",
      "Notebook.ipynb                        lstm-text-classifier.ipynb\r\n",
      "One-Hot+MLP.ipynb                     main.py\r\n",
      "README.md                             \u001b[34mmodels\u001b[m\u001b[m\r\n",
      "__init__.py                           requirements.txt\r\n",
      "\u001b[34m__pycache__\u001b[m\u001b[m                           result-0.6468.txt\r\n",
      "char-ngram-bag-of-words-0.67.txt      result-w2v-true.txt\r\n",
      "\u001b[34mcheckpoints\u001b[m\u001b[m                           result-w2v2-true.txt\r\n",
      "cnn_text_classifier.ipynb             \u001b[34msubmissions\u001b[m\u001b[m\r\n",
      "config.py                             textcnn_model.png\r\n",
      "\u001b[34mdata\u001b[m\u001b[m                                  tfidf_text_classifier.ipynb\r\n",
      "dataset_inspur 01-45-27-966.csv       \u001b[34mtutorial\u001b[m\u001b[m\r\n",
      "dataset_inspur.csv                    \u001b[34mutils\u001b[m\u001b[m\r\n",
      "\u001b[34membeddings\u001b[m\u001b[m                            word-ngram-bag-of-words-0.66.txt\r\n",
      "fasttext.ipynb                        word-ngram-bag-of-words.ipynb\r\n",
      "keras-version.ipynb                   y_true.csv\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "165490,雨天游蓬莱，人很多，景色没有想象的好，\r",
      "\r\n",
      "165491,周末去的人不多，10点一开门就去了，给闺女办了护照25元送了一个小挎包，里面有地图护照还有50元迷币，午餐有38、48的套餐，母女两人差不多够吃。4.5小时体验了8个职业，基本不用排队，小朋友很喜欢吵着还要再来。可能是是地下室，空气流通不好，我待了5个小时出来头疼。临出门墙上挂着孩子的照片，25元一张，本来不想要，无奈看着她的萌照不忍心不要啊，可能大部分家长都是这个心理吧。一大一下200多的门票，没有大人可玩的项目，就是一个陪同票，觉得有点贵了。没带充电宝，给孩子拍了好多照片，最后没电了，服务中心也没有相应的服务，要是大人的服务再做细点就好了，还有出门的照片要是送给小朋友们岂不是更好。一张照片的成本也就1-2元，门票都那么高了，还要说25元，真是有点圈钱了。 查看全部\r",
      "\r\n",
      "165492,最喜欢这的水，太清凉了，还有这里啤酒是直接放到水沟里面的，，\r",
      "\r\n",
      "165493,特别差。各种导游只是为了赚钱。跟孔子不沾半毛钱关系。尤其是路上的马车特别坑\r",
      "\r\n",
      "165494,坐8号线森林公园南门站下直接到。里面有小山有湖有湿地，有专门的跑步道，\r",
      "\r\n",
      "165495,性价比不高，国人参观国门还收这么高门票。\r",
      "\r\n",
      "165496,要想真正体验青海湖之美，不论是骑车还是自驾，一定要好好沿着湖走，找到自己真正的心仪之地，停下来，才是青海湖的魅力所在。强烈建议不要去二郎剑，没什么可看的，票价天高，人满为患，根本到不了湖边，就一处乱石滩还全是垃圾。推荐去黑马河（最好晚上住一天），这里是离湖边最近的镇子，步行到湖边只需半个小时，人很少，还不要票，水面清澈。湖边会有一些当地人在卖手工艺品，也有牵着马和牦牛给你合影的，但因为不是旅游景点，他们都很和气，不会强买强卖。\r",
      "\r\n",
      "165497,值得一去，栈道很有意思，坐索道上下的，但是中途还是要爬，觉得累\r",
      "\r\n",
      "165498,主要还是青岛稍微有些冷，要不更好看\r",
      "\r\n",
      "165499,真不好玩。很无聊的一个地方。人也不多。\r",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!tail ./data/Preliminary-texting.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
