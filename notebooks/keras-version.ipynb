{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "\n",
    "\n",
    "from keras.layers import Input, Dense, Embedding, Conv1D, Conv2D, MaxPooling1D, MaxPool2D\n",
    "from keras.layers import Reshape, Flatten, Dropout, Concatenate\n",
    "from keras.layers import SpatialDropout1D, concatenate\n",
    "from keras.layers import GRU, Bidirectional, GlobalAveragePooling1D, GlobalMaxPooling1D\n",
    "from keras.layers.recurrent import LSTM,SimpleRNN\n",
    "from keras.layers import Activation\n",
    "\n",
    "from keras.callbacks import Callback\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "from keras.models import load_model\n",
    "\n",
    "from keras.utils.vis_utils import plot_model\n",
    "\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "import jieba\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入自定义库\n",
    "from utils.data_utils import clean_str\n",
    "from utils.data_utils import build_vocab\n",
    "from utils.data_utils import get_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 基本参数配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = '/Users/tsw/ScenicSpotReviews'\n",
    "\n",
    "W2V_DIR = BASE_DIR + '/embeddings/'\n",
    "\n",
    "TEXT_DATA_DIR = BASE_DIR + '/data/'\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 100\n",
    "\n",
    "MAX_NUM_WORDS = 33950\n",
    "MAX_NB_WORDS = 30000\n",
    "\n",
    "EMBEDDING_DIM = 300\n",
    "\n",
    "VALIDATION_SPLIT = 0.2\n",
    "\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dataset = pd.read_csv('./data/training-inspur.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COMMCONTENT_SEG = []\n",
    "\n",
    "for sent in df_dataset['COMMCONTENT']:\n",
    "\n",
    "    # Extract Sentence\n",
    "    sent = str(sent).strip()\n",
    "\n",
    "    sent = clean_str(sent)\n",
    "\n",
    "    stopwords = [\" \",\"!\",\"....................................................................\"]\n",
    "\n",
    "    seg_list = jieba.cut(sent, cut_all=False)\n",
    "\n",
    "    seg_list = [i for i in seg_list if i not in stopwords]\n",
    "    \n",
    "    COMMCONTENT_SEG.append(\" \".join(seg_list))\n",
    "df_dataset['COMMCONTENT_SEG'] = pd.DataFrame(COMMCONTENT_SEG,columns=['COMMCONTENT_SEG'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dataset = df_dataset[df_dataset['COMMCONTENT_SEG']!=\"\"]\n",
    "df_dataset = df_dataset.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构建 Vocab 、word2index、index2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab,vocab_freqs = build_vocab(df_dataset['COMMCONTENT_SEG'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = min(MAX_NB_WORDS, len(vocab_freqs)) + 2\n",
    "word2index = {x[0]: i+2 for i, x in enumerate(vocab_freqs.most_common(MAX_NB_WORDS))}\n",
    "word2index[\"PAD\"] = 0\n",
    "word2index[\"UNK\"] = 1\n",
    "index2word = {v:k for k, v in word2index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(X, y, train_ratio=0.8):\n",
    "    X = np.array(X)\n",
    "    # seq_lens = np.array(seq_lens)\n",
    "    data_size = len(X)\n",
    "\n",
    "    # Shuffle the data\n",
    "    shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "    X, y = X[shuffle_indices], y[shuffle_indices]\n",
    "\n",
    "    # Split into train and validation set\n",
    "    train_end_index = int(train_ratio*data_size)\n",
    "    train_X = X[:train_end_index]\n",
    "    train_y = y[:train_end_index]\n",
    "\n",
    "    valid_X = X[train_end_index:]\n",
    "    valid_y = y[train_end_index:]\n",
    "    \n",
    "    return train_X,train_y,valid_X,valid_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X,train_y,valid_X,valid_y = split_dataset(df_dataset['COMMCONTENT_SEG'], \n",
    "                                                df_dataset['COMMLEVEL'], \n",
    "                                                train_ratio=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 将词转换为 index 向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "\n",
    "tokenizer.fit_on_texts(df_dataset['COMMCONTENT_SEG'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X[12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.texts_to_sequences([train_X[12]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sequences = tokenizer.texts_to_sequences(train_X)\n",
    "test_sequences = tokenizer.texts_to_sequences(valid_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_train_sequences = pad_sequences(train_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "padded_test_sequences = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_test_sequences[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_train_sequences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Indexing word embeddings.')  \n",
    "embeddings_index = {}\n",
    "with open('./embeddings/zhihu.vec','r') as f:\n",
    "    for i in f:\n",
    "        values = i.split(' ')\n",
    "        word = str(values[0])\n",
    "        embedding = np.asarray(values[1:],dtype='float')\n",
    "        embeddings_index[word] = embedding\n",
    "print('word embedding',len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_words = min(MAX_NB_WORDS,len(word2index))\n",
    "nb_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embedding_matrix = np.zeros((nb_words + 1, EMBEDDING_DIM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word, i in word2index.items():\n",
    "    if i > MAX_NB_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(str(word).upper())\n",
    "    if embedding_vector is not None:\n",
    "        word_embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_embedding_matrix[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(nb_words + 1,\n",
    "                                            EMBEDDING_DIM,\n",
    "                                             weights=[word_embedding_matrix],\n",
    "                 input_length=MAX_SEQUENCE_LENGTH,\n",
    "#                             mask_zero=True,\n",
    "                 trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_train_sequences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "#写一个LossHistory类，保存loss和acc\n",
    "class LossHistory(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = {'batch': [], 'epoch': []}\n",
    "        self.accuracy = {'batch': [], 'epoch': []}\n",
    "        self.val_loss = {'batch': [], 'epoch': []}\n",
    "        self.val_acc = {'batch': [], 'epoch': []}\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        self.losses['batch'].append(logs.get('loss'))\n",
    "        self.accuracy['batch'].append(logs.get('acc'))\n",
    "        self.val_loss['batch'].append(logs.get('val_loss'))\n",
    "        self.val_acc['batch'].append(logs.get('val_acc'))\n",
    "\n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        self.losses['epoch'].append(logs.get('loss'))\n",
    "        self.accuracy['epoch'].append(logs.get('acc'))\n",
    "        self.val_loss['epoch'].append(logs.get('val_loss'))\n",
    "        self.val_acc['epoch'].append(logs.get('val_acc'))\n",
    "\n",
    "    def loss_plot(self, loss_type):\n",
    "        iters = range(len(self.losses[loss_type]))\n",
    "        #创建一个图\n",
    "        plt.figure()\n",
    "        # acc\n",
    "        plt.plot(iters, self.accuracy[loss_type], 'r', label='train acc')#plt.plot(x,y)，这个将数据画成曲线\n",
    "        # loss\n",
    "        plt.plot(iters, self.losses[loss_type], 'g', label='train loss')\n",
    "        if loss_type == 'epoch':\n",
    "            # val_acc\n",
    "            plt.plot(iters, self.val_acc[loss_type], 'b', label='val acc')\n",
    "            # val_loss\n",
    "            plt.plot(iters, self.val_loss[loss_type], 'k', label='val loss')\n",
    "        plt.grid(True)#设置网格形式\n",
    "        plt.xlabel(loss_type)\n",
    "        plt.ylabel('acc-loss')#给x，y轴加注释\n",
    "        plt.legend(loc=\"upper right\")#设置图例显示位置\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"build model...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_model(self, params):\n",
    "        \"\"\"\n",
    "        Build un-compiled model of shallow-and-wide CNN\n",
    "        Args:\n",
    "            params: dictionary of parameters for NN\n",
    "        Returns:\n",
    "            Un-compiled model\n",
    "        \"\"\"\n",
    "\n",
    "        inp = Input(shape=(params['text_size'], params['embedding_size']))\n",
    "\n",
    "        outputs = []\n",
    "        for i in range(len(params['kernel_sizes_cnn'])):\n",
    "            output_i = Conv1D(params['filters_cnn'], kernel_size=params['kernel_sizes_cnn'][i],\n",
    "                              activation=None,\n",
    "                              kernel_regularizer=l2(params['coef_reg_cnn']),\n",
    "                              padding='same')(inp)\n",
    "            output_i = BatchNormalization()(output_i)\n",
    "            output_i = Activation('relu')(output_i)\n",
    "            output_i = GlobalMaxPooling1D()(output_i)\n",
    "            outputs.append(output_i)\n",
    "\n",
    "        output = concatenate(outputs, axis=1)\n",
    "\n",
    "        output = Dropout(rate=params['dropout_rate'])(output)\n",
    "        output = Dense(params['dense_size'], activation=None,\n",
    "                       kernel_regularizer=l2(params['coef_reg_den']))(output)\n",
    "        output = BatchNormalization()(output)\n",
    "        output = Activation('relu')(output)\n",
    "        output = Dropout(rate=params['dropout_rate'])(output)\n",
    "        output = Dense(self.n_classes, activation=None,\n",
    "                       kernel_regularizer=l2(params['coef_reg_den']))(output)\n",
    "        output = BatchNormalization()(output)\n",
    "        act_output = Activation(params.get(\"last_layer_activation\", \"sigmoid\"))(output)\n",
    "        model = Model(inputs=inp, outputs=act_output)\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = cnn_model()\n",
    "batch_size = 256\n",
    "epochs = 8\n",
    "\n",
    "# model.fit(x_train, y_train,\n",
    "#           validation_split=0.1,\n",
    "#           batch_size=batch_size,\n",
    "#           epochs=epochs,\n",
    "#           shuffle=True)\n",
    "#创建一个实例LossHistory\n",
    "history = LossHistory()\n",
    "\n",
    "model.fit(x=padded_train_sequences[:], y=to_categorical(train_y-1, num_classes=None)[:], \n",
    "                    validation_data=(padded_test_sequences[:], to_categorical(valid_y-1, num_classes=None)[:]), \n",
    "                    batch_size=batch_size, \n",
    "                    #callbacks=[checkpoint],\n",
    "                    callbacks=[history],\n",
    "                    epochs=epochs,\n",
    "                    verbose=1\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_cnn(maxlen=MAX_SEQUENCE_LENGTH, max_features=2000, embed_size=32):\n",
    "    \n",
    "    # Inputs\n",
    "    sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "\n",
    "    # Embeddings layers\n",
    "    emb_comment =  Embedding(input_dim = nb_words+1, \n",
    "                             output_dim = EMBEDDING_DIM, \n",
    "                             weights=[word_embedding_matrix], \n",
    "                             input_length=MAX_SEQUENCE_LENGTH, \n",
    "                             mask_zero=False,\n",
    "                             trainable=False)(sequence_input)\n",
    "\n",
    "    # conv layers\n",
    "    convs = []\n",
    "    \n",
    "    filter_sizes = [2, 3, 4, 5]\n",
    "    \n",
    "    for fsz in filter_sizes:\n",
    "        l_conv = Conv1D(filters=100, kernel_size=fsz, activation='relu')(emb_comment)\n",
    "        \n",
    "        l_pool = MaxPooling1D(maxlen - fsz + 1)(l_conv)\n",
    "        \n",
    "        l_pool = Flatten()(l_pool)\n",
    "        \n",
    "        convs.append(l_pool)\n",
    "    \n",
    "    merge = concatenate(convs, axis=1)\n",
    "\n",
    "    out = Dropout(0.5)(merge)\n",
    "    \n",
    "    output = Dense(32, activation='relu')(out)\n",
    "\n",
    "    output = Dense(len(np.unique(valid_y)), activation='softmax')(output)\n",
    "\n",
    "    # model = Model([sequence_input], output)\n",
    "    # model = Model(inputs=sequence_input, output)\n",
    "    model = Model(sequence_input, output)\n",
    "    \n",
    "    #  adam = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "    #  model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=['accuracy'])\n",
    "    # 优化器我这里用了adadelta，也可以使用其他方法\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='Adadelta', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = text_cnn()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "epochs = 8\n",
    "\n",
    "# model.fit(x_train, y_train,\n",
    "#           validation_split=0.1,\n",
    "#           batch_size=batch_size,\n",
    "#           epochs=epochs,\n",
    "#           shuffle=True)\n",
    "#创建一个实例LossHistory\n",
    "history = LossHistory()\n",
    "\n",
    "model.fit(x=padded_train_sequences[:], y=to_categorical(train_y-1, num_classes=None)[:], \n",
    "                    validation_data=(padded_test_sequences[:], to_categorical(valid_y-1, num_classes=None)[:]), \n",
    "                    batch_size=batch_size, \n",
    "                    #callbacks=[checkpoint],\n",
    "                    callbacks=[history],\n",
    "                    epochs=epochs,\n",
    "                    verbose=1\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score, acc = model.evaluate(padded_test_sequences[:],to_categorical(valid_y-1, num_classes=None)[:], batch_size=batch_size)\n",
    "#     print(score, acc)\n",
    "print('test_loss: %f, accuracy: %f' % (score, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.loss_plot('epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model, \n",
    "           to_file='./textcnn_model.png', \n",
    "           show_shapes=True, \n",
    "           show_layer_names=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model\n",
    "![](./textcnn_model.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train a 1D convnet with global maxpoolinnb_wordsg\n",
    "#left model 第一块神经网络，卷积窗口是5*50（50是词向量维度）\n",
    "model_left = Sequential()\n",
    "# model.add(Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32'))\n",
    "model_left.add(embedding_layer)\n",
    "model_left.add(Conv1D(128, 5, activation='tanh'))\n",
    "model_left.add(MaxPooling1D(5))\n",
    "model_left.add(Conv1D(128, 5, activation='tanh'))\n",
    "model_left.add(MaxPooling1D(5))\n",
    "model_left.add(Conv1D(128, 5, activation='tanh'))\n",
    "model_left.add(MaxPooling1D(18))\n",
    "model_left.add(Flatten())\n",
    "\n",
    "#right model <span style=\"font-family:Arial, Helvetica, sans-serif;\">第二块神经网络，卷积窗口是4*50</span>\n",
    "\n",
    "model_right = Sequential()\n",
    "model_right.add(embedding_layer)\n",
    "model_right.add(Conv1D(128, 4, activation='tanh'))\n",
    "model_right.add(MaxPooling1D(4))\n",
    "model_right.add(Conv1D(128, 4, activation='tanh'))\n",
    "model_right.add(MaxPooling1D(4))\n",
    "model_right.add(Conv1D(128, 4, activation='tanh'))\n",
    "model_right.add(MaxPooling1D(28))\n",
    "model_right.add(Flatten())\n",
    "\n",
    "#third model <span style=\"font-family:Arial, Helvetica, sans-serif;\">第三块神经网络，卷积窗口是6*50</span>\n",
    "model_3 = Sequential()\n",
    "model_3.add(embedding_layer)\n",
    "model_3.add(Conv1D(128, 6, activation='tanh'))\n",
    "model_3.add(MaxPooling1D(3))\n",
    "model_3.add(Conv1D(128, 6, activation='tanh'))\n",
    "model_3.add(MaxPooling1D(3))\n",
    "model_3.add(Conv1D(128, 6, activation='tanh'))\n",
    "model_3.add(MaxPooling1D(30))\n",
    "model_3.add(Flatten())\n",
    "\n",
    "merged = Concatenate([model_left, model_right,model_3])\n",
    "# out = Concatenate()([model_left.output, model_right.output,model_3.output])\n",
    "# merged = keras.layers.Merge([model_left, model_right,model_3], mode='concat') # 将三种不同卷积窗口的卷积层组合 连接在一起，当然也可以只是用三个model中的一个，一样可以得到不错的效果，只是本文采用论文中的结构设计\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(model_3) # add merge\n",
    "# model.add(Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32'))\n",
    "\n",
    "model.add(Dense(128, activation='tanh')) # 全连接层\n",
    "\n",
    "# softmax，输出文本属于20种类别中每个类别的概率\n",
    "model.add(Dense(len(np.unique(valid_y)), activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32', name='main_input')\n",
    "# preds = Dense(len(np.unique(valid_y)), activation='softmax')(x)\n",
    "\n",
    "# model = Model()\n",
    "# merged.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 优化器我这里用了adadelta，也可以使用其他方法\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='Adadelta',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# happy learning!\n",
    "# model.fit(x_train, y_train, validation_data=(x_val, y_val),\n",
    "#           nb_epoch=2, batch_size=128)\n",
    "batch_size = 256\n",
    "epochs = 5\n",
    "model.fit(x=padded_train_sequences[:], y=to_categorical(train_y-1, num_classes=None)[:], \n",
    "                    validation_data=(padded_test_sequences[:], to_categorical(valid_y-1, num_classes=None)[:]), \n",
    "                    batch_size=128, \n",
    "                    #callbacks=[checkpoint], \n",
    "                    epochs=2,\n",
    "                    verbose=1\n",
    "         )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score, acc = model.evaluate(padded_test_sequences[:],to_categorical(valid_y-1, num_classes=None)[:], batch_size=batch_size)\n",
    "print(score, acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_l= Sequential()  \n",
    "model_l.add(Dense(50, input_shape=(784,)))  \n",
    "model_l.add(Activation('relu'))  \n",
    "   \n",
    "model_r = Sequential()  \n",
    "model_r.add(Dense(50, input_shape=(784,)))  \n",
    "model_r.add(Activation('relu'))  \n",
    "   \n",
    "model = Sequential()\n",
    "# merged = concatenate([model_left, model_right,model_3])\n",
    "model.add(concatenate([model_l, model_r]))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32', name='main_input')\n",
    "x =Embedding(nb_words + 1,\n",
    "                 EMBEDDING_DIM,\n",
    "                 weights=[word_embedding_matrix],\n",
    "                 input_length=MAX_SEQUENCE_LENGTH,\n",
    "                 mask_zero=True,\n",
    "                 trainable=False)\n",
    "\n",
    "drop_out = Dropout(0.1, name='dropout')(x)\n",
    "\n",
    "w_aspect = Lambda(get_aspect, output_shape=(emb,), name=\"w_aspect\")(drop_out)\n",
    "\n",
    "w_context = Lambda(get_context, output_shape=(maxlen-1,emb), name=\"w_context\")(drop_out)\n",
    "\n",
    "w_aspect = Dense(emb, W_regularizer=l2(0.01), name=\"w_aspect_1\")(w_aspect)\n",
    "\n",
    "# hop 1\n",
    "w_aspects = RepeatVector(maxlen-1, name=\"w_aspects1\")(w_aspect)\n",
    "\n",
    "merged = merge([w_context, w_aspects], name='merged1', mode='concat')\n",
    "\n",
    "distributed = TimeDistributed(Dense(1, W_regularizer=l2(0.01), activation='tanh'), name=\"distributed1\")(merged)\n",
    "\n",
    "flat_alpha = Flatten(name=\"flat_alpha1\")(distributed)\n",
    "\n",
    "alpha = Dense(maxlen-1, activation='softmax', name=\"alpha1\")(flat_alpha)\n",
    "\n",
    "w_context_trans = Permute((2, 1), name=\"w_context_trans1\")(w_context)\n",
    "\n",
    "r_ = merge([w_context_trans, alpha], output_shape=(emb, 1), name=\"r_1\", mode=get_R)\n",
    "\n",
    "r = Reshape((emb,), name=\"r1\")(r_)\n",
    "\n",
    "w_aspect_linear = Dense(emb, W_regularizer=l2(0.01), activation='linear')(w_aspect)\n",
    "\n",
    "merged = merge([r, w_aspect_linear], mode='sum')\n",
    "\n",
    "w_aspect = Dense(emb, W_regularizer=l2(0.01), name=\"w_aspect_2\")(merged)\n",
    "\n",
    "# hop 2\n",
    "w_aspects = RepeatVector(maxlen-1, name=\"w_aspects2\")(w_aspect)\n",
    "merged = merge([w_context, w_aspects], name='merged2', mode='concat')\n",
    "distributed = TimeDistributed(Dense(1, W_regularizer=l2(0.01), activation='tanh'), name=\"distributed2\")(merged)\n",
    "flat_alpha = Flatten(name=\"flat_alpha2\")(distributed)\n",
    "alpha = Dense(maxlen-1, activation='softmax', name=\"alpha2\")(flat_alpha)\n",
    "w_context_trans = Permute((2, 1), name=\"w_context_trans2\")(w_context)\n",
    "r_ = merge([w_context_trans, alpha], output_shape=(emb, 1), name=\"r_2\", mode=get_R)\n",
    "r = Reshape((emb,), name=\"r2\")(r_)\n",
    "w_aspect_linear = Dense(emb, W_regularizer=l2(0.01), activation='linear')(w_aspect)\n",
    "merged = merge([r, w_aspect_linear], mode='sum')\n",
    "\n",
    "w_aspect = Dense(emb, W_regularizer=l2(0.01), name=\"w_aspect_3\")(merged)\n",
    "\n",
    "# hop 3\n",
    "w_aspects = RepeatVector(maxlen-1, name=\"w_aspects3\")(w_aspect)\n",
    "merged = merge([w_context, w_aspects], name='merged3', mode='concat')\n",
    "distributed = TimeDistributed(Dense(1, W_regularizer=l2(0.01), activation='tanh'), name=\"distributed3\")(merged)\n",
    "flat_alpha = Flatten(name=\"flat_alpha3\")(distributed)\n",
    "alpha = Dense(maxlen-1, activation='softmax', name=\"alpha3\")(flat_alpha)\n",
    "w_context_trans = Permute((2, 1), name=\"w_context_trans3\")(w_context)\n",
    "r_ = merge([w_context_trans, alpha], output_shape=(emb, 1), name=\"r_3\", mode=get_R)\n",
    "r = Reshape((emb,), name=\"r3\")(r_)\n",
    "w_aspect_linear = Dense(emb, W_regularizer=l2(0.01), activation='linear')(w_aspect)\n",
    "merged = merge([r, w_aspect_linear], mode='sum')\n",
    "\n",
    "h_ = Activation('tanh')(merged)\n",
    "\n",
    "out = Dense(3, activation='softmax')(h_)\n",
    "\n",
    "output = out\n",
    "\n",
    "model = Model(input=[main_input], output=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(embedding_layer)\n",
    "model.add(LSTM(200, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(120))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(len(np.unique(valid_y)),activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filepath=\"./weights-improvement-{epoch:02d}-{val_acc:.3f}.hdf5\"\n",
    "# checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "batch_size = 256\n",
    "epochs = 5\n",
    "model.fit(x=padded_train_sequences[:], \n",
    "                    y=to_categorical(train_y-1, num_classes=None)[:], \n",
    "                    validation_data=(padded_test_sequences[:], to_categorical(valid_y-1, num_classes=None)[:]), \n",
    "                    batch_size=batch_size, \n",
    "                    #callbacks=[checkpoint], \n",
    "                    epochs=epochs,\n",
    "                    verbose=1     \n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " score, acc = model.evaluate(padded_test_sequences[:],to_categorical(valid_y-1, num_classes=None)[:], batch_size=batch_size)\n",
    "    print(score, acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(score, acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "preds = Dense(len(np.unique(valid_y)), activation='softmax')(x)\n",
    "embedded_sequences = embedding_layer(sequence_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Conv1D(128, 5, activation='relu')(embedded_sequences)\n",
    "x = MaxPooling1D(5)(x)\n",
    "x = Conv1D(128, 5, activation='relu')(x)\n",
    "x = MaxPooling1D(5)(x)\n",
    "x = Conv1D(128, 5, activation='relu')(x)\n",
    "x = MaxPooling1D(18)(x)  # global max pooling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Flatten()(x)\n",
    "x = Dense(128, activation='relu')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = Dense(len(np.unique(valid_y)), activation='softmax')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(sequence_input, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# happy learning!\n",
    "# model.fit(x_train, y_train, validation_data=(x_val, y_val),\n",
    "#           nb_epoch=2, batch_size=128)\n",
    "model.fit(x=padded_train_sequences[:], y=to_categorical(train_y-1, num_classes=None)[:], \n",
    "                    validation_data=(padded_test_sequences[:], to_categorical(valid_y-1, num_classes=None)[:]), \n",
    "                    batch_size=128, \n",
    "                    #callbacks=[checkpoint], \n",
    "                    epochs=2,\n",
    "                    verbose=1\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()  # or Graph or whatever\n",
    "model.add(embedding_layer)  # Adding Input Length\n",
    "model.add(LSTM(units=50, activation='tanh'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(3, activation='softmax')) # Dense=>全连接层,输出维度=3\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x=padded_train_sequences[:], y=to_categorical(train_y-1, num_classes=None)[:], \n",
    "                    validation_data=(padded_test_sequences[:], to_categorical(valid_y-1, num_classes=None)[:]), \n",
    "                    batch_size=128, \n",
    "                    #callbacks=[checkpoint], \n",
    "                    epochs=2,\n",
    "                    verbose=1\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##定义网络结构\n",
    "def train_lstm(n_symbols,embedding_weights,x_train,y_train,x_test,y_test):\n",
    "    print 'Defining a Simple Keras Model...'\n",
    "    model = Sequential()  # or Graph or whatever\n",
    "    model.add(Embedding(output_dim=vocab_dim,\n",
    "                        input_dim=n_symbols,\n",
    "                        mask_zero=True,\n",
    "                        weights=[embedding_weights],\n",
    "                        input_length=input_length))  # Adding Input Length\n",
    "    model.add(LSTM(output_dim=50, activation='tanh'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(3, activation='softmax')) # Dense=>全连接层,输出维度=3\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    print 'Compiling the Model...'\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',metrics=['accuracy'])\n",
    "\n",
    "    print \"Train...\" # batch_size=32\n",
    "    model.fit(x_train, y_train, batch_size=batch_size, epochs=n_epoch,verbose=1)\n",
    "\n",
    "    print \"Evaluate...\"\n",
    "    score = model.evaluate(x_test, y_test,\n",
    "                                batch_size=batch_size)\n",
    "\n",
    "    yaml_string = model.to_yaml()\n",
    "    with open('../model/lstm.yml', 'w') as outfile:\n",
    "        outfile.write( yaml.dump(yaml_string, default_flow_style=True) )\n",
    "    model.save_weights('../model/lstm.h5')\n",
    "    print 'Test score:', score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_train_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH=588\n",
    "def get_simple_rnn_model():\n",
    "    embedding_dim = 300\n",
    "    \n",
    "    embedding_matrix = np.random.random((MAX_NB_WORDS, embedding_dim))\n",
    "    \n",
    "    inp = Input(shape=(MAX_LENGTH, ))\n",
    "    \n",
    "    x = Embedding(input_dim=MAX_NB_WORDS, output_dim=embedding_dim, input_length=MAX_LENGTH, \n",
    "                  weights=[embedding_matrix], trainable=True)(inp)\n",
    "    \n",
    "    x = SpatialDropout1D(0.3)(x)\n",
    "    \n",
    "    x = Bidirectional(GRU(100, return_sequences=True))(x)\n",
    "    \n",
    "    avg_pool = GlobalAveragePooling1D()(x)\n",
    "    \n",
    "    max_pool = GlobalMaxPooling1D()(x)\n",
    "    \n",
    "    conc = concatenate([avg_pool, max_pool])\n",
    "    \n",
    "    outp = Dense(input_dim=100, activation=\"softmax\", units=3)(conc)\n",
    "    \n",
    "    model = Model(inputs=inp, outputs=outp)\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "rnn_simple_model = get_simple_rnn_model()\n",
    "\n",
    "def get_simple_rnn_model():\n",
    "    MAX_LENGTH=588\n",
    "    \n",
    "    inp = Input(shape=(MAX_LENGTH, ))\n",
    "    \n",
    "    x = Embedding(nb_words + 1,\n",
    "                 EMBEDDING_DIM,\n",
    "                 weights=[word_embedding_matrix],\n",
    "                 input_length=MAX_SEQUENCE_LENGTH,\n",
    "                 trainable=False)(inp)\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    # RNN cell\n",
    "    model.add()\n",
    "    # dropout\n",
    "#     x = SpatialDropout1D(0.3)(x)\n",
    "    \n",
    "    x = Bidirectional(GRU(100, return_sequences=True))(x)\n",
    "    \n",
    "    avg_pool = GlobalAveragePooling1D()(x)\n",
    "    \n",
    "    max_pool = GlobalMaxPooling1D()(x)\n",
    "    \n",
    "    conc = concatenate([avg_pool, max_pool])\n",
    "    \n",
    "#     outp = Dense(input_dim=200, activation=\"softmax\", units=3)(conc)\n",
    "    outp = Dense(1, activation=\"softmax\")(conc)\n",
    "    \n",
    "    model = Model(inputs=inp, outputs=outp)\n",
    "    \n",
    "    # optimizer\n",
    "    model.compile(loss='sparse_categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('./weights-improvement-01-0.360.hdf5')\n",
    "\n",
    "y_pred_rnn_simple = model.predict(padded_test_sequences, verbose=1, batch_size=2048)\n",
    "\n",
    "y_pred_rnn_simple = pd.DataFrame(y_pred_rnn_simple, columns=['prediction'])\n",
    "y_pred_rnn_simple['prediction'] = y_pred_rnn_simple['prediction'].map(lambda p: 1 if p >= 0.5 else 0)\n",
    "y_pred_rnn_simple.to_csv('./y_pred_rnn_simple.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, auc, roc_auc_score\n",
    "y_pred_rnn_simple = pd.read_csv('./y_pred_rnn_simple.csv')\n",
    "print(accuracy_score(valid_y, y_pred_rnn_simple))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_simple_model = get_simple_rnn_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_simple_model = model()\n",
    "plot_model(rnn_simple_model, \n",
    "           to_file='./rnn_simple_model.png', \n",
    "           show_shapes=True, \n",
    "           show_layer_names=True)\n",
    "Model\n",
    "![](./rnn_simple_model.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model\n",
    "![](./rnn_simple_model.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pydot\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath=\"./weights-improvement-{epoch:02d}-{val_acc:.3f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "batch_size = 256\n",
    "epochs = 2\n",
    "\n",
    "history = model.fit(x=padded_train_sequences, \n",
    "                    y=train_y, \n",
    "                    validation_data=(padded_test_sequences, valid_y), \n",
    "                    batch_size=batch_size, \n",
    "                    callbacks=[checkpoint], \n",
    "                    epochs=epochs,\n",
    "                    verbose=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_rnn_simple_model = load_model('./weights-improvement-01-0.318.hdf5')\n",
    "\n",
    "y_pred_rnn_simple = best_rnn_simple_model.predict(padded_test_sequences, verbose=1, batch_size=2048)\n",
    "\n",
    "y_pred_rnn_simple = pd.DataFrame(y_pred_rnn_simple, columns=['prediction'])\n",
    "y_pred_rnn_simple['prediction'] = y_pred_rnn_simple['prediction'].map(lambda p: 1 if p >= 0.5 else 0)\n",
    "y_pred_rnn_simple.to_csv('./y_pred_rnn_simple.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, auc, roc_auc_score\n",
    "y_pred_rnn_simple = pd.read_csv('./y_pred_rnn_simple.csv')\n",
    "print(accuracy_score(valid_y, y_pred_rnn_simple))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./data/tweets.csv', encoding='latin1', usecols=['Sentiment', 'SentimentText'])\n",
    "data.columns = ['sentiment', 'text']\n",
    "data = data.sample(frac=1, random_state=42)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in data.head(10).iterrows():\n",
    "    print(row[1]['sentiment'], row[1]['text']) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "推文是有噪声的，让我们通过删除url（网址）、hashtag（主题标签）和user mentions（用户提及）来清除它们。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(tweet):\n",
    "    tweet = re.sub(r'http\\S+', '', tweet)\n",
    "    tweet = re.sub(r\"#(\\w+)\", '', tweet)\n",
    "    tweet = re.sub(r\"@(\\w+)\", '', tweet)\n",
    "    tweet = re.sub(r'[^\\w\\s]', '', tweet)\n",
    "    tweet = tweet.strip().lower()\n",
    "    tokens = word_tokenize(tweet)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['tokens'] = data.text.progress_map(tokenize)\n",
    "data['cleaned_text'] = data['tokens'].map(lambda tokens: ' '.join(tokens))\n",
    "data[['sentiment', 'cleaned_text']].to_csv('./data/cleaned_text.csv')\n",
    "\n",
    "data = pd.read_csv('./data/cleaned_text.csv')\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(data['cleaned_text'],\n",
    "                  data['sentiment'],\n",
    "                  test_size=0.1,\n",
    "                   random_state=42,\n",
    "                   stratify=data['sentiment'])\n",
    "\n",
    "print(x_train.shape, x_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(y_test).to_csv('./predictions/y_true.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 基于词ngrams的词袋模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_word = TfidfVectorizer(max_features=40000,\n",
    "                                  min_df=5,\n",
    "                                  max_df=0.5,\n",
    "                                  analyzer='word',\n",
    "                                  stop_words='english',\n",
    "                                  ngram_range=(1, 2))\n",
    "\n",
    "vectorizer_word.fit(x_train, leave=False)\n",
    "\n",
    "tfidf_matrix_word_train = vectorizer_word.transform(x_train)\n",
    "tfidf_matrix_word_test = vectorizer_word.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_word = LogisticRegression(solver='sag', verbose=2)\n",
    "lr_word.fit(tfidf_matrix_word_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(lr_word, './models/lr_word_ngram.pkl')\n",
    "\n",
    "y_pred_word = lr_word.predict(tfidf_matrix_word_test)\n",
    "pd.DataFrame(y_pred_word, columns=['y_pred']).to_csv('./predictions/lr_word_ngram.csv', \n",
    "index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_word = pd.read_csv('./predictions/lr_word_ngram.csv')\n",
    "print(accuracy_score(y_test, y_pred_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 基于字符ngrams的词袋模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentLSTM:\n",
    "    def __init__(self):\n",
    "        self.tokenizer = Tokenizer(num_words=vocab_size)\n",
    "        self.stop_words = []\n",
    "        self.model = None\n",
    "\n",
    "    def load_stop_word(self,path='dict/stop_word.txt'):\n",
    "        with open(path, 'r') as f:\n",
    "            for line in f:\n",
    "                content = line.strip()\n",
    "                self.stop_words.append(content.decode('utf-8'))\n",
    "\n",
    "    def jieba_cut(self,line):\n",
    "        lcut = jieba.lcut(line)\n",
    "        cut = [x for x in lcut if x not in self.stop_words]\n",
    "        cut = \" \".join(cut)\n",
    "        return cut\n",
    "\n",
    "    def load_cuted_corpus(self, dir, input):\n",
    "        f = open(dir + '/' + input , 'r')\n",
    "        lines = f.readlines()\n",
    "        texts = []\n",
    "        labels = []\n",
    "        for line in lines:\n",
    "            fields = line.split()\n",
    "            rate = int(fields[0])\n",
    "            if rate==0 or rate==3:\n",
    "                continue\n",
    "            elif rate < 3:\n",
    "                rate = 0\n",
    "            else:\n",
    "                rate = 1\n",
    "            cont = fields[1:]\n",
    "            cont = \" \".join(cont)\n",
    "            texts.append(cont)\n",
    "            labels.append(rate)\n",
    "\n",
    "        self.tokenizer.fit_on_texts(texts)\n",
    "        f.close()\n",
    "        return texts,labels\n",
    "\n",
    "    def load_data(self):\n",
    "        x,y = self.load_cuted_corpus('corpus', 'review.csv')\n",
    "        x = self.tokenizer.texts_to_sequences(x)\n",
    "        x = S.pad_sequences(x,maxlen=sentence_max_len)\n",
    "        y = to_categorical(y,num_classes=2)\n",
    "        return ((x[0:500000],y[0:500000]), (x[500000:], y[500000:]))\n",
    "\n",
    "    def train(self,epochs=50):\n",
    "        print 'building model ...'\n",
    "        self.model = SentimentLSTM.build_model()\n",
    "\n",
    "        print 'loading data ...'\n",
    "        (text_train, rate_train), (text_test, rate_text) = self.load_data()\n",
    "\n",
    "        print 'training model ...'\n",
    "        self.model.fit(text_train, rate_train,batch_size=1000,epochs=epochs)\n",
    "        self.model.save('model/keras.model')\n",
    "        score = self.model.evaluate(text_test,rate_text)\n",
    "        print score\n",
    "\n",
    "    def load_trained_model(self,path):\n",
    "        model = SentimentLSTM.build_model()\n",
    "        model.load_weights(path)\n",
    "        return model\n",
    "\n",
    "    def predict_text(self,text):\n",
    "        if self.model == None:\n",
    "            self.model = self.load_trained_model(model_path)\n",
    "            self.load_stop_word()\n",
    "            self.load_cuted_corpus('corpus', 'review.csv')\n",
    "\n",
    "        vect = self.jieba_cut(text)\n",
    "        vect = vect.encode('utf-8')\n",
    "        vect = self.tokenizer.texts_to_sequences([vect,])\n",
    "        print vect\n",
    "        return self.model.predict_classes(S.pad_sequences(np.array(vect),100))\n",
    "\n",
    "    @staticmethod\n",
    "    def build_model():\n",
    "        model = Sequential()\n",
    "        model.add(Embedding(vocab_size, 256, input_length=sentence_max_len))\n",
    "        model.add(Bidirectional(LSTM(128,implementation=2)))\n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Dense(2, activation='relu'))\n",
    "        model.compile('RMSprop', 'categorical_crossentropy', metrics=['accuracy'])\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    lstm = SentimentLSTM()\n",
    "    lstm.train(10)\n",
    "    while True:\n",
    "        input = raw_input('Please input text:')\n",
    "        if input == 'quit':\n",
    "            break\n",
    "        print lstm.predict_text(input)\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
