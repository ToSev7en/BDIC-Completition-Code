{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "from keras.models import load_model\n",
    "\n",
    "from keras.layers import Activation,BatchNormalization\n",
    "from keras.layers import Input, Dense, Embedding, Conv1D, Conv2D, MaxPooling1D, MaxPool2D\n",
    "from keras.layers import Reshape, Flatten, Dropout, Concatenate\n",
    "from keras.layers import SpatialDropout1D, concatenate, Lambda\n",
    "from keras.layers import GRU, Bidirectional, GlobalAveragePooling1D, GlobalMaxPooling1D\n",
    "from keras.layers.recurrent import LSTM,SimpleRNN\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from keras.callbacks import Callback, ModelCheckpoint, EarlyStopping\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "import jieba\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入自定义库\n",
    "from utils.data_utils import clean_str\n",
    "from utils.data_utils import build_vocab\n",
    "from utils.data_utils import get_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(102024, 2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 加载训练集合\n",
    "df_train_dataset = pd.read_csv('./data/training-inspur.csv', encoding='utf-8')\n",
    "# 加载测试集\n",
    "df_test_dataset = pd.read_csv('./data/Preliminary-texting-1.csv', encoding='utf-8')\n",
    "# 查看数据集合 shape\n",
    "df_test_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 提取数据集中所需的字段\n",
    "df_train_dataset = df_train_dataset[['COMMCONTENT', 'COMMLEVEL']]\n",
    "df_test_dataset = df_test_dataset[['COMMCONTENT']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(122024, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 合并数据集用于构建词汇表\n",
    "df_all_dataset = pd.concat([df_train_dataset, df_test_dataset], ignore_index=True)\n",
    "df_all_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = '/Users/tsw/ScenicSpotReviews'\n",
    "\n",
    "W2V_DIR = BASE_DIR + '/embeddings/'\n",
    "\n",
    "TEXT_DATA_DIR = BASE_DIR + '/data/'\n",
    "\n",
    "MAX_NUM_WORDS = 33950\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 150 # 每篇文章选取150个词\n",
    "\n",
    "MAX_NB_WORDS = 80000 # 将字典设置为含有1万个词84480\n",
    "\n",
    "EMBEDDING_DIM = 300 # 词向量维度，300维\n",
    "\n",
    "VALIDATION_SPLIT = 0.1 # 测试集大小，全部数据的10%\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "NUM_LABELS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seg_corpus(corpus):\n",
    "    seg_corpus = []\n",
    "    for line in corpus:\n",
    "        line = str(line).strip()\n",
    "        seg_list = jieba.cut(line, cut_all=False)\n",
    "        # 过滤空字符\n",
    "        seg_list = [w for w in seg_list if w != ' ']\n",
    "        seg_corpus.append(\" \".join(seg_list))\n",
    "    return seg_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/5n/2_by50851fxc4d_snc1d9wf80000gn/T/jieba.cache\n",
      "Loading model cost 0.859 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "# 对所有文本分词\n",
    "seged_text = seg_corpus(df_all_dataset['COMMCONTENT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>COMMCONTENT</th>\n",
       "      <th>COMMLEVEL</th>\n",
       "      <th>COMMCONTENT_SEG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>普通公园一个只是多了几个泉而已，人不多，适合老人孩子闲逛，买票的话还是贵了，人家说6.30之...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>普通 公园 一个 只是 多 了 几个 泉 而已 ， 人不多 ， 适合 老人 孩子 闲逛 ， ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>跟儿子在里面玩了一天，非常好！跟儿子在里面玩了一天，非常好！真的很不错哦，有空还要去</td>\n",
       "      <td>1.0</td>\n",
       "      <td>跟 儿子 在 里面 玩 了 一天 ， 非常 好 ！ 跟 儿子 在 里面 玩 了 一天 ， 非...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>这已经是第五次来这里玩了。每次孩子都很喜欢，不愿意从水里出来。有机会还会再来。还有比我更忠诚...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>这 已经 是 第五次 来 这里 玩 了 。 每次 孩子 都 很 喜欢 ， 不 愿意 从水里 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>当天在携程上定的票，打温泉度假村咨询电话和携程客服都说次日生效，但到酒店后，票能用。请客服人...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>当天 在 携程 上定 的 票 ， 打 温泉 度假村 咨询电话 和 携程 客服 都 说 次日 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>烟台历史的一部分，非常值得推荐去看看！海边景色也很漂亮！</td>\n",
       "      <td>1.0</td>\n",
       "      <td>烟台 历史 的 一部分 ， 非常 值得 推荐 去 看看 ！ 海边 景色 也 很漂亮 ！</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         COMMCONTENT  COMMLEVEL  \\\n",
       "0  普通公园一个只是多了几个泉而已，人不多，适合老人孩子闲逛，买票的话还是贵了，人家说6.30之...        1.0   \n",
       "1         跟儿子在里面玩了一天，非常好！跟儿子在里面玩了一天，非常好！真的很不错哦，有空还要去        1.0   \n",
       "2  这已经是第五次来这里玩了。每次孩子都很喜欢，不愿意从水里出来。有机会还会再来。还有比我更忠诚...        1.0   \n",
       "3  当天在携程上定的票，打温泉度假村咨询电话和携程客服都说次日生效，但到酒店后，票能用。请客服人...        1.0   \n",
       "4                       烟台历史的一部分，非常值得推荐去看看！海边景色也很漂亮！        1.0   \n",
       "\n",
       "                                     COMMCONTENT_SEG  \n",
       "0  普通 公园 一个 只是 多 了 几个 泉 而已 ， 人不多 ， 适合 老人 孩子 闲逛 ， ...  \n",
       "1  跟 儿子 在 里面 玩 了 一天 ， 非常 好 ！ 跟 儿子 在 里面 玩 了 一天 ， 非...  \n",
       "2  这 已经 是 第五次 来 这里 玩 了 。 每次 孩子 都 很 喜欢 ， 不 愿意 从水里 ...  \n",
       "3  当天 在 携程 上定 的 票 ， 打 温泉 度假村 咨询电话 和 携程 客服 都 说 次日 ...  \n",
       "4        烟台 历史 的 一部分 ， 非常 值得 推荐 去 看看 ！ 海边 景色 也 很漂亮 ！  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 将分词后的数据并入 df_all_dataset\n",
    "df_all_dataset['COMMCONTENT_SEG'] = pd.DataFrame(seged_text,columns=['COMMCONTENT_SEG'])\n",
    "df_all_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_corpus\n",
    "text_corpus = df_all_dataset['COMMCONTENT_SEG']\n",
    "# 传入我们词向量的字典\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS) \n",
    "# 传入我们的训练数据，得到训练数据中出现的词的字典\n",
    "tokenizer.fit_on_texts(text_corpus) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 根据训练数据中出现的词的字典，将训练数据转换为sequences\n",
    "dataset_sequences = tokenizer.texts_to_sequences(text_corpus) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 100134 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(122024, 150)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_dataset_sequences = pad_sequences(dataset_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "padded_dataset_sequences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_dataset_seg = df_all_dataset['COMMCONTENT_SEG'][20000:]\n",
    "test_dataset_sequences = tokenizer.texts_to_sequences(df_test_dataset_seg)\n",
    "padded_test_dataset_sequences = pad_sequences(test_dataset_sequences, maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 划分训练集和测试集\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_X,valid_X,train_y,valid_y =train_test_split(padded_dataset_sequences[:df_train_dataset.shape[0]], \n",
    "                                                  df_all_dataset['COMMLEVEL'][:df_train_dataset.shape[0]], \n",
    "                                                  test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label one-hot 表示\n",
    "labels = df_all_dataset['COMMLEVEL'].dropna().map(int)#.values.tolist()\n",
    "labels = to_categorical(labels-1) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80002"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab,vocab_freqs = build_vocab(df_all_dataset['COMMCONTENT_SEG'])\n",
    "vocab_size = min(MAX_NB_WORDS, len(vocab_freqs)) + 2\n",
    "word2index = {x[0]: i+2 for i, x in enumerate(vocab_freqs.most_common(MAX_NB_WORDS))}\n",
    "word2index[\"PAD\"] = 0\n",
    "word2index[\"UNK\"] = 1\n",
    "index2word = {v:k for k, v in word2index.items()}\n",
    "len(word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing word embeddings.\n",
      "word embedding 195201\n"
     ]
    }
   ],
   "source": [
    "print('Indexing word embeddings.')  \n",
    "embeddings_index = {}\n",
    "with open('./embeddings/sgns.weibo.word','r') as f:\n",
    "    f = f.readlines()\n",
    "    for i in f[1:]:\n",
    "        values = i.strip().split(' ')\n",
    "        word = str(values[0])\n",
    "        embedding = np.asarray(values[1:],dtype='float')\n",
    "        embeddings_index[word] = embedding\n",
    "print('word embedding',len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80000"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_words = min(MAX_NB_WORDS,len(word2index))\n",
    "nb_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embedding_matrix = np.zeros((nb_words + 1, EMBEDDING_DIM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word, i in word2index.items():\n",
    "    if i > MAX_NB_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(str(word).upper())\n",
    "    if embedding_vector is not None:\n",
    "        word_embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_embedding_layer =  Embedding(input_dim = nb_words+1, \n",
    "                             output_dim = EMBEDDING_DIM, \n",
    "                            weights=[word_embedding_matrix], \n",
    "                             input_length=MAX_SEQUENCE_LENGTH, \n",
    "                             mask_zero=True,\n",
    "                             trainable=False\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Lambda\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_conv1D_(emb_matrix, max_len):\n",
    "    \n",
    "    # The embedding layer containing the word vectors\n",
    "    emb_layer = Embedding(\n",
    "        input_dim=emb_matrix.shape[0],\n",
    "        output_dim=emb_matrix.shape[1],\n",
    "        weights=[emb_matrix],\n",
    "        input_length=max_len,\n",
    "        trainable=True\n",
    "    )\n",
    "    \n",
    "    # 1D convolutions that can iterate over the word vectors\n",
    "    conv1 = Conv1D(filters=128, kernel_size=1, padding='same', activation='relu')\n",
    "    conv2 = Conv1D(filters=128, kernel_size=2, padding='same', activation='relu')\n",
    "    conv3 = Conv1D(filters=128, kernel_size=3, padding='same', activation='relu')\n",
    "    conv4 = Conv1D(filters=128, kernel_size=4, padding='same', activation='relu')\n",
    "    conv5 = Conv1D(filters=32, kernel_size=5, padding='same', activation='relu')\n",
    "    conv6 = Conv1D(filters=32, kernel_size=6, padding='same', activation='relu')\n",
    "\n",
    "    # Define inputs\n",
    "    seq1 = Input(shape=(max_len,))\n",
    "#     seq2 = Input(shape=(max_len,))\n",
    "\n",
    "    # Run inputs through embedding\n",
    "    emb1 = emb_layer(seq1)\n",
    "#     emb2 = emb_layer(seq2)\n",
    "    emb1 = SpatialDropout1D(0.3)(emb1)\n",
    "    emb1 = Bidirectional(GRU(100, return_sequences=True))(emb1)\n",
    "\n",
    "    # Run through CONV + GAP layers\n",
    "    conv1a = conv1(emb1)\n",
    "    glob1a = GlobalAveragePooling1D()(conv1a)\n",
    "#     conv1b = conv1(emb2)\n",
    "#     glob1b = GlobalAveragePooling1D()(conv1b)\n",
    "\n",
    "    conv2a = conv2(emb1)\n",
    "    glob2a = GlobalAveragePooling1D()(conv2a)\n",
    "#     conv2b = conv2(emb2)\n",
    "#     glob2b = GlobalAveragePooling1D()(conv2b)\n",
    "\n",
    "    conv3a = conv3(emb1)\n",
    "    glob3a = GlobalAveragePooling1D()(conv3a)\n",
    "#     conv3b = conv3(emb2)\n",
    "#     glob3b = GlobalAveragePooling1D()(conv3b)\n",
    "\n",
    "    conv4a = conv4(emb1)\n",
    "    glob4a = GlobalAveragePooling1D()(conv4a)\n",
    "#     conv4b = conv4(emb2)\n",
    "#     glob4b = GlobalAveragePooling1D()(conv4b)\n",
    "\n",
    "    conv5a = conv5(emb1)\n",
    "    glob5a = GlobalAveragePooling1D()(conv5a)\n",
    "#     conv5b = conv5(emb2)\n",
    "#     glob5b = GlobalAveragePooling1D()(conv5b)\n",
    "\n",
    "    conv6a = conv6(emb1)\n",
    "    glob6a = GlobalAveragePooling1D()(conv6a)\n",
    "#     conv6b = conv6(emb2)\n",
    "#     glob6b = GlobalAveragePooling1D()(conv6b)\n",
    "\n",
    "    mergea = concatenate([glob1a, glob2a, glob3a, glob4a, glob5a, glob6a])\n",
    "#     mergeb = concatenate([glob1b, glob2b, glob3b, glob4b, glob5b, glob6b])\n",
    "\n",
    "    # We take the explicit absolute difference between the two sentences\n",
    "    # Furthermore we take the multiply different entries to get a different measure of equalness\n",
    "#     diff = Lambda(lambda x: K.abs(x[0] - x[1]), output_shape=(4 * 128 + 2*32,))([mergea, mergeb])\n",
    "#     mul = Lambda(lambda x: x[0] * x[1], output_shape=(4 * 128 + 2*32,))([mergea, mergeb])\n",
    "    \n",
    "#     diff = Lambda(lambda x: K.max(x), output_shape=(64,))(mergea)\n",
    "#     mul = Lambda(lambda x: x[0] * x[1], output_shape=(4 * 128 + 2*32,))([mergea])\n",
    "\n",
    "    # Add the magic features\n",
    "#     magic_input = Input(shape=(5,))\n",
    "#     magic_dense = BatchNormalization()(magic_input)\n",
    "#     magic_dense = Dense(64, activation='relu')(magic_dense)\n",
    "\n",
    "    # Add the distance features (these are now TFIDF (character and word), Fuzzy matching, \n",
    "    # nb char 1 and 2, word mover distance and skew/kurtosis of the sentence vector)\n",
    "#     distance_input = Input(shape=(20,))\n",
    "#     distance_dense = BatchNormalization()(distance_input)\n",
    "#     distance_dense = Dense(128, activation='relu')(distance_dense)\n",
    "\n",
    "    # Merge the Magic and distance features with the difference layer\n",
    "#     merge = concatenate([diff, mul, magic_dense, distance_dense])\n",
    "#     merge = concatenate([diff, mul])\n",
    "\n",
    "    # The MLP that determines the outcome\n",
    "    x = Dropout(0.2)(mergea)\n",
    "#     x = BatchNormalization()(x)\n",
    "    x = Dense(300, activation='relu')(x)\n",
    "\n",
    "#     x = Dropout(0.2)(x)\n",
    "#     x = BatchNormalization()(x)\n",
    "    pred = Dense(3, activation='softmax')(x)\n",
    "\n",
    "    # model = Model(inputs=[seq1, seq2, magic_input, distance_input], outputs=pred)\n",
    "#     model = Model(inputs=[seq1, seq2, magic_input, distance_input], outputs=pred)\n",
    "    model = Model(inputs=seq1, outputs=pred)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model_conv1D_(word_embedding_matrix,MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_22 (InputLayer)           (None, 150)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_17 (Embedding)        (None, 150, 300)     24000300    input_22[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_2 (SpatialDro (None, 150, 300)     0           embedding_17[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, 150, 200)     240600      spatial_dropout1d_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_97 (Conv1D)              (None, 150, 128)     25728       bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_98 (Conv1D)              (None, 150, 128)     51328       bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_99 (Conv1D)              (None, 150, 128)     76928       bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_100 (Conv1D)             (None, 150, 128)     102528      bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_101 (Conv1D)             (None, 150, 32)      32032       bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_102 (Conv1D)             (None, 150, 32)      38432       bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_127 (G (None, 128)          0           conv1d_97[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_128 (G (None, 128)          0           conv1d_98[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_129 (G (None, 128)          0           conv1d_99[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_130 (G (None, 128)          0           conv1d_100[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_131 (G (None, 32)           0           conv1d_101[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_132 (G (None, 32)           0           conv1d_102[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_25 (Concatenate)    (None, 576)          0           global_average_pooling1d_127[0][0\n",
      "                                                                 global_average_pooling1d_128[0][0\n",
      "                                                                 global_average_pooling1d_129[0][0\n",
      "                                                                 global_average_pooling1d_130[0][0\n",
      "                                                                 global_average_pooling1d_131[0][0\n",
      "                                                                 global_average_pooling1d_132[0][0\n",
      "__________________________________________________________________________________________________\n",
      "dropout_15 (Dropout)            (None, 576)          0           concatenate_25[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_12 (Dense)                (None, 300)          173100      dropout_15[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_13 (Dense)                (None, 3)            903         dense_12[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 24,741,879\n",
      "Trainable params: 24,741,879\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 18000 samples, validate on 2000 samples\n",
      "Epoch 1/5\n",
      "18000/18000 [==============================] - 242s 13ms/step - loss: 1.0613 - acc: 0.4230 - val_loss: 0.8996 - val_acc: 0.5765\n",
      "Epoch 2/5\n",
      "18000/18000 [==============================] - 224s 12ms/step - loss: 0.8330 - acc: 0.6076 - val_loss: 0.7803 - val_acc: 0.6460\n",
      "Epoch 3/5\n",
      "18000/18000 [==============================] - 222s 12ms/step - loss: 0.7190 - acc: 0.6742 - val_loss: 0.7369 - val_acc: 0.6600\n",
      "Epoch 4/5\n",
      "18000/18000 [==============================] - 223s 12ms/step - loss: 0.6280 - acc: 0.7291 - val_loss: 0.7648 - val_acc: 0.6635\n",
      "Epoch 5/5\n",
      "18000/18000 [==============================] - 220s 12ms/step - loss: 0.5308 - acc: 0.7839 - val_loss: 0.7685 - val_acc: 0.6710\n"
     ]
    }
   ],
   "source": [
    "batch_size = 256\n",
    "epochs = 5\n",
    "\n",
    "# model.fit(x=train_X, y=to_categorical(train_y-1, num_classes=3), \n",
    "#                     validation_data=(valid_X, to_categorical(valid_y-1, num_classes=3)[:]), \n",
    "#                     batch_size=batch_size, \n",
    "#                     epochs=epochs,\n",
    "#           shuffle=True,\n",
    "#                     verbose=1\n",
    "#          )\n",
    "history = model.fit(x=train_X, y=to_categorical(train_y-1, num_classes=3), \n",
    "                    validation_data=(valid_X, to_categorical(valid_y-1, num_classes=3)[:]), \n",
    "                    batch_size=batch_size, \n",
    "                    epochs=epochs,\n",
    "                    shuffle=True,\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 18000 samples, validate on 2000 samples\n",
      "Epoch 1/1\n",
      "18000/18000 [==============================] - 222s 12ms/step - loss: 0.4478 - acc: 0.8255 - val_loss: 0.8174 - val_acc: 0.6640\n"
     ]
    }
   ],
   "source": [
    "batch_size = 256\n",
    "epochs = 1\n",
    "\n",
    "# model.fit(x=train_X, y=to_categorical(train_y-1, num_classes=3), \n",
    "#                     validation_data=(valid_X, to_categorical(valid_y-1, num_classes=3)[:]), \n",
    "#                     batch_size=batch_size, \n",
    "#                     epochs=epochs,\n",
    "#           shuffle=True,\n",
    "#                     verbose=1\n",
    "#          )\n",
    "history = model.fit(x=train_X, y=to_categorical(train_y-1, num_classes=3), \n",
    "                    validation_data=(valid_X, to_categorical(valid_y-1, num_classes=3)[:]), \n",
    "                    batch_size=batch_size, \n",
    "                    epochs=epochs,\n",
    "                    shuffle=True,\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    0.377911\n",
       "1    0.332098\n",
       "3    0.289991\n",
       "dtype: float64"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_test_preds = model.predict(padded_test_dataset_sequences, batch_size=1024)\n",
    "conv1d = np.argmax(all_test_preds,axis=1)[:]+1\n",
    "pd.Series(conv1d).value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"lstm+conv1d-0.6640-0.33-0.37-0.28.txt\", conv1d,fmt=\"%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 18000 samples, validate on 2000 samples\n",
      "Epoch 1/1\n",
      "18000/18000 [==============================] - 104s 6ms/step - loss: 0.8070 - acc: 0.6355 - val_loss: 0.8080 - val_acc: 0.6255\n"
     ]
    }
   ],
   "source": [
    "batch_size = 256\n",
    "epochs = 1\n",
    "\n",
    "# model.fit(x=train_X, y=to_categorical(train_y-1, num_classes=3), \n",
    "#                     validation_data=(valid_X, to_categorical(valid_y-1, num_classes=3)[:]), \n",
    "#                     batch_size=batch_size, \n",
    "#                     epochs=epochs,\n",
    "#           shuffle=True,\n",
    "#                     verbose=1\n",
    "#          )\n",
    "history = model.fit(x=train_X, y=to_categorical(train_y-1, num_classes=3), \n",
    "                    validation_data=(valid_X, to_categorical(valid_y-1, num_classes=3)[:]), \n",
    "                    batch_size=batch_size, \n",
    "                    epochs=epochs,\n",
    "                    shuffle=True,\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 18000 samples, validate on 2000 samples\n",
      "Epoch 1/1\n",
      "18000/18000 [==============================] - 99s 6ms/step - loss: 0.7753 - acc: 0.6542 - val_loss: 0.8023 - val_acc: 0.6280\n"
     ]
    }
   ],
   "source": [
    "batch_size = 256\n",
    "epochs = 1\n",
    "\n",
    "# model.fit(x=train_X, y=to_categorical(train_y-1, num_classes=3), \n",
    "#                     validation_data=(valid_X, to_categorical(valid_y-1, num_classes=3)[:]), \n",
    "#                     batch_size=batch_size, \n",
    "#                     epochs=epochs,\n",
    "#           shuffle=True,\n",
    "#                     verbose=1\n",
    "#          )\n",
    "history = model.fit(x=train_X, y=to_categorical(train_y-1, num_classes=3), \n",
    "                    validation_data=(valid_X, to_categorical(valid_y-1, num_classes=3)[:]), \n",
    "                    batch_size=batch_size, \n",
    "                    epochs=epochs,\n",
    "                    shuffle=True,\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 18000 samples, validate on 2000 samples\n",
      "Epoch 1/1\n",
      "18000/18000 [==============================] - 101s 6ms/step - loss: 0.7448 - acc: 0.6731 - val_loss: 0.7807 - val_acc: 0.6420\n"
     ]
    }
   ],
   "source": [
    "batch_size = 256\n",
    "epochs = 1\n",
    "\n",
    "# model.fit(x=train_X, y=to_categorical(train_y-1, num_classes=3), \n",
    "#                     validation_data=(valid_X, to_categorical(valid_y-1, num_classes=3)[:]), \n",
    "#                     batch_size=batch_size, \n",
    "#                     epochs=epochs,\n",
    "#           shuffle=True,\n",
    "#                     verbose=1\n",
    "#          )\n",
    "history = model.fit(x=train_X, y=to_categorical(train_y-1, num_classes=3), \n",
    "                    validation_data=(valid_X, to_categorical(valid_y-1, num_classes=3)[:]), \n",
    "                    batch_size=batch_size, \n",
    "                    epochs=epochs,\n",
    "                    shuffle=True,\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 18000 samples, validate on 2000 samples\n",
      "Epoch 1/1\n",
      "18000/18000 [==============================] - 103s 6ms/step - loss: 0.7223 - acc: 0.6852 - val_loss: 0.7771 - val_acc: 0.6420\n"
     ]
    }
   ],
   "source": [
    "batch_size = 256\n",
    "epochs = 1\n",
    "\n",
    "# model.fit(x=train_X, y=to_categorical(train_y-1, num_classes=3), \n",
    "#                     validation_data=(valid_X, to_categorical(valid_y-1, num_classes=3)[:]), \n",
    "#                     batch_size=batch_size, \n",
    "#                     epochs=epochs,\n",
    "#           shuffle=True,\n",
    "#                     verbose=1\n",
    "#          )\n",
    "history = model.fit(x=train_X, y=to_categorical(train_y-1, num_classes=3), \n",
    "                    validation_data=(valid_X, to_categorical(valid_y-1, num_classes=3)[:]), \n",
    "                    batch_size=batch_size, \n",
    "                    epochs=epochs,\n",
    "                    shuffle=True,\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    0.375216\n",
       "1    0.354554\n",
       "3    0.270231\n",
       "dtype: float64"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_test_preds = model.predict(padded_test_dataset_sequences, batch_size=1024)\n",
    "conv1d = np.argmax(all_test_preds,axis=1)[:]+1\n",
    "pd.Series(conv1d).value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"lstm+conv1d-0.6770-result.txt\", conv1d,fmt=\"%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rnn_cnn_model():\n",
    "    embedding_dim = 300\n",
    "    inp = Input(shape=(MAX_SEQUENCE_LENGTH,))\n",
    "    x =  Embedding(input_dim = nb_words+1, \n",
    "                             output_dim = EMBEDDING_DIM, \n",
    "                             weights=[word_embedding_matrix], \n",
    "                             input_length=MAX_SEQUENCE_LENGTH, \n",
    "                             mask_zero=False,\n",
    "                             trainable=True\n",
    "                            )(inp)\n",
    "    x = SpatialDropout1D(0.3)(x)\n",
    "    x = Bidirectional(GRU(100, return_sequences=True))(x)\n",
    "    x = Conv1D(64, kernel_size=3, padding=\"valid\", kernel_initializer=\"he_uniform\")(x)\n",
    "    avg_pool = GlobalAveragePooling1D()(x)\n",
    "    max_pool = GlobalMaxPooling1D()(x)\n",
    "    kmax_pool = Lambda(lambda x: K.max(x, axis=1), output_shape=(64,))(x)\n",
    "    conc = concatenate([avg_pool, max_pool, kmax_pool])\n",
    "    outp = Dense(3, activation=\"softmax\")(conc)\n",
    "\n",
    "    model = Model(inputs=inp, outputs=outp)\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_cnn_model = get_rnn_cnn_model()\n",
    "\n",
    "plot_model(rnn_cnn_model, to_file='./rnn_cnn_model.png', \n",
    "show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!['./rnn_cnn_model.png'](./rnn_cnn_model.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "epochs = 4\n",
    "\n",
    "# model.fit(x=train_X, y=to_categorical(train_y-1, num_classes=3), \n",
    "#                     validation_data=(valid_X, to_categorical(valid_y-1, num_classes=3)[:]), \n",
    "#                     batch_size=batch_size, \n",
    "#                     epochs=epochs,\n",
    "#           shuffle=True,\n",
    "#                     verbose=1\n",
    "#          )\n",
    "history = rnn_cnn_model.fit(x=train_X, y=to_categorical(train_y-1, num_classes=3), \n",
    "                    validation_data=(valid_X, to_categorical(valid_y-1, num_classes=3)[:]), \n",
    "                    batch_size=batch_size, \n",
    "                    epochs=epochs,\n",
    "                    shuffle=True,\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_test_preds = rnn_cnn_model.predict(padded_test_dataset_sequences, batch_size=1024)\n",
    "w2v7 = np.argmax(all_test_preds,axis=1)[:]+1\n",
    "pd.Series(w2v7).value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tainable = True\n",
    "batch_size = 256\n",
    "epochs = 4\n",
    "\n",
    "# model.fit(x=train_X, y=to_categorical(train_y-1, num_classes=3), \n",
    "#                     validation_data=(valid_X, to_categorical(valid_y-1, num_classes=3)[:]), \n",
    "#                     batch_size=batch_size, \n",
    "#                     epochs=epochs,\n",
    "#           shuffle=True,\n",
    "#                     verbose=1\n",
    "#          )\n",
    "history = rnn_cnn_model.fit(x=train_X, y=to_categorical(train_y-1, num_classes=3), \n",
    "                    validation_data=(valid_X, to_categorical(valid_y-1, num_classes=3)[:]), \n",
    "                    batch_size=batch_size, \n",
    "                    epochs=epochs,\n",
    "                    shuffle=True,\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_test_preds = rnn_cnn_model.predict(padded_test_dataset_sequences, batch_size=1024)\n",
    "w2v8 = np.argmax(all_test_preds,axis=1)[:]+1\n",
    "pd.Series(w2v8).value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add kmax pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "epochs = 4\n",
    "\n",
    "# model.fit(x=train_X, y=to_categorical(train_y-1, num_classes=3), \n",
    "#                     validation_data=(valid_X, to_categorical(valid_y-1, num_classes=3)[:]), \n",
    "#                     batch_size=batch_size, \n",
    "#                     epochs=epochs,\n",
    "#           shuffle=True,\n",
    "#                     verbose=1\n",
    "#          )\n",
    "history = rnn_cnn_model.fit(x=train_X, y=to_categorical(train_y-1, num_classes=3), \n",
    "                    validation_data=(valid_X, to_categorical(valid_y-1, num_classes=3)[:]), \n",
    "                    batch_size=batch_size, \n",
    "                    epochs=epochs,\n",
    "                    shuffle=True,\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "epochs = 1\n",
    "\n",
    "# model.fit(x=train_X, y=to_categorical(train_y-1, num_classes=3), \n",
    "#                     validation_data=(valid_X, to_categorical(valid_y-1, num_classes=3)[:]), \n",
    "#                     batch_size=batch_size, \n",
    "#                     epochs=epochs,\n",
    "#           shuffle=True,\n",
    "#                     verbose=1\n",
    "#          )\n",
    "history = rnn_cnn_model.fit(x=train_X, y=to_categorical(train_y-1, num_classes=3), \n",
    "                    validation_data=(valid_X, to_categorical(valid_y-1, num_classes=3)[:]), \n",
    "                    batch_size=batch_size, \n",
    "                    epochs=epochs,\n",
    "                    shuffle=True,\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_test_preds = rnn_cnn_model.predict(padded_test_dataset_sequences, batch_size=256)\n",
    "rnn_cnn_k_max = np.argmax(all_test_preds,axis=1)[:]+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(rnn_cnn_k_max).value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"rnn-cnn-0.6685-result.txt\", rnn_cnn_k_max,fmt=\"%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "epochs = 5\n",
    "\n",
    "# model.fit(x=train_X, y=to_categorical(train_y-1, num_classes=3), \n",
    "#                     validation_data=(valid_X, to_categorical(valid_y-1, num_classes=3)[:]), \n",
    "#                     batch_size=batch_size, \n",
    "#                     epochs=epochs,\n",
    "#           shuffle=True,\n",
    "#                     verbose=1\n",
    "#          )\n",
    "history = rnn_cnn_model.fit(x=train_X, y=to_categorical(train_y-1, num_classes=3), \n",
    "                    validation_data=(valid_X, to_categorical(valid_y-1, num_classes=3)[:]), \n",
    "                    batch_size=batch_size, \n",
    "                    epochs=epochs,\n",
    "                    shuffle=True,\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_test_preds = rnn_cnn_model.predict(padded_test_dataset_sequences, batch_size=256)\n",
    "rnn_cnn_k_max2 = np.argmax(all_test_preds,axis=1)[:]+1\n",
    "pd.Series(rnn_cnn_k_max2).value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"rnn-cnn-0.6770-result.txt\", rnn_cnn_k_max2,fmt=\"%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_test_preds = rnn_cnn_model.predict(padded_test_dataset_sequences, batch_size=256)\n",
    "w2v6 = np.argmax(all_test_preds,axis=1)[:]+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(w2v6).value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"rnn-cnn-0.6320-result.txt\", w2v6,fmt=\"%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(pre_embedding_layer)\n",
    "model.add(Bidirectional(GRU(256, dropout=0.2, recurrent_dropout=0.1, return_sequences=True)))\n",
    "model.add(Bidirectional(GRU(256, dropout=0.2, recurrent_dropout=0.1)))\n",
    "model.add(Dense(NUM_LABELS, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rnn_cnn_model():\n",
    "    embedding_dim = 300\n",
    "    inp = Input(shape=(MAX_SEQUENCE_LENGTH,))\n",
    "    x =  Embedding(input_dim = nb_words+1, \n",
    "                             output_dim = EMBEDDING_DIM, \n",
    "                             weights=[word_embedding_matrix], \n",
    "                             input_length=MAX_SEQUENCE_LENGTH, \n",
    "                             mask_zero=False,\n",
    "                             trainable=False\n",
    "                            )(inp)\n",
    "    x = SpatialDropout1D(0.3)(x)\n",
    "    x = Bidirectional(GRU(128, return_sequences=True,dropout=0.1,recurrent_dropout=0.1))(x)\n",
    "    x = Conv1D(64, kernel_size=3, padding=\"valid\", kernel_initializer=\"glorot_uniform\")(x)\n",
    "    avg_pool = GlobalAveragePooling1D()(x)\n",
    "#     max_pool = GlobalMaxPooling1D()(x)\n",
    "    kmax_pool = Lambda(lambda x: K.max(x, axis=1), output_shape=(64,))(x)\n",
    "    conc = concatenate([avg_pool, kmax_pool])\n",
    "    outp = Dense(3, activation=\"softmax\")(conc)\n",
    "\n",
    "    model = Model(inputs=inp, outputs=outp)\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "              #optimizer='adam',\n",
    "                  optimizer=Adam(lr=1e-3),\n",
    "              metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_cnn_model = get_rnn_cnn_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "epochs = 5\n",
    "\n",
    "# model.fit(x=train_X, y=to_categorical(train_y-1, num_classes=3), \n",
    "#                     validation_data=(valid_X, to_categorical(valid_y-1, num_classes=3)[:]), \n",
    "#                     batch_size=batch_size, \n",
    "#                     epochs=epochs,\n",
    "#           shuffle=True,\n",
    "#                     verbose=1\n",
    "#          )\n",
    "history = rnn_cnn_model.fit(x=train_X, y=to_categorical(train_y-1, num_classes=3), \n",
    "                    validation_data=(valid_X, to_categorical(valid_y-1, num_classes=3)[:]), \n",
    "                    batch_size=batch_size, \n",
    "                    epochs=epochs,\n",
    "                    shuffle=True,\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "epochs = 5\n",
    "\n",
    "# model.fit(x=train_X, y=to_categorical(train_y-1, num_classes=3), \n",
    "#                     validation_data=(valid_X, to_categorical(valid_y-1, num_classes=3)[:]), \n",
    "#                     batch_size=batch_size, \n",
    "#                     epochs=epochs,\n",
    "#           shuffle=True,\n",
    "#                     verbose=1\n",
    "#          )\n",
    "history = rnn_cnn_model.fit(x=train_X, y=to_categorical(train_y-1, num_classes=3), \n",
    "                    validation_data=(valid_X, to_categorical(valid_y-1, num_classes=3)[:]), \n",
    "                    batch_size=batch_size, \n",
    "                    epochs=epochs,\n",
    "                    shuffle=True,\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
