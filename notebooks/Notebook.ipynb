{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re            # 正则匹配\n",
    "import jieba         # 中文分词\n",
    "import codecs        # 文件编码转换\n",
    "import collections   # 统计词频"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x10d616150>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd # torch 中自动计算梯度模块\n",
    "import torch.nn as nn             # 神经网络模块\n",
    "import torch.nn.functional as F   # 神经网络模块中的常用功能 \n",
    "import torch.optim as optim       # 模型优化器模块\n",
    "import torch.utils.data as Data\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入自定义库\n",
    "from utils.data_utils import clean_str\n",
    "from utils.data_utils import build_vocab\n",
    "from utils.data_utils import get_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preliminary-texting.csv dataset.py              \u001b[34mtrain\u001b[m\u001b[m\r\n",
      "__init__.py             \u001b[34mdev\u001b[m\u001b[m                     training-inspur.csv\r\n",
      "\u001b[31mdata.rar\u001b[m\u001b[m                get_data.sh\r\n"
     ]
    }
   ],
   "source": [
    "!ls ./data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Dateset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"ROWKEY\",\"COMMCONTENT\",\"COMMLEVEL\"\r",
      "\r\n",
      "\"1080003\",\"普通公园一个只是多了几个泉而已，人不多，适合老人孩子闲逛，买票的话还是贵了，人家说6.30之前进园不用花钱\",\"1\"\r",
      "\r\n",
      "\"1080004\",\"跟儿子在里面玩了一天，非常好！跟儿子在里面玩了一天，非常好！真的很不错哦，有空还要去\",\"1\"\r",
      "\r\n",
      "\"1080005\",\"这已经是第五次来这里玩了。每次孩子都很喜欢，不愿意从水里出来。有机会还会再来。还有比我更忠诚的客户吗？哈哈\",\"1\"\r",
      "\r\n",
      "\"1080006\",\"当天在携程上定的票，打温泉度假村咨询电话和携程客服都说次日生效，但到酒店后，票能用。请客服人员了解清楚再回答咨询问题。不然听信，就得中途掉头回家了。\",\"1\"\r",
      "\r\n",
      "\"1080007\",\"烟台历史的一部分，非常值得推荐去看看！海边景色也很漂亮！\",\"1\"\r",
      "\r\n",
      "\"1080008\",\"周末看看动物亲近亲近大自然挺好的，媳妇儿还跟猴子拍照，猴子满身爬，挺好玩，如果动物再多点就好了，门票小贵\",\"1\"\r",
      "\r\n",
      "\"1080009\",\"五四广场青岛旅游景点必打卡又一地点，标志性红包建筑雕塑矗立在市政府对面亦是海边，还是有点意思的，旁边有游船可出海游玩\",\"1\"\r",
      "\r\n",
      "\"1080010\",\"五四广场坐落在山东省青岛市市南区，在海边，那个红红的火炬就是五四广场的标志。广场上有大片绿地，广场上游玩的人挺多的，聚集火车站不太远。\",\"1\"\r",
      "\r\n",
      "\"1080011\",\"环境好，景色美，值得游览，感觉很好\",\"1\"\r",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!head ./data/training-inspur.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Preprocessing\n",
    "- 去除标点符号\n",
    "- 中文分词\n",
    "- 去除空格\n",
    "- 去除缺失项"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 加载数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dataset = pd.read_csv('./data/training-inspur.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/5n/2_by50851fxc4d_snc1d9wf80000gn/T/jieba.cache\n",
      "Loading model cost 0.724 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "COMMCONTENT_SEG = []\n",
    "\n",
    "for sent in df_dataset['COMMCONTENT']:\n",
    "\n",
    "    # Extract Sentence\n",
    "    sent = str(sent).strip()\n",
    "\n",
    "    sent = clean_str(sent)\n",
    "\n",
    "    stopwords = [\" \",\"!\",\"....................................................................\"]\n",
    "\n",
    "    seg_list = jieba.cut(sent, cut_all=False)\n",
    "\n",
    "    seg_list = [i for i in seg_list if i not in stopwords]\n",
    "    \n",
    "    COMMCONTENT_SEG.append(\" \".join(seg_list))\n",
    "df_dataset['COMMCONTENT_SEG'] = pd.DataFrame(COMMCONTENT_SEG,columns=['COMMCONTENT_SEG'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dataset = df_dataset[df_dataset['COMMCONTENT_SEG']!=\"\"]\n",
    "df_dataset = df_dataset.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>ROWKEY</th>\n",
       "      <th>COMMCONTENT</th>\n",
       "      <th>COMMLEVEL</th>\n",
       "      <th>COMMCONTENT_SEG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1080003</td>\n",
       "      <td>普通公园一个只是多了几个泉而已，人不多，适合老人孩子闲逛，买票的话还是贵了，人家说6.30之...</td>\n",
       "      <td>1</td>\n",
       "      <td>普通 公园 一个 只是 多 了 几个 泉 而已 人不多 适合 老人 孩子 闲逛 买票 的话 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1080004</td>\n",
       "      <td>跟儿子在里面玩了一天，非常好！跟儿子在里面玩了一天，非常好！真的很不错哦，有空还要去</td>\n",
       "      <td>1</td>\n",
       "      <td>跟 儿子 在 里面 玩 了 一天 非常 好 跟 儿子 在 里面 玩 了 一天 非常 好 真的...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1080005</td>\n",
       "      <td>这已经是第五次来这里玩了。每次孩子都很喜欢，不愿意从水里出来。有机会还会再来。还有比我更忠诚...</td>\n",
       "      <td>1</td>\n",
       "      <td>这 已经 是 第五次 来 这里 玩 了 每次 孩子 都 很 喜欢 不 愿意 从水里 出来 有...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1080006</td>\n",
       "      <td>当天在携程上定的票，打温泉度假村咨询电话和携程客服都说次日生效，但到酒店后，票能用。请客服人...</td>\n",
       "      <td>1</td>\n",
       "      <td>当天 在 携程 上定 的 票 打 温泉 度假村 咨询电话 和 携程 客服 都 说 次日 生效...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1080007</td>\n",
       "      <td>烟台历史的一部分，非常值得推荐去看看！海边景色也很漂亮！</td>\n",
       "      <td>1</td>\n",
       "      <td>烟台 历史 的 一部分 非常 值得 推荐 去 看看 海边 景色 也 很漂亮</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index   ROWKEY                                        COMMCONTENT  \\\n",
       "0      0  1080003  普通公园一个只是多了几个泉而已，人不多，适合老人孩子闲逛，买票的话还是贵了，人家说6.30之...   \n",
       "1      1  1080004         跟儿子在里面玩了一天，非常好！跟儿子在里面玩了一天，非常好！真的很不错哦，有空还要去   \n",
       "2      2  1080005  这已经是第五次来这里玩了。每次孩子都很喜欢，不愿意从水里出来。有机会还会再来。还有比我更忠诚...   \n",
       "3      3  1080006  当天在携程上定的票，打温泉度假村咨询电话和携程客服都说次日生效，但到酒店后，票能用。请客服人...   \n",
       "4      4  1080007                       烟台历史的一部分，非常值得推荐去看看！海边景色也很漂亮！   \n",
       "\n",
       "   COMMLEVEL                                    COMMCONTENT_SEG  \n",
       "0          1  普通 公园 一个 只是 多 了 几个 泉 而已 人不多 适合 老人 孩子 闲逛 买票 的话 ...  \n",
       "1          1  跟 儿子 在 里面 玩 了 一天 非常 好 跟 儿子 在 里面 玩 了 一天 非常 好 真的...  \n",
       "2          1  这 已经 是 第五次 来 这里 玩 了 每次 孩子 都 很 喜欢 不 愿意 从水里 出来 有...  \n",
       "3          1  当天 在 携程 上定 的 票 打 温泉 度假村 咨询电话 和 携程 客服 都 说 次日 生效...  \n",
       "4          1              烟台 历史 的 一部分 非常 值得 推荐 去 看看 海边 景色 也 很漂亮  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 导出处理后数据集 CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset exists in: ./dataset_inspur.csv\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists('./dataset_inspur.csv'):\n",
    "    print(\"Downloading...\")\n",
    "    \n",
    "    df_dataset.to_csv('./dataset_inspur.csv', \n",
    "                      sep=\",\", \n",
    "                      columns=['ROWKEY','COMMCONTENT_SEG','COMMLEVEL'],\n",
    "                      index=False, \n",
    "                      encoding='utf-8')\n",
    "else:\n",
    "    print(\"Dataset exists in: ./dataset_inspur.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab,vocab_freqs = build_vocab(df_dataset['COMMCONTENT_SEG'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('的', 33632), ('了', 13661), ('是', 8170), ('很', 7257), ('去', 6813)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_freqs.most_common(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33950"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab_freqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = '/Users/tsw/ScenicSpotReviews'\n",
    "W2V_DIR = BASE_DIR + '/embeddings/'\n",
    "TEXT_DATA_DIR = BASE_DIR + '/data/'\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 588\n",
    "\n",
    "MAX_NUM_WORDS = 33950\n",
    "\n",
    "EMBEDDING_DIM = 300\n",
    "\n",
    "VALIDATION_SPLIT = 0.2\n",
    "\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = min(MAX_NUM_WORDS, len(vocab_freqs)) + 2\n",
    "word2index = {x[0]: i+2 for i, x in enumerate(vocab_freqs.most_common(33950))}\n",
    "word2index[\"PAD\"] = 0\n",
    "word2index[\"UNK\"] = 1\n",
    "index2word = {v:k for k, v in word2index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30002"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2index[\"壁纸\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'普通'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index2word[560]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequences(vectorized_seqs, seq_lengths):\n",
    "    seq_tensor = torch.zeros((len(vectorized_seqs),max(seq_lengths))).long()\n",
    "    for idx, (seq, seq_len) in enumerate(zip(vectorized_seqs, seq_lengths)):\n",
    "        seq_tensor[idx, :seq_len] = torch.LongTensor(seq)\n",
    "    return seq_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#seq_tensor = pad_sequences(vectorized_seqs,seq_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing word vectors...\n",
      "Found 150128 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# Lookup Table\n",
    "# First, build index mapping words in the embeddings set to their embedding vector\n",
    "print('Indexing word vectors...')\n",
    "\n",
    "embeddings_index = {}\n",
    "\n",
    "with open(os.path.join(W2V_DIR, 'zhihu.vec')) as f:\n",
    "    all_lines = f.readlines()\n",
    "    lines = all_lines[1:]\n",
    "    for line in lines:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### word embedding Lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33950"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_words = min(MAX_NUM_WORDS,len(word2index))\n",
    "num_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embedding_matrix = np.zeros((num_words + 1, EMBEDDING_DIM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word, i in word2index.items():\n",
    "    if i > MAX_NUM_WORDS:\n",
    "        continue\n",
    "    \n",
    "    embedding_vector = embeddings_index.get(str(word))\n",
    "    \n",
    "    if embedding_vector is not None:\n",
    "        word_embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('word_embedding_matrix.shape:', (33951, 300))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"word_embedding_matrix.shape:\",word_embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练集/测试集 划分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(X, y, train_ratio=0.8):\n",
    "    X = np.array(X)\n",
    "    # seq_lens = np.array(seq_lens)\n",
    "    data_size = len(X)\n",
    "\n",
    "    # Shuffle the data\n",
    "    shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "    X, y = X[shuffle_indices], y[shuffle_indices]\n",
    "\n",
    "    # Split into train and validation set\n",
    "    train_end_index = int(train_ratio*data_size)\n",
    "    train_X = X[:train_end_index]\n",
    "    train_y = y[:train_end_index]\n",
    "\n",
    "    valid_X = X[train_end_index:]\n",
    "    valid_y = y[train_end_index:]\n",
    "    \n",
    "    return train_X,train_y,valid_X,valid_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X,train_y,valid_X,valid_y = split_dataset(df_dataset['COMMCONTENT_SEG'], \n",
    "                                                df_dataset['COMMLEVEL'], \n",
    "                                                train_ratio=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15995,)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将 words 转换为\n",
    "def text_to_seqs(texts):\n",
    "    seqs = []\n",
    "    for text in texts:\n",
    "        words = []\n",
    "        text = text.strip().split(\" \")\n",
    "        for word in text:\n",
    "            words.append(word2index[word])\n",
    "        seqs.append(words)\n",
    "    return seqs\n",
    "\n",
    "# 将 words 转换为\n",
    "def text_to_sequences(vectorized_seqs, seq_lengths):\n",
    "    seqs_tensor = torch.zeros((len(vectorized_seqs),max(seq_lengths))).long()\n",
    "#     print(seqs_tensor.shape)\n",
    "    for idx, (seq, seq_len) in enumerate(zip(vectorized_seqs, seq_lengths)):\n",
    "        seqs_tensor[idx, :seq_len] = torch.LongTensor(seq)\n",
    "    return seqs_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "def pad_sequences(vectorized_seqs, seq_lengths):\n",
    "    seq_tensor = torch.zeros((len(vectorized_seqs),max(seq_lengths))).long()\n",
    "    for idx, (seq, seq_len) in enumerate(zip(vectorized_seqs, seq_lengths)):\n",
    "        seq_tensor[idx, :seq_len] = torch.LongTensor(seq)\n",
    "    return seq_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X_vectorized_seqs = text_to_seqs(train_X)\n",
    "valid_X_vectorized_seqs = text_to_seqs(valid_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15995, 3999)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_X_vectorized_seqs),len(valid_X_vectorized_seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X_seq_lengths = [len(i) for i in train_X_vectorized_seqs]\n",
    "valid_X_seq_lengths = [len(i) for i in valid_X_vectorized_seqs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15995, 3999)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_X_seq_lengths),len(valid_X_seq_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X_tensor = text_to_sequences(train_X_vectorized_seqs,train_X_seq_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_X_tensor = text_to_sequences(valid_X_seq_lengths,valid_X_seq_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y_tensor = torch.from_numpy(np.array(train_y.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_dataset = Data.TensorDataset(train_X_tensor,train_y_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把 dataset 放入 DataLoader\n",
    "data_loader = Data.DataLoader(\n",
    "    dataset=torch_dataset,      # torch TensorDataset format\n",
    "    batch_size=BATCH_SIZE,      # mini batch size\n",
    "    shuffle=True,               # 要不要打乱数据 (打乱比较好)\n",
    "    num_workers=2,              # 多线程来读数据\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequence(seq, to_ix):\n",
    "    idxs = [to_ix[w] for w in seq]\n",
    "    return torch.tensor(idxs, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = '/Users/tsw/ScenicSpotReviews'\n",
    "\n",
    "W2V_DIR = BASE_DIR + '/embeddings/'\n",
    "\n",
    "TEXT_DATA_DIR = BASE_DIR + '/data/'\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 588\n",
    "\n",
    "MAX_NUM_WORDS = 33950\n",
    "\n",
    "EMBEDDING_DIM = 300\n",
    "\n",
    "VALIDATION_SPLIT = 0.2\n",
    "\n",
    "HIDDEN_DIM=64\n",
    "\n",
    "\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMTagger(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
    "        super(LSTMTagger, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # word_embeddings layer\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "\n",
    "        # The linear layer that maps from hidden state space to tag space\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
    "        self.hidden = self.init_hidden()\n",
    "\n",
    "    def init_hidden(self):\n",
    "        # Before we've done anything, we dont have any hidden state.\n",
    "        # Refer to the Pytorch documentation to see exactly\n",
    "        # why they have this dimensionality.\n",
    "        # The axes semantics are (num_layers, minibatch_size, hidden_dim)\n",
    "        return (torch.zeros(1, 1, self.hidden_dim),\n",
    "                torch.zeros(1, 1, self.hidden_dim))\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        lstm_out, self.hidden = self.lstm(\n",
    "            embeds.view(len(sentence), 1, -1), self.hidden)\n",
    "        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
    "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "        return tag_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([   353,   2616,     24,   2050,      8,   3675,      2,     29,\n",
      "           177,     54,     68,   4746,      9,  11463,     10,     78,\n",
      "             3,    362,    926,     24,     59,    528,    183,     48,\n",
      "            30,     67,    452])\n",
      "tensor([[-1.2748, -1.0953, -0.9517],\n",
      "        [-1.2725, -1.1569, -0.9029],\n",
      "        [-1.3127, -0.9682, -1.0466],\n",
      "        [-1.2592, -0.9199, -1.1471],\n",
      "        [-1.2245, -0.9501, -1.1413],\n",
      "        [-1.2281, -0.9344, -1.1574],\n",
      "        [-1.0572, -1.1701, -1.0723],\n",
      "        [-1.2318, -0.9712, -1.1099],\n",
      "        [-1.1322, -1.1616, -1.0087],\n",
      "        [-1.2227, -1.1217, -0.9681],\n",
      "        [-1.1414, -1.1219, -1.0357],\n",
      "        [-1.1147, -1.0592, -1.1231],\n",
      "        [-1.1376, -1.0471, -1.1133],\n",
      "        [-1.0497, -0.9777, -1.2955],\n",
      "        [-1.1428, -1.0444, -1.1112],\n",
      "        [-1.1097, -0.9987, -1.1974],\n",
      "        [-1.2452, -0.9770, -1.0917],\n",
      "        [-1.3424, -0.9859, -1.0060],\n",
      "        [-1.2913, -1.0178, -1.0114],\n",
      "        [-1.4784, -0.8560, -1.0580],\n",
      "        [-1.4352, -1.0043, -0.9273],\n",
      "        [-1.1871, -1.1279, -0.9911],\n",
      "        [-1.2667, -1.0504, -0.9985],\n",
      "        [-1.2075, -1.0487, -1.0480],\n",
      "        [-1.2037, -1.0370, -1.0630],\n",
      "        [-1.1672, -1.0037, -1.1324],\n",
      "        [-0.9273, -1.1123, -1.2889]])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected input batch_size (27) to match target batch_size (3).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-145-9a198c86d3af>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;31m# Step 4. Compute the loss, gradients, and update the parameters by\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;31m#  calling optimizer.step()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/DeepLearningEnv/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/DeepLearningEnv/lib/python3.6/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    191\u001b[0m         \u001b[0m_assert_no_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m         return F.nll_loss(input, target, self.weight, self.size_average,\n\u001b[0;32m--> 193\u001b[0;31m                           self.ignore_index, self.reduce)\n\u001b[0m\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/DeepLearningEnv/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mnll_loss\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce)\u001b[0m\n\u001b[1;32m   1328\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m         raise ValueError('Expected input batch_size ({}) to match target batch_size ({}).'\n\u001b[0;32m-> 1330\u001b[0;31m                          .format(input.size(0), target.size(0)))\n\u001b[0m\u001b[1;32m   1331\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1332\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected input batch_size (27) to match target batch_size (3)."
     ]
    }
   ],
   "source": [
    "labels = [\"1\",\"2\",\"3\"]\n",
    "label_to_ix = {\"1\": 0, \"2\": 1, \"3\": 2}\n",
    "\n",
    "model = LSTMTagger(EMBEDDING_DIM, HIDDEN_DIM, len(word2index), len(label_to_ix))\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "# See what the scores are before training\n",
    "# Note that element i,j of the output is the score for tag j for word i.\n",
    "# Here we don't need to train, so the code is wrapped in torch.no_grad()\n",
    "# with torch.no_grad():\n",
    "#     inputs = prepare_sequence(training_data[0][0], word2index)\n",
    "#     label_scores = model(inputs)\n",
    "#     print(label_scores)\n",
    "\n",
    "for epoch in range(300):  # again, normally you would NOT do 300 epochs, it is toy data\n",
    "    for sentence, labels in training_data:\n",
    "        # Step 1. Remember that Pytorch accumulates gradients.\n",
    "        # We need to clear them out before each instance\n",
    "        model.zero_grad()\n",
    "        \n",
    "#         print(sentence)\n",
    "\n",
    "        # Also, we need to clear out the hidden state of the LSTM,\n",
    "        # detaching it from its history on the last instance.\n",
    "        model.hidden = model.init_hidden()\n",
    "\n",
    "        # Step 2. Get our inputs ready for the network, that is, turn them into\n",
    "        # Tensors of word indices.\n",
    "        sentence_in = prepare_sequence(sentence, word2index)\n",
    "        print(sentence_in)\n",
    "        #labels = prepare_sequence(labels, label_to_ix)\n",
    "        labels = torch.tensor([1,2,3], dtype=torch.long)\n",
    "\n",
    "        # Step 3. Run our forward pass.\n",
    "        labels_scores = model(sentence_in)\n",
    "        \n",
    "        print(labels_scores)\n",
    "\n",
    "        # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "        #  calling optimizer.step()\n",
    "        loss = loss_function(labels_scores, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# See what the scores are after training\n",
    "# with torch.no_grad():\n",
    "#     inputs = prepare_sequence(training_data[0][0], word2index)\n",
    "#     labels_scores = model(inputs)\n",
    "\n",
    "#     # The sentence is \"the dog ate the apple\".  i,j corresponds to score for tag j\n",
    "#     # for word i. The predicted tag is the maximum scoring tag.\n",
    "#     # Here, we can see the predicted sequence below is 0 1 2 0 1\n",
    "#     # since 0 is index of the maximum value of row 1,\n",
    "#     # 1 is the index of maximum value of row 2, etc.\n",
    "#     # Which is DET NOUN VERB DET NOUN, the correct sequence!\n",
    "#     print(tag_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, label_size):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    " \n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    " \n",
    "#         self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "    \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size = 300,\n",
    "            hidden_size = 64,\n",
    "            num_layers = 1,\n",
    "            batch_first = False\n",
    "        )\n",
    "        \n",
    "        self.hidden2label = nn.Linear(64,3)\n",
    " \n",
    "#         self.hidden2label = nn.Linear(hidden_dim, label_size)\n",
    "    \n",
    "        self.hidden = self.init_hidden()\n",
    " \n",
    "    def init_hidden(self):\n",
    "        # Before we've done anything, we dont have any hidden state.\n",
    "        # Refer to the Pytorch documentation to see exactly\n",
    "        # why they have this dimensionality.\n",
    "        # The axes semantics are (num_layers, minibatch_size, hidden_dim)\n",
    "        return (torch.zeros(1, 1, self.hidden_dim),\n",
    "                torch.zeros(1, 1, self.hidden_dim))\n",
    " \n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        lstm_out, self.hidden = self.lstm(\n",
    "            embeds.view(len(sentence), 1, -1), self.hidden)\n",
    "        label_space = self.hidden2label(lstm_out.view(len(sentence), -1))\n",
    "        label_scores = F.log_softmax(label_space,dim=None)\n",
    "#         label_scores = nn.LogSoftmax(dim=label_space)\n",
    "        return label_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = LSTMClassifier(EMBEDDING_DIM, HIDDEN_DIM, len(word2inddx), len(tag_to_ix))\n",
    "model = LSTMClassifier(EMBEDDING_DIM, HIDDEN_DIM, len(word2index), 3)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = []\n",
    "for sent,label in list(zip(train_X,train_y)):\n",
    "    training_data.append((sent.split(\" \"),label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.1190, -1.0934, -1.0838],\n",
      "        [-0.9497, -1.2108, -1.1546],\n",
      "        [-0.9388, -1.1502, -1.2298],\n",
      "        [-0.9333, -1.1615, -1.2251],\n",
      "        [-0.9641, -1.2443, -1.1071],\n",
      "        [-0.9483, -1.2474, -1.1228],\n",
      "        [-0.9744, -1.3424, -1.0178],\n",
      "        [-0.9813, -1.2142, -1.1140],\n",
      "        [-1.0991, -1.0904, -1.1064],\n",
      "        [-1.0631, -1.0701, -1.1658],\n",
      "        [-1.1996, -1.1411, -0.9697],\n",
      "        [-0.8917, -1.1714, -1.2726],\n",
      "        [-1.0936, -1.3022, -0.9338],\n",
      "        [-1.1101, -1.1369, -1.0508],\n",
      "        [-0.9960, -1.0973, -1.2144],\n",
      "        [-0.9800, -1.0846, -1.2494],\n",
      "        [-1.0642, -1.2958, -0.9642],\n",
      "        [-1.0611, -1.2229, -1.0229],\n",
      "        [-1.0389, -1.1774, -1.0844],\n",
      "        [-0.9874, -1.1101, -1.2109],\n",
      "        [-0.9401, -1.1184, -1.2637],\n",
      "        [-0.9727, -1.2311, -1.1088],\n",
      "        [-1.0073, -1.2971, -1.0176],\n",
      "        [-0.9683, -1.3388, -1.0268],\n",
      "        [-1.2060, -1.3781, -0.8017],\n",
      "        [-1.2453, -1.3628, -0.7848],\n",
      "        [-1.1418, -1.0745, -1.0809]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tsw/anaconda/envs/DeepLearningEnv/lib/python3.6/site-packages/ipykernel_launcher.py:37: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "# See what the scores are before training\n",
    "# Note that element i,j of the output is the score for tag j for word i.\n",
    "# Here we don't need to train, so the code is wrapped in torch.no_grad()\n",
    "with torch.no_grad():\n",
    "    model.zero_grad()\n",
    "    inputs = prepare_sequence(training_data[0][0], word2index)\n",
    "    label_scores = model(inputs)\n",
    "    print(label_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tsw/anaconda/envs/DeepLearningEnv/lib/python3.6/site-packages/ipykernel_launcher.py:37: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected input batch_size (27) to match target batch_size (3).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-94-a5bb9551f342>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;31m# Step 4. Compute the loss, gradients, and update the parameters by\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;31m#  calling optimizer.step()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/DeepLearningEnv/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/DeepLearningEnv/lib/python3.6/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    757\u001b[0m         \u001b[0m_assert_no_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m         return F.cross_entropy(input, target, self.weight, self.size_average,\n\u001b[0;32m--> 759\u001b[0;31m                                self.ignore_index, self.reduce)\n\u001b[0m\u001b[1;32m    760\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/DeepLearningEnv/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce)\u001b[0m\n\u001b[1;32m   1440\u001b[0m         \u001b[0;34m>>\u001b[0m\u001b[0;34m>\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m     \"\"\"\n\u001b[0;32m-> 1442\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/DeepLearningEnv/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mnll_loss\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce)\u001b[0m\n\u001b[1;32m   1328\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m         raise ValueError('Expected input batch_size ({}) to match target batch_size ({}).'\n\u001b[0;32m-> 1330\u001b[0;31m                          .format(input.size(0), target.size(0)))\n\u001b[0m\u001b[1;32m   1331\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1332\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected input batch_size (27) to match target batch_size (3)."
     ]
    }
   ],
   "source": [
    "for epoch in range(100):  # again, normally you would NOT do 300 epochs, it is toy data\n",
    "    for sentence, labels in training_data:\n",
    "        # Step 1. Remember that Pytorch accumulates gradients.\n",
    "        # We need to clear them out before each instance\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Also, we need to clear out the hidden state of the LSTM,\n",
    "        # detaching it from its history on the last instance.\n",
    "        model.hidden = model.init_hidden()\n",
    "\n",
    "        # Step 2. Get our inputs ready for the network, that is, turn them into\n",
    "        # Tensors of word indices.\n",
    "        sentence_in = prepare_sequence(sentence, word2index)\n",
    "        # targets = prepare_sequence(labels, label_to_ix)\n",
    "        labels = torch.tensor([1,2,3], dtype=torch.long)\n",
    "\n",
    "        # Step 3. Run our forward pass.\n",
    "        label_scores = model(sentence_in)\n",
    "\n",
    "        # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "        #  calling optimizer.step()\n",
    "        loss = loss_function(label_scores, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size = 32,\n",
    "            hidden_size = 64,\n",
    "            num_layers = 1,\n",
    "            batch_first = True\n",
    "        )\n",
    "        \n",
    "        self.out = nn.Linear(64,3)\n",
    "        \n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        self.word_embedding = nn.Embedding.from_pretrained(torch.from_numpy(word_embedding_matrix))\n",
    "\n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "\n",
    "        # The linear layer that maps from hidden state space to tag space\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
    "        \n",
    "        self.hidden = self.init_hidden()\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        r_out, (h_n, h_c) = self.lstm(x, None)\n",
    "        out = self.out(r_out[:,-1,:])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = LSTM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(lstm.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(100):\n",
    "    for step, (x,y) in enumerate(train_loader):\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = nn.Embedding.from_pretrained(torch.from_numpy(word_embedding_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
    "#         super(LSTMClassifier, self).__init__()\n",
    "#         self.hidden_dim = hidden_dim\n",
    "\n",
    "#         # self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "#         self.word_embedding = nn.Embedding.from_pretrained(torch.from_numpy(word_embedding_matrix))\n",
    "\n",
    "#         # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "#         # with dimensionality hidden_dim.\n",
    "#         self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "\n",
    "#         # The linear layer that maps from hidden state space to tag space\n",
    "#         self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
    "        \n",
    "#         self.hidden = self.init_hidden()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM,HIDDEN_DIM,len(word2index),len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X,train_y,valid_X,valid_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(zip(train_X,train_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = []\n",
    "for sent,label in list(zip(train_X,train_y)):\n",
    "    training_data.append((sent.split(\" \"),label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare_sequence(training_data, word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Data.TensorDataset(train_X,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Data.TensorDataset(train_X_tensor,train_y_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See what the scores are before training\n",
    "# Note that element i,j of the output is the score for tag j for word i.\n",
    "# Here we don't need to train, so the code is wrapped in torch.no_grad()\n",
    "with torch.no_grad():\n",
    "    model.zero_grad()\n",
    "    inputs = prepare_sequence(training_data[0][0], word2index)\n",
    "#     print(inputs)\n",
    "    label_scores = model(inputs)\n",
    "    print(label_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(10):  # again, normally you would NOT do 300 epochs, it is toy data\n",
    "    for step, (batch_x, batch_y) in enumerate(data_loader):  # 每一步 loader 释放一小批数据用来学习\n",
    "#         print(batch_x.shape)\n",
    "        batch_x = batch_x.view(-1, 1, 588)\n",
    "#         print(batch_x.shape)\n",
    "        # for sentence, labels in training_data:\n",
    "        # Step 1. Remember that Pytorch accumulates gradients.\n",
    "        # We need to clear them out before each instance\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Also, we need to clear out the hidden state of the LSTM,\n",
    "        # detaching it from its history on the last instance.\n",
    "        model.hidden = model.init_hidden()\n",
    "\n",
    "        # Step 2. Get our inputs ready for the network, that is, turn them into\n",
    "        # Tensors of word indices.\n",
    "#         sentence_in = prepare_sequence(batch_x, word2index)\n",
    "        # targets = prepare_sequence(labels, label_to_ix)\n",
    "        labels = torch.tensor([1,2,3], dtype=torch.long)\n",
    "\n",
    "        # Step 3. Run our forward pass.\n",
    "        label_scores = model(batch_x)\n",
    "\n",
    "        # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "        #  calling optimizer.step()\n",
    "        loss = loss_function(label_scores, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(100):  # again, normally you would NOT do 300 epochs, it is toy data\n",
    "    for sentence, labels in training_data:\n",
    "        # Step 1. Remember that Pytorch accumulates gradients.\n",
    "        # We need to clear them out before each instance\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Also, we need to clear out the hidden state of the LSTM,\n",
    "        # detaching it from its history on the last instance.\n",
    "        model.hidden = model.init_hidden()\n",
    "\n",
    "        # Step 2. Get our inputs ready for the network, that is, turn them into\n",
    "        # Tensors of word indices.\n",
    "        sentence_in = prepare_sequence(sentence, word2index)\n",
    "        # targets = prepare_sequence(labels, label_to_ix)\n",
    "        labels = torch.tensor([1,2,3], dtype=torch.long)\n",
    "\n",
    "        # Step 3. Run our forward pass.\n",
    "        label_scores = model(sentence_in)\n",
    "\n",
    "        # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "        #  calling optimizer.step()\n",
    "        loss = loss_function(label_scores, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor([1,2,3], dtype=torch.long).view(1,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequence(seq, to_ix):\n",
    "    idxs = [to_ix[w] for w in seq]\n",
    "    return torch.tensor(idxs, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = ['普通','公园','一个','只是','多', '了', '几个', '泉', '而已', '人不多', '适合','老人', '孩子', '闲逛', '买票']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pseq = prepare_sequence(seq, word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pseq[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input = torch.LongTensor(pseq)\n",
    "embedding(torch.LongTensor(pseq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one = pd.read_csv('./data/train/1_train.txt', encoding='utf-8',header=None,names=['COMMLEVEL','COMMCONTENT'])\n",
    "two = pd.read_csv('./data/train/2_train.txt', encoding='utf-8',header=None,names=['COMMLEVEL','COMMCONTENT'])\n",
    "three = pd.read_csv('./data/train/3_train.txt', encoding='utf-8',header=None,names=['COMMLEVEL','COMMCONTENT'])\n",
    "train = pd.concat([one,two,three],ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dataset = pd.concat([train,test],ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_and_labels():\n",
    "\n",
    "    # Load the data\n",
    "    positive_examples = list(codecs.open(\"data/rt-polaritydata/rt-polarity.pos\", 'r', 'utf-8').readlines())\n",
    "    positive_examples = [s.strip() for s in positive_examples]\n",
    "    positive_examples = [get_tokens(clean_str(sent)) for sent in positive_examples]\n",
    "    negative_examples = list(open(\"data/rt-polaritydata/rt-polarity.neg\", \"r\").readlines())\n",
    "    negative_examples = [s.strip() for s in negative_examples]\n",
    "    negative_examples = [get_tokens(clean_str(sent)) for sent in negative_examples]\n",
    "    X = positive_examples + negative_examples\n",
    "\n",
    "    # Labels\n",
    "    positive_labels = [[0,1] for _ in positive_examples]\n",
    "    negative_labels = [[1,0] for _ in negative_examples]\n",
    "    y = np.concatenate([positive_labels, negative_labels], 0)\n",
    "\n",
    "    print \"Total: %i, NEG: %i, POS: %i\" % (len(y), np.sum(y[:, 0]), np.sum(y[:, 1]))\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_examples = list(codecs.open(\"./data/train/1_train.txt\", 'r', 'utf-8').readlines())\n",
    "positive_examples = [s.strip() for s in positive_examples]\n",
    "\n",
    "# positive_examples = [get_tokens(clean_str(sent)) for sent in positive_examples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_labels = [[1,0,0] for _ in positive_examples]\n",
    "positive_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.empty(num_recs,dtype=list)\n",
    "y = np.zeros(num_recs)\n",
    "i=0\n",
    "with open('./data/training-inspur.csv','r+') as f:\n",
    "    for line in f:\n",
    "        _,label, sentence = line.strip().split(\"\\t\")\n",
    "        words = nltk.word_tokenize(sentence.lower())\n",
    "        seqs = []\n",
    "        for word in words:\n",
    "            if word in word2index:\n",
    "                seqs.append(word2index[word])\n",
    "            else:\n",
    "                seqs.append(word2index[\"UNK\"])\n",
    "        X[i] = seqs\n",
    "        y[i] = int(label)\n",
    "        i += 1\n",
    "X = sequence.pad_sequences(X, maxlen=MAX_SENTENCE_LENGTH)\n",
    "\n",
    "def pad_sequences(vectorized_seqs, seq_lengths):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"word_embedding_matrix.shape:\",word_embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word, i in word2index.items():\n",
    "    if i > MAX_NB_WORDS:\n",
    "        continue\n",
    "    \n",
    "    embedding_vector = embeddings_index.get(str(word))\n",
    "    \n",
    "    if embedding_vector is not None:\n",
    "        word_embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### jia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = nn.Embedding.from_pretrained(torch.from_numpy(word_embedding_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.LongTensor([3])\n",
    "embedding(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with codecs.open('./data/training-inspur.csv','r', 'utf-8') as f:\n",
    "    all_lines = f.readlines()\n",
    "    \n",
    "    # 跳过第一行 Header\n",
    "    for line in all_lines[1:]:\n",
    "        # Extract Sentence\n",
    "        _key, sentence, label = line.strip().split('\",\"')\n",
    "        \n",
    "        sentence = clean_str(sentence)\n",
    "        \n",
    "        stopwords = [\" \",\"!\",\"....................................................................\"]\n",
    "        \n",
    "        seg_list = jieba.cut(sentence, cut_all=False)\n",
    "        \n",
    "        seg_list = [i for i in seg_list if i not in stopwords]\n",
    "        \n",
    "        length.append(len(seg_list))\n",
    "        \n",
    "        word_list.append(seg_list)\n",
    "        \n",
    "        if len(seg_list) > max_length:\n",
    "            max_length = len(seg_list)\n",
    "        \n",
    "        for word in seg_list:\n",
    "            word_freqs[word] += 1\n",
    "        num_recs += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## EDA \n",
    "length = []\n",
    "max_length = 0\n",
    "word_freqs = collections.Counter()\n",
    "word_list = []\n",
    "num_recs = 0\n",
    "with codecs.open('./data/training-inspur.csv','r', 'utf-8') as f:\n",
    "    all_lines = f.readlines()\n",
    "    # 跳过第一行\n",
    "    for line in all_lines[1:]:\n",
    "        # Extract Sentence\n",
    "        _key, sentence, label = line.strip().split('\",\"')\n",
    "        \n",
    "        sentence = clean_str(sentence)\n",
    "        \n",
    "        stopwords = [\" \",\"!\",\"....................................................................\"]\n",
    "        \n",
    "        seg_list = jieba.cut(sentence, cut_all=False)\n",
    "        \n",
    "        seg_list = [i for i in seg_list if i not in stopwords]\n",
    "        \n",
    "        length.append(len(seg_list))\n",
    "        \n",
    "        word_list.append(seg_list)\n",
    "        \n",
    "        if len(seg_list) > max_length:\n",
    "            max_length = len(seg_list)\n",
    "        \n",
    "        for word in seg_list:\n",
    "            word_freqs[word] += 1\n",
    "        num_recs += 1\n",
    "        \n",
    "print('max_length ',max_length)\n",
    "print('nb_words ', len(word_freqs))\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist(length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.6241, -1.1140,  0.2834, -0.1117,  1.4462,  1.6221,  0.9165,\n",
      "          0.9102, -0.8726,  1.2840,  0.4811, -2.1449, -0.7621, -0.3971,\n",
      "          0.1165, -0.2806, -1.1074, -0.3137, -0.7684, -0.4783, -0.2467,\n",
      "          0.6146, -0.5697,  1.0643,  1.8119,  0.2465, -0.8759, -0.0283,\n",
      "          0.1164, -0.5778,  1.4203, -1.6684,  0.9633,  2.5660, -0.8036,\n",
      "         -0.0420,  0.0867,  0.1872,  0.2350,  0.5459, -0.1542, -1.6307,\n",
      "         -2.0396,  0.3050, -1.0852,  0.0199,  0.3821,  1.3308,  0.6283,\n",
      "          0.6861]])\n"
     ]
    }
   ],
   "source": [
    "embeds = nn.Embedding(len(word2index), 50)  # 2 words in vocab, 5 dimensional embeddings\n",
    "lookup_tensor = torch.tensor([word2index[\"你\"]], dtype=torch.long)\n",
    "hello_embed = embeds(lookup_tensor)\n",
    "print(hello_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2index[\"你\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(33952, 50)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
